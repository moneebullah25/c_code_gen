<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="Applying Activations, Gradients, and Batch Normalization">

<title>c_code_gen - Batch Normalization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="c_code_gen - Batch Normalization">
<meta property="og:description" content="Applying Activations, Gradients, and Batch Normalization">
<meta property="og:image" content="https://moneebullah25.github.io/c_code_gen/02_bn_files/figure-html/cell-13-output-2.png">
<meta property="og:site-name" content="c_code_gen">
<meta property="og:image:height" content="372">
<meta property="og:image:width" content="1218">
<meta name="twitter:title" content="c_code_gen - Batch Normalization">
<meta name="twitter:description" content="Applying Activations, Gradients, and Batch Normalization">
<meta name="twitter:image" content="https://moneebullah25.github.io/c_code_gen/02_bn_files/figure-html/cell-13-output-2.png">
<meta name="twitter:image-height" content="372">
<meta name="twitter:image-width" content="1218">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">c_code_gen</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto">
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./bn.html">Batch Normalization</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">c_code_gen</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bigram.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bigram Language Model Character Level</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mlp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multilayer Perceptron</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bn.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Batch Normalization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./wavenet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Wavenet</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nano_gpt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nano GPT</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./uml_diagrams.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">UML Diagrams</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./huggingface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HuggingFace</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#starter-code-from-previous-mlp-part" id="toc-starter-code-from-previous-mlp-part" class="nav-link active" data-scroll-target="#starter-code-from-previous-mlp-part">1- Starter Code (from previous MLP part)</a></li>
  <li><a href="#fixing-the-initialization" id="toc-fixing-the-initialization" class="nav-link" data-scroll-target="#fixing-the-initialization">2- Fixing the Initialization</a></li>
  <li><a href="#fixing-the-saturated-tanh" id="toc-fixing-the-saturated-tanh" class="nav-link" data-scroll-target="#fixing-the-saturated-tanh">3- Fixing the saturated tanh</a>
  <ul class="collapse">
  <li><a href="#explaining-the-problem" id="toc-explaining-the-problem" class="nav-link" data-scroll-target="#explaining-the-problem">3.1 - Explaining The Problem</a></li>
  <li><a href="#solution" id="toc-solution" class="nav-link" data-scroll-target="#solution">3.2- Solution</a></li>
  </ul></li>
  <li><a href="#calculating-the-init-scale-kaiming-initialization" id="toc-calculating-the-init-scale-kaiming-initialization" class="nav-link" data-scroll-target="#calculating-the-init-scale-kaiming-initialization">4- Calculating the Init Scale: Kaiming Initialization</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">5- Batch Normalization</a>
  <ul class="collapse">
  <li><a href="#explaination" id="toc-explaination" class="nav-link" data-scroll-target="#explaination">5.1- Explaination</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">5.2- Sampling</a>
  <ul class="collapse">
  <li><a href="#it-can-be-done-during-training-without-additional-step" id="toc-it-can-be-done-during-training-without-additional-step" class="nav-link" data-scroll-target="#it-can-be-done-during-training-without-additional-step">5.2.1- It can be done during training (without additional step)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#pytorch-ifying-the-model" id="toc-pytorch-ifying-the-model" class="nav-link" data-scroll-target="#pytorch-ifying-the-model">6- PyTorch-ifying the model</a></li>
  <li><a href="#visualizations" id="toc-visualizations" class="nav-link" data-scroll-target="#visualizations">7- Visualizations</a>
  <ul class="collapse">
  <li><a href="#forward-pass-activations-stats" id="toc-forward-pass-activations-stats" class="nav-link" data-scroll-target="#forward-pass-activations-stats">7.1- Forward Pass Activations stats</a></li>
  <li><a href="#backward-pass-gradient-statistics" id="toc-backward-pass-gradient-statistics" class="nav-link" data-scroll-target="#backward-pass-gradient-statistics">7.2- Backward pass Gradient Statistics</a></li>
  <li><a href="#parameter-activation-and-gradient-statistics" id="toc-parameter-activation-and-gradient-statistics" class="nav-link" data-scroll-target="#parameter-activation-and-gradient-statistics">7.3- Parameter Activation and Gradient Statistics</a></li>
  <li><a href="#update-data-raito-over-time" id="toc-update-data-raito-over-time" class="nav-link" data-scroll-target="#update-data-raito-over-time">7.4- update: data raito over time</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/moneebullah25/c_code_gen/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Batch Normalization</h1>
</div>

<div>
  <div class="description">
    Applying Activations, Gradients, and Batch Normalization
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="starter-code-from-previous-mlp-part" class="level1">
<h1>1- Starter Code (from previous MLP part)</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> <span class="bu">open</span>(<span class="st">"names.txt"</span>, <span class="st">"r"</span>).read().splitlines()</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(words[:<span class="dv">8</span>])</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(words))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']
32033</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">""</span>.join(words))))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>stoi <span class="op">=</span> {s: i <span class="op">+</span> <span class="dv">1</span> <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>stoi[<span class="st">"."</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>itos <span class="op">=</span> {i: s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(stoi)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(itos)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}
27</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>block_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_dataset(words):  </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  X, Y <span class="op">=</span> [], []</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ch <span class="kw">in</span> w <span class="op">+</span> <span class="st">'.'</span>:</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>      ix <span class="op">=</span> stoi[ch]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>      X.append(context)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>      Y.append(ix)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>      context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>  X <span class="op">=</span> torch.tensor(X)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>  Y <span class="op">=</span> torch.tensor(Y)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(X.shape, Y.shape)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> X, Y</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>random.shuffle(words)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span><span class="op">*</span><span class="bu">len</span>(words))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(words))</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>Xtr, Ytr <span class="op">=</span> build_dataset(words[:n1])</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>Xdev, Ydev <span class="op">=</span> build_dataset(words[n1:n2])</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>Xte, Yte <span class="op">=</span> build_dataset(words[n2:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">10</span> </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>C  <span class="op">=</span> torch.randn((vocab_size, n_embd),generator<span class="op">=</span>g)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn((n_embd <span class="op">*</span> block_size, n_hidden),generator<span class="op">=</span>g)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> torch.randn(n_hidden,generator<span class="op">=</span>g)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),generator<span class="op">=</span>g) </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.randn(vocab_size,generator<span class="op">=</span>g) </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11897</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>lossi <span class="op">=</span> []</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(Xtr), (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># batch </span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix]</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward pass</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed characters into vector space</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    embcat <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)) <span class="co"># flatten</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer activation</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer </span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># cross-entropy loss</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># backward pass</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">100000</span> <span class="cf">else</span> <span class="fl">0.01</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">-=</span> lr <span class="op">*</span> p.grad</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># track stats</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> / </span><span class="sc">{</span>max_steps<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    lossi.append(loss.item())</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0 / 200000: 27.881732940673828</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    x,y <span class="op">=</span> {</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"train"</span>: (Xtr, Ytr),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"dev"</span>: (Xdev, Ydev),</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"test"</span>: (Xte, Yte),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    }[split]</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[x] <span class="co"># (N, block_size, n_embd)</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    embcat <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)) <span class="co"># (N, block_size * n_embd)</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># (N, n_hidden)</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># (N, n_hidden)</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (N, vocab_size)</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>split_loss(<span class="st">"train"</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>split_loss(<span class="st">"dev"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train loss: 25.167741775512695
dev loss: 25.140993118286133</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling from the model</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span> <span class="op">+</span> <span class="dv">10</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> []</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> block_size</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        emb <span class="op">=</span> C[torch.tensor([context])]</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> torch.tanh(emb.view(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">@</span> W1 <span class="op">+</span> b1)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> F.softmax(logits, dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        ix <span class="op">=</span> torch.multinomial(probs, num_samples <span class="op">=</span> <span class="dv">1</span>).item()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shift the Context Window</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        context <span class="op">=</span> context[<span class="dv">1</span>:] <span class="op">+</span> [ix]</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ix <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        out.append(ix)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">""</span>.join(itos[i] <span class="cf">for</span> i <span class="kw">in</span> out))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>rrnhvqbgcdyhshwjkfickkhgkbdnrzkhjkyikdixjyrnuodrewaesyxshvyycoewidtesjrhguyozvyycoewidtesjrhguyozvwucovwidfcvkhhjkdokhdnejkygkhojyhguyozvwucovwths
rrnhodyddddddddddddddddddddddddddddddddddddddddddddddddddddddxshvyycoes

rrnhvvyhvkrhxvymhjryhcyysewylsovyyhadiddxshvyyhjkyixfntraevqhccyhguyozvyyhjkdixjy

rrnhvvyhcyyshvyyhjkdoewidtesjrhguyozvwthslrhcgfyyoaddradiddadiddxshvyyhjkyixfkcoyhshwjkficbesrrmhokyicbesrrmhokyrnnxvymhjzigkyoadiddadiddadiddadiddaddrzvwthslrindiejoyhshodydzshvyyhdjrhguyozvwuhjryhcyryhcyysovyyhjkdixjyrhgtfxjkcynsrqhs
rrnhvqbgcdyhddrzvwthslrhcgfccamhjzigkyozvwthslkdokhdnejkyikkoon
hvqbgsewgmtiofvwovsvcvvyhcyysewgmtiofvyrhguyozvwucovwthslrhcgfyyoadiddxshvyyhjkdixjy

rrnhvvyhcyyshvyyhjkdixjyrnuxjrrhshojyhguyozvwthslkdaadjdyhddrzvwthslrhcgfccymgdoshvyyhjkdoewiekkoovwidtesjrhguyozvyyhouyhodydiddxshvyyhjkdixjyrnnxvymhjkdixjyhguyozvwthslkdaadjdyhddrzvwucovwixtyxfvwovsmcjkhgkbdnrzccvnhguyozvyycoewidtesjrhguyozvwthsewgmtiojyhguyozvwthslrhcgfycbyhidigdyxzvkhhjkdixjyrnuxjrrhshwjkhgkbdnrzothslkdjxvcyhguyozvyyhdyrhguyozvwucovwthslkdixjyrnuodrewaesyxshvyicbesyxshvyyhjkdixjyrhguyozvwuhjryhcyyshvyyhjkdixjy
rrnhvqbgcdyhdyrzucovwths
rrnhodydiejoyhshodydiejoyhshwjkfickkhgkbdnrzkhjciyfvyrhguyozvwuhjryhcyyszvyyhodydiedixjyrnuoduhjryhcyysewgmtiofvyrhuhrzkhjnfccyhguyozvyhcyyshvyyhodydiddxshvyyhjkdixjyrnuxjnhguyozvwucovwthslkdaadzdyxzvkhhxoyvyhcyysovyyhddrzvwthsewgmtiofvwovsvcvvkhhjkdgxyyrzucovwths
rrnhvqbgxyyrzjtyofwyxshvyyhjkyixfkcovwthsewylsonhovwixtyxfvwthslkdokhdnguhddrzvwuhjryhcyyszvwthslkdaadjdyhshfyyoadiddxshvyyhdyrzgthxjyrnnxvymhjzigkyojyhguyozvwthslkdaadjdyhddrewaesyxshvyyhddrzvwuhjryhcyyshvyyhjkygkhynguhddrzvwuhjryhcyyshvyyhjkyikkoon
rrnhodydiddxshvdixjyrnnvqhccyhguyozvwuhjryhcyysrnshxjyrnnxvymhjkyixfkkrnnxvymhjzigkyojyhguyozvwucovwthslrhcgfccyhguyozvwuhjryhcyyshvyyhodydiejoyhshwjkhgkbdnrzothslrhcgfccymgubdxshvyyhjkyixfkcoyhshwjkhgkbynrrhwvkhhjkyixfkcoyhshodydiejoyhshwjkhgkbdnrzothsewgmtiofvwows
rrnhodydiejoyhshojyhguyozvwuhjryhodydiediddadiddadiddadiddxshvyyhodydddddddhehozwidikdixjy
hvcyhguyozvwthslrhcgfccyhguyozvwths
rrnhvvyhcyyshvyyhodydiedaejqhgcbyhidikdixjyrnurzshvyyhjkigkyojvwucovwthslkdaadzdyxzvkhhjkyojyhguyozvwuhjryhcyyshvyycoesrrmhokyrnnxvymhjryhcyysewywseshwjkhgkbdnrzoahodydiejoyhshfyyoadiddadiddadiddxshvyyhjkdixjyrnnxvymhjzigdyxzvkhhjkdixjyrnnxvqrhshodyddddddddhshojyhguyozvyyhddrxjhthojyhguyozvwuhjryhcyysewgmtiofvyrhguyozvyyhodydiediediddxshvyyhjkyikkoon
rrnhodydiejoyhshvyyhjkdixjyrnuxjrrhshodydiejoyhshojyhguyozvwthslkdokhdnejkhgkbdnrzshvyyhjkdokhmldoewidtesjrhguyozvwths
rrnhvvyhcyryhcyysovyycoewiekkoovwixtyxfvwovsmcjrkcoyhshojyhguyozvwuhjryhcyysewgmtiofvwaesyxshvyyhodydddxshvyyhodydiediddxshvyyhjkhgkbdnrzkhjnfccyhguyozvwthslrhcvyhcywcvkhhcyysewylsouhozwidtesjrhgcbyhidikdixjyrhguyozvwthslkdaadzdyxzvkhhjkygkbdnrzothslkdixjy
</code></pre>
</div>
</div>
</section>
<section id="fixing-the-initialization" class="level1">
<h1>2- Fixing the Initialization</h1>
<p>we expect similar probability for each character = 1 / 27</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>expected_loss <span class="op">=</span> <span class="op">-</span> torch.tensor(<span class="dv">1</span> <span class="op">/</span> <span class="dv">27</span>).log()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(expected_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(3.2958)</code></pre>
</div>
</div>
<p>As you can see the previous approach to the problem was to randomnly initializes the weights and biases of the neural network which is a problem becuase our first few cycle of our training will entirely be dedicated to lowering the weights to point where neural network actually starts to learn the pattern.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer activation</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the above lines of code W1 and b1 are initialized randomnly so when hpreact is calculated and its activation is calculated using tanh function, the output of <code>h</code> tensor will most likely to be taking extreme values like 1 or -1.</p>
<p>Now if our <code>h</code> tensor is getting extreme values like 1 or -1, that neuron became dead and can’t longer take part in the learning process because during back propagation no loss will pass though that neuron. And the reason for that lies in the implementation of loss backpropgation function of the tanh:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> tanh(<span class="va">self</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.data</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>)<span class="op">/</span>(math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> Value(t, (<span class="va">self</span>, ), <span class="st">'tanh'</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _backward():</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.grad <span class="op">+=</span> (<span class="dv">1</span> <span class="op">-</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> out.grad</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    out._backward <span class="op">=</span> _backward</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the _backward() function that gradient is calculated by subtracting the square of the output of tanh from 1 and then multiplying it with the global gradient in the case <code>out.grad</code> in line <code>self.grad += (1 - t**2) * out.grad</code>. So if <code>h</code> tensor is getting extreme values like 1 or -1 the output will definately be <code>0 * out.grad</code> will will become 0 hence no loss will be propagated through the network and neural network can’t learn.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">10</span> </span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>C  <span class="op">=</span> torch.randn((vocab_size, n_embd),generator<span class="op">=</span>g)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn((n_embd <span class="op">*</span> block_size, n_hidden),generator<span class="op">=</span>g)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> torch.randn(n_hidden,generator<span class="op">=</span>g)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span> <span class="co"># Initialize to small values</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.randn(vocab_size,generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span> <span class="co"># Remove bias</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11897</code></pre>
</div>
</div>
<p>In neural network training, the phenomenon of dead neurons arises when the weights and biases are randomly initialized, leading to extreme values in the hidden layer activations. This issue primarily affects networks that utilize activation functions with saturation characteristics, such as the hyperbolic tangent (tanh) function.</p>
<p>Consider the following code snippet, where <code>W1</code> and <code>b1</code> are randomly initialized weights and biases, and <code>hpreact</code> represents the pre-activation of the hidden layer:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1  <span class="co"># hidden layer pre-activation</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> torch.tanh(hpreact)    <span class="co"># hidden layer activation using tanh</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>When <code>hpreact</code> is calculated and then passed through the tanh activation function, the resulting <code>h</code> tensor may take extreme values like 1 or -1. This situation can render neurons “dead” during training.</p>
<p>The tanh activation function is defined as follows:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">-</span> <span class="dv">1</span>) <span class="op">/</span> (math.exp(<span class="dv">2</span><span class="op">*</span>x) <span class="op">+</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the context of neural network training, the derivative of tanh is crucial for backpropagation. The backward pass for tanh involves multiplying the global gradient (<code>out.grad</code>) by the derivative of tanh, which is (1 - ^2(x)).</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh_backward(x, out_grad):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> tanh(x)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span> <span class="op">-</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> out_grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If the hidden layer activations (<code>h</code>) are consistently pushed to extreme values like 1 or -1, the gradient during backpropagation becomes close to zero. In the tanh activation’s derivative, the term (1 - ^2(x)) approaches zero for extreme values of (x), effectively nullifying the gradient.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Backward pass for tanh in the network</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh_backward_pass(x, out_grad):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> tanh(x)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">1</span> <span class="op">-</span> t<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> out_grad</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># During backpropagation</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>h_grad <span class="op">=</span> tanh_backward_pass(hpreact, global_gradient)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>W1_grad <span class="op">=</span> embcat.T <span class="op">@</span> h_grad</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>b1_grad <span class="op">=</span> torch.<span class="bu">sum</span>(h_grad, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(Xtr), (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># batch </span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix]</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># forward pass</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> C[Xb] <span class="co"># embed characters into vector space</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>embcat <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)) <span class="co"># flatten</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer activation</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer </span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># cross-entropy loss</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"initial loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>initial loss = 3.32205867767334</code></pre>
</div>
</div>
</section>
<section id="fixing-the-saturated-tanh" class="level1">
<h1>3- Fixing the saturated tanh</h1>
<p><code>h</code> has many values = 1 or -1, so the gradient is 0</p>
<section id="explaining-the-problem" class="level2">
<h2 class="anchored" data-anchor-id="explaining-the-problem">3.1 - Explaining The Problem</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].hist(hpreact.flatten().data, bins <span class="op">=</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"h pre-activation"</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].hist(h.flatten().data, bins <span class="op">=</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"h"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot(torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>), torch.tanh(torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)))<span class="op">;</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"tanh"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(0.5, 1.0, 'tanh')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-13-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>comment: There’re many <code>values = 1 or -1</code> in tanh because pre-activations with extreme values</p>
<p>➡️ changing the input doesn’t change the outputvalue much</p>
<p>➡️ the derivative of tanh is <code>1 - tanh^2</code></p>
<p>➡️ the gradient is 0 for <code>all values = 1 or -1</code></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(h.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.99</span>, cmap <span class="op">=</span> <span class="st">"gray"</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"h &gt; 0.99: white is True, black is False"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>comment: if a column is completely white, it’s a dead neuron</p>
</section>
<section id="solution" class="level2">
<h2 class="anchored" data-anchor-id="solution">3.2- Solution</h2>
<p>To address the issue of dead neurons, various initialization techniques exist, such as He initialization, which helps mitigate the saturation problem during the initial phases of training. He initialization scales the weights based on the number of input units to the neuron.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># He initialization for tanh activation</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn(input_size, hidden_size) <span class="op">*</span> math.sqrt(<span class="dv">2</span> <span class="op">/</span> input_size)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> torch.zeros(hidden_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Applying proper weight initialization techniques can significantly contribute to the alleviation of dead neurons and facilitate more effective training in neural networks.</p>
<p>Multiplying weights with small number calculated using <code>math.sqrt(2 / input_size)</code> is more systematic approach and it’s the recommended way but for the moment we can multiply weights with small value to mitigate this issue</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">10</span> </span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>C  <span class="op">=</span> torch.randn((vocab_size, n_embd),generator<span class="op">=</span>g)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn((n_embd <span class="op">*</span> block_size, n_hidden),generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.2</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> torch.randn(n_hidden,generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span> <span class="co"># Initialize to small values</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.randn(vocab_size,generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span> <span class="co"># Remove bias</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>11897</code></pre>
</div>
</div>
<p>Now using the above solution we are making sure that we are not wasting the first few cycles of our training to squash down our weights to the point where our neural network actually starts learning, instead we somehow get the idea what our initial loss will look like before training and initialize the weights of the neural network accordingly.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(Xtr), (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co"># batch </span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix]</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># forward pass</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> C[Xb] <span class="co"># embed characters into vector space</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>embcat <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)) <span class="co"># flatten</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># hidden layer pre-activation</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer activation</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer </span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># cross-entropy loss</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"initial loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>initial loss = 3.3134593963623047</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].hist(hpreact.flatten().data, bins <span class="op">=</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"h pre-activation"</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].hist(h.flatten().data, bins <span class="op">=</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"h"</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].plot(torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>), torch.tanh(torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)))<span class="op">;</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"tanh"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(0.5, 1.0, 'tanh')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-17-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>As we can see that most of our neurons in the neural network are active</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">20</span>, <span class="dv">10</span>))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(h.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.99</span>, cmap <span class="op">=</span> <span class="st">"gray"</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"h &gt; 0.99: white is True, black is False"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="calculating-the-init-scale-kaiming-initialization" class="level1">
<h1>4- Calculating the Init Scale: Kaiming Initialization</h1>
<p>Where did the numbers (0.01, 0.1, 0.2) come from in initializing the weights and biases?</p>
<p><strong>Kaiming Initialization</strong> is a weight initialization technique designed to address the vanishing/exploding gradient problem that can occur during the training of deep neural networks. It was introduced by Kaiming He et al.&nbsp;and is particularly effective when using activation functions like ReLU (Rectified Linear Unit). The initialization scales the weights based on the number of input units to the neuron, providing a good compromise between avoiding saturation and preventing explosion of gradients.</p>
<p>Now, let’s discuss the provided code and how Kaiming Initialization is applied:</p>
<p><strong>Original Initialization (<code>w</code>):</strong></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This initializes the weights <code>w</code> with random values. With mean close to 0 and standard deviation close to 1.</p>
<p><strong>Pre-activation (<code>y</code>):</strong></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">@</span> w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This calculates the pre-activation <code>y</code> by performing matrix multiplication between input <code>x</code> and weights <code>w</code>.With mean close to 0 and standard deviation close to 3. The relatively high standard deviation indicates that the spread of the pre-activation values is large. This is also expected when you multiply a random input matrix <code>x</code> with randomly initialized weights <code>w</code>. The weights contribute to the variability in the pre-activation values, leading to a higher standard deviation.</p>
<p><strong>Kaiming Initialization (<code>w_scaled</code>):</strong></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>w_scaled <span class="op">=</span> w <span class="op">/</span> math.sqrt(w.shape[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This scales the weights using Kaiming Initialization, dividing each weight by the square root of the number of input units (fan_in). In this case, <code>w.shape[0]</code> is 10.</p>
<p><strong>Scaled Pre-activation (<code>y_scaled</code>):</strong></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>y_scaled <span class="op">=</span> x <span class="op">@</span> w_scaled</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This calculates the pre-activation using the scaled weights.</p>
<p>By scaling the weights using Kaiming Initialization, the code aims to ensure that the pre-activation values remain within a reasonable range, preventing issues related to vanishing/exploding gradients and promoting stable training of the neural network. However due to modern innovations like Residual Networks, Normalization (Layer, Batch, Group), and Optimizers (Adam, RMSprop).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1000 training examples, 10 features</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>,<span class="dv">10</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 10 features, 200 neurons</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="co"># pre-activation</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">@</span> w</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x mean = </span><span class="sc">{</span>x<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">, std = </span><span class="sc">{</span>x<span class="sc">.</span>std()<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"y mean = </span><span class="sc">{</span>y<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">, std = </span><span class="sc">{</span>y<span class="sc">.</span>std()<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">20</span>, <span class="dv">5</span>))</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].hist(x.flatten().data, bins <span class="op">=</span> <span class="dv">50</span>, density <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xticks(torch.linspace(y.<span class="bu">min</span>(), y.<span class="bu">max</span>(), <span class="dv">10</span>))</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">"x original"</span>)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].hist(y.flatten().data, bins <span class="op">=</span> <span class="dv">50</span>, density <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xticks(torch.linspace(y.<span class="bu">min</span>(), y.<span class="bu">max</span>(), <span class="dv">10</span>))</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">"y original"</span>)<span class="op">;</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a><span class="co"># scale the weights by Kaiming Initialization: dividing by sqrt(fan_in) = sqrt(10) in this case</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>w_scaled <span class="op">=</span> w <span class="op">/</span>  math.sqrt(w.shape[<span class="dv">0</span>])</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>y_scaled <span class="op">=</span> x <span class="op">@</span> w_scaled</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].hist(y_scaled.flatten().data, bins <span class="op">=</span> <span class="dv">50</span>, density <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xticks(torch.linspace(y.<span class="bu">min</span>(), y.<span class="bu">max</span>(), <span class="dv">10</span>))</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">"y scaled"</span>)<span class="op">;</span></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"y scaled mean = </span><span class="sc">{</span>y_scaled<span class="sc">.</span>mean()<span class="sc">:.3f}</span><span class="ss">, std = </span><span class="sc">{</span>y_scaled<span class="sc">.</span>std()<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x mean = -0.002, std = 0.999
y mean = -0.005, std = 3.042
y scaled mean = -0.002, std = 0.962</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-19-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="batch-normalization" class="level1">
<h1>5- Batch Normalization</h1>
<p>We want to make pre-activations not extreme, but roughly a gaussian distribution (mean = 0, std = 1)</p>
<p>In the modern deep learning, Batch Normalization has emerged as a pivotal innovation, offering a departure from the traditional emphasis on meticulous weight initialization methods like Kaiming Initialization or scaling down weights during random initialization. This technique provides a means to dynamically normalize weights during the training process, mitigating issues related to internal covariate shift.</p>
<p>In the conventional paradigm, weights are typically initialized using techniques like Kaiming Initialization to ensure stable and effective training. However, as the network evolves during training, the distributions of activations in different layers can shift, leading to challenges in convergence. This is where Batch Normalization comes into play.</p>
<p>The fundamental principle behind Batch Normalization is to maintain normal distribution of weights within each layer. This is achieved by normalizing the pre-activation values using the mean and standard deviation computed across a mini-batch of data:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>hpreact <span class="op">=</span> (hpreact <span class="op">-</span> hpreact.mean(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)) <span class="op">/</span> (hpreact.std(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="fl">1e-6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>While Batch Normalization ensures normalized weights during forward passes, there is a crucial consideration: it may not be desirable to have normalized weights throughout the entire training process. This is due to the fact that normalization might impede the model’s ability to learn and adapt dynamically.</p>
<p>To address this concern, a dynamic scaling approach is introduced. Instead of uniformly normalizing weights during each forward pass, the weights are allowed to undergo dynamic shifts, scalings, down-scalings, or expansions based on the evolving needs of the training process. This is accomplished through the introduction of <code>bngain</code> (Batch Normalization Gain) and <code>bnbias</code> (Batch Normalization Bias) at the initialization of weights:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>bngain <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>bnbias <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2, bngain, bnbias]</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the training loop, the dynamic scaling is applied:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>hpreact <span class="op">=</span> (hpreact <span class="op">-</span> hpreact.mean(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)) <span class="op">/</span> (hpreact.std(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>hpreact <span class="op">=</span> hpreact <span class="op">*</span> bngain <span class="op">+</span> bnbias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This approach allows the neural network to learn the optimal scaling and shifting for its weights during the training process, adapting to the data distribution more flexibly. It strikes a balance between normalization and adaptability, enhancing the network’s ability to converge efficiently while avoiding the potential drawbacks of constant normalization.</p>
<section id="explaination" class="level2">
<h2 class="anchored" data-anchor-id="explaination">5.1- Explaination</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MLP revisited</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproducibility</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>C  <span class="op">=</span> torch.randn((vocab_size, n_embd),            generator<span class="op">=</span>g)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn((n_embd <span class="op">*</span> block_size, n_hidden), generator<span class="op">=</span>g) <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>)<span class="op">/</span>((n_embd <span class="op">*</span> block_size)<span class="op">**</span><span class="fl">0.5</span>) <span class="co">#* 0.2</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="co">#b1 = torch.randn(n_hidden,                        generator=g) * 0.01</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),          generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                      generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="co"># BatchNorm parameters</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>bngain <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>bnbias <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, W2, b2, bngain, bnbias]</span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>12097</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>lossi <span class="op">=</span> []</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward pass</span></span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>    embcat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear layer</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="co">#+ b1 # hidden layer pre-activation</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># BatchNorm layer</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------------------------------------------</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>    bnmeani <span class="op">=</span> hpreact.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>    bnstdi <span class="op">=</span> hpreact.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we want it to be gaussian only at initialization (not always), the Neural Network may need to change the it</span></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale and Shift: scale the normalized batch by a learnable parameter (gamma) and shift it by another learnable parameter (beta)</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> bngain <span class="op">*</span> (hpreact <span class="op">-</span> bnmeani) <span class="op">/</span> bnstdi <span class="op">+</span> bnbias</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------------------------------------------</span></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Non-linearity</span></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer</span></span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># backward pass</span></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update</span></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">100000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># track stats</span></span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>    lossi.append(loss.log10().item())</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      0/ 200000: 3.3239</code></pre>
</div>
</div>
</section>
<section id="sampling" class="level2">
<h2 class="anchored" data-anchor-id="sampling">5.2- Sampling</h2>
<p>How to forward a single example to the model and get a prediction, even the model uses the mean ans std of the batch to normalize the input</p>
<p>We need a step after training, and find the mean and std of the whole training set</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calibrate the batch norm statistics</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[Xtr]</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    embcat <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    bnmean <span class="op">=</span> hpreact.mean(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    bnstd <span class="op">=</span> hpreact.std(dim<span class="op">=</span><span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    x,y <span class="op">=</span> {</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"train"</span>: (Xtr, Ytr),</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"dev"</span>: (Xdev, Ydev),</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"test"</span>: (Xte, Yte),</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    }[split]</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[x] <span class="co"># (N, block_size, n_embd)</span></span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a>    embcat <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)) <span class="co"># (N, block_size * n_embd)</span></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># (N, n_hidden)</span></span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># batch norm in test mode</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> (hpreact <span class="op">-</span> bnmean) <span class="op">/</span> (bnstd <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> hpreact <span class="op">*</span> bngain <span class="op">+</span> bnbias</span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># (N, n_hidden)</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (N, vocab_size)</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>split_loss(<span class="st">"train"</span>)</span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a>split_loss(<span class="st">"dev"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train loss: 3.262329339981079
dev loss: 3.2617669105529785</code></pre>
</div>
</div>
<section id="it-can-be-done-during-training-without-additional-step" class="level3">
<h3 class="anchored" data-anchor-id="it-can-be-done-during-training-without-additional-step">5.2.1- It can be done during training (without additional step)</h3>
<p>Now with the above <code>bngain</code> and <code>bnbias</code> approach there is issue that we are about the draw the sample after training we had also to do extra step</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># batch norm in test mode</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> (hpreact <span class="op">-</span> bnmean) <span class="op">/</span> (bnstd <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> hpreact <span class="op">*</span> bngain <span class="op">+</span> bnbias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To solve this we could initialize the running mean and running standard deviation. And manually calculate the running mean and running standard deviation during training</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    bnmean_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnmean_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnmeani</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    bnstd_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnstd_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnstdi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Using the above approach we eventually have <code>bnmean_running</code> should equal to <code>bnmean</code> after training, same for <code>bnstd_running</code> and <code>bnstd</code>. And after the training we could use the <code>bnmean_running</code> and <code>bnstd_running</code> to draw sample</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># batch norm in test mode</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> (hpreact <span class="op">-</span> bnmean_running) <span class="op">/</span> (bnstd_running <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> hpreact <span class="op">*</span> bngain <span class="op">+</span> bnbias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And another thing to notice with the running mean and running standard deviation approach is that now we don’t need to calculate the <code>b1</code>, because it wouldn’t have effect on <code>bnmeani</code> and <code>bnstdi</code> when calculating the <code>hpreact</code> in the training loop.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MLP revisited</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">10</span> <span class="co"># the dimensionality of the character embedding vectors</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># the number of neurons in the hidden layer of the MLP</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>) <span class="co"># for reproducibility</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>C  <span class="op">=</span> torch.randn((vocab_size, n_embd),            generator<span class="op">=</span>g)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn((n_embd <span class="op">*</span> block_size, n_hidden), generator<span class="op">=</span>g) <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>)<span class="op">/</span>((n_embd <span class="op">*</span> block_size)<span class="op">**</span><span class="fl">0.5</span>) <span class="co">#* 0.2</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="co">#b1 = torch.randn(n_hidden,                        generator=g) * 0.01</span></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn((n_hidden, vocab_size),          generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.randn(vocab_size,                      generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span></span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="co"># BatchNorm parameters</span></span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>bngain <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>bnbias <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>bnmean_running <span class="op">=</span> torch.zeros((<span class="dv">1</span>, n_hidden))</span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>bnstd_running <span class="op">=</span> torch.ones((<span class="dv">1</span>, n_hidden))</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, W2, b2, bngain, bnbias]</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters)) <span class="co"># number of parameters in total</span></span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a>  p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>12097</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># same optimization as last time</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>lossi <span class="op">=</span> []</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># minibatch construct</span></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix] <span class="co"># batch X,Y</span></span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward pass</span></span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed the characters into vectors</span></span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>    embcat <span class="op">=</span> emb.view(emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>) <span class="co"># concatenate the vectors</span></span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear layer</span></span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="co">#+ b1 # hidden layer pre-activation</span></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># BatchNorm layer</span></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------------------------------------------</span></span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a>    bnmeani <span class="op">=</span> hpreact.mean(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a>    bnstdi <span class="op">=</span> hpreact.std(<span class="dv">0</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> bngain <span class="op">*</span> (hpreact <span class="op">-</span> bnmeani) <span class="op">/</span> bnstdi <span class="op">+</span> bnbias</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a>        bnmean_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnmean_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnmeani</span>
<span id="cb60-24"><a href="#cb60-24" aria-hidden="true" tabindex="-1"></a>        bnstd_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnstd_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnstdi</span>
<span id="cb60-25"><a href="#cb60-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># -------------------------------------------------------------</span></span>
<span id="cb60-26"><a href="#cb60-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Non-linearity</span></span>
<span id="cb60-27"><a href="#cb60-27" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer</span></span>
<span id="cb60-28"><a href="#cb60-28" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb60-29"><a href="#cb60-29" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb60-30"><a href="#cb60-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-31"><a href="#cb60-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># backward pass</span></span>
<span id="cb60-32"><a href="#cb60-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb60-33"><a href="#cb60-33" aria-hidden="true" tabindex="-1"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb60-34"><a href="#cb60-34" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb60-35"><a href="#cb60-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-36"><a href="#cb60-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update</span></span>
<span id="cb60-37"><a href="#cb60-37" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">100000</span> <span class="cf">else</span> <span class="fl">0.01</span> <span class="co"># step learning rate decay</span></span>
<span id="cb60-38"><a href="#cb60-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb60-39"><a href="#cb60-39" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr <span class="op">*</span> p.grad</span>
<span id="cb60-40"><a href="#cb60-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-41"><a href="#cb60-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># track stats</span></span>
<span id="cb60-42"><a href="#cb60-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>: <span class="co"># print every once in a while</span></span>
<span id="cb60-43"><a href="#cb60-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss">/</span><span class="sc">{</span>max_steps<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb60-44"><a href="#cb60-44" aria-hidden="true" tabindex="-1"></a>    lossi.append(loss.log10().item())</span>
<span id="cb60-45"><a href="#cb60-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      0/ 200000: 3.3239</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_loss(split):</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    x,y <span class="op">=</span> {</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"train"</span>: (Xtr, Ytr),</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"dev"</span>: (Xdev, Ydev),</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"test"</span>: (Xte, Yte),</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    }[split]</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[x] <span class="co"># (N, block_size, n_embd)</span></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>    embcat <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)) <span class="co"># (N, block_size * n_embd)</span></span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> embcat <span class="op">@</span> W1 <span class="op">+</span> b1 <span class="co"># (N, n_hidden)</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># batch norm in test mode</span></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> (hpreact <span class="op">-</span> bnmean_running) <span class="op">/</span> (bnstd_running <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> hpreact <span class="op">*</span> bngain <span class="op">+</span> bnbias</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># (N, n_hidden)</span></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># (N, vocab_size)</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss"> loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>split_loss(<span class="st">"train"</span>)</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>split_loss(<span class="st">"dev"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train loss: 3.270005226135254
dev loss: 3.269122362136841</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="pytorch-ifying-the-model" class="level1">
<h1>6- PyTorch-ifying the model</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># visualization parameters (play with these)</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>gain <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>last_layer_confidence <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>using_batch_norm <span class="op">=</span> <span class="va">True</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>div_by_fan_in <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear:</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fan_in, fan_out, bias <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> torch.randn((fan_in, fan_out), generator <span class="op">=</span> g)</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> div_by_fan_in:</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.weight <span class="op">/=</span> fan_in <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros((fan_out)) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> x <span class="op">@</span> <span class="va">self</span>.weight</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.out <span class="op">+=</span> <span class="va">self</span>.bias</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.weight] <span class="op">+</span> ([] <span class="cf">if</span> <span class="va">self</span>.bias <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> [<span class="va">self</span>.bias])</span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BatchNorm1d:</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span></span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># parameters (trained with backprop)</span></span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># buffers (trained while running `momentum update`)</span></span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.running_mean <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.running_var <span class="op">=</span> torch.ones(dim)</span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb65-33"><a href="#cb65-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb65-34"><a href="#cb65-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># batch mean</span></span>
<span id="cb65-35"><a href="#cb65-35" aria-hidden="true" tabindex="-1"></a>            xmean <span class="op">=</span> x.mean(<span class="dv">0</span>, keepdim<span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb65-36"><a href="#cb65-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># batch variance</span></span>
<span id="cb65-37"><a href="#cb65-37" aria-hidden="true" tabindex="-1"></a>            xvar <span class="op">=</span> x.var(<span class="dv">0</span>, keepdim<span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb65-38"><a href="#cb65-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb65-39"><a href="#cb65-39" aria-hidden="true" tabindex="-1"></a>            xmean <span class="op">=</span> <span class="va">self</span>.running_mean</span>
<span id="cb65-40"><a href="#cb65-40" aria-hidden="true" tabindex="-1"></a>            xvar <span class="op">=</span> <span class="va">self</span>.running_var</span>
<span id="cb65-41"><a href="#cb65-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-42"><a href="#cb65-42" aria-hidden="true" tabindex="-1"></a>        xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb65-43"><a href="#cb65-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb65-44"><a href="#cb65-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-45"><a href="#cb65-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update the buffers in training</span></span>
<span id="cb65-46"><a href="#cb65-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb65-47"><a href="#cb65-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb65-48"><a href="#cb65-48" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.running_mean <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_mean <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xmean</span>
<span id="cb65-49"><a href="#cb65-49" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.running_var <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum) <span class="op">*</span> <span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.momentum <span class="op">*</span> xvar</span>
<span id="cb65-50"><a href="#cb65-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb65-51"><a href="#cb65-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb65-52"><a href="#cb65-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-53"><a href="#cb65-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb65-54"><a href="#cb65-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb65-55"><a href="#cb65-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-56"><a href="#cb65-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-57"><a href="#cb65-57" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Tanh:</span>
<span id="cb65-58"><a href="#cb65-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb65-59"><a href="#cb65-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> torch.tanh(x)</span>
<span id="cb65-60"><a href="#cb65-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb65-61"><a href="#cb65-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-62"><a href="#cb65-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb65-63"><a href="#cb65-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> []</span>
<span id="cb65-64"><a href="#cb65-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-65"><a href="#cb65-65" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb65-66"><a href="#cb65-66" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb65-67"><a href="#cb65-67" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">2147483647</span>)</span>
<span id="cb65-68"><a href="#cb65-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-69"><a href="#cb65-69" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.randn((vocab_size, n_embd),       generator <span class="op">=</span> g)</span>
<span id="cb65-70"><a href="#cb65-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-71"><a href="#cb65-71" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> using_batch_norm:</span>
<span id="cb65-72"><a href="#cb65-72" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [</span>
<span id="cb65-73"><a href="#cb65-73" aria-hidden="true" tabindex="-1"></a>        Linear(n_embd <span class="op">*</span> block_size, n_hidden), Tanh(),</span>
<span id="cb65-74"><a href="#cb65-74" aria-hidden="true" tabindex="-1"></a>        Linear(         n_hidden, n_hidden), Tanh(),</span>
<span id="cb65-75"><a href="#cb65-75" aria-hidden="true" tabindex="-1"></a>        Linear(         n_hidden, n_hidden), Tanh(),</span>
<span id="cb65-76"><a href="#cb65-76" aria-hidden="true" tabindex="-1"></a>        Linear(         n_hidden, n_hidden), Tanh(),</span>
<span id="cb65-77"><a href="#cb65-77" aria-hidden="true" tabindex="-1"></a>        Linear(         n_hidden, n_hidden), Tanh(),</span>
<span id="cb65-78"><a href="#cb65-78" aria-hidden="true" tabindex="-1"></a>        Linear(         n_hidden, vocab_size)</span>
<span id="cb65-79"><a href="#cb65-79" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb65-80"><a href="#cb65-80" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> using_batch_norm:</span>
<span id="cb65-81"><a href="#cb65-81" aria-hidden="true" tabindex="-1"></a>    layers <span class="op">=</span> [</span>
<span id="cb65-82"><a href="#cb65-82" aria-hidden="true" tabindex="-1"></a>    Linear(n_embd <span class="op">*</span> block_size, n_hidden, bias <span class="op">=</span> <span class="va">False</span>), BatchNorm1d(n_hidden) ,Tanh(),</span>
<span id="cb65-83"><a href="#cb65-83" aria-hidden="true" tabindex="-1"></a>    Linear(         n_hidden, n_hidden, bias <span class="op">=</span> <span class="va">False</span>), BatchNorm1d(n_hidden) ,Tanh(),</span>
<span id="cb65-84"><a href="#cb65-84" aria-hidden="true" tabindex="-1"></a>    Linear(         n_hidden, n_hidden, bias <span class="op">=</span> <span class="va">False</span>), BatchNorm1d(n_hidden) ,Tanh(),</span>
<span id="cb65-85"><a href="#cb65-85" aria-hidden="true" tabindex="-1"></a>    Linear(         n_hidden, n_hidden, bias <span class="op">=</span> <span class="va">False</span>), BatchNorm1d(n_hidden) ,Tanh(),</span>
<span id="cb65-86"><a href="#cb65-86" aria-hidden="true" tabindex="-1"></a>    Linear(         n_hidden, n_hidden, bias <span class="op">=</span> <span class="va">False</span>), BatchNorm1d(n_hidden) ,Tanh(),</span>
<span id="cb65-87"><a href="#cb65-87" aria-hidden="true" tabindex="-1"></a>    Linear(         n_hidden, vocab_size, bias <span class="op">=</span> <span class="va">False</span>), BatchNorm1d(vocab_size) <span class="co"># you can add it here too</span></span>
<span id="cb65-88"><a href="#cb65-88" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb65-89"><a href="#cb65-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-90"><a href="#cb65-90" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb65-91"><a href="#cb65-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># last layer: make less confident</span></span>
<span id="cb65-92"><a href="#cb65-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> using_batch_norm:</span>
<span id="cb65-93"><a href="#cb65-93" aria-hidden="true" tabindex="-1"></a>        layers[<span class="op">-</span><span class="dv">1</span>].weight <span class="op">*=</span> last_layer_confidence</span>
<span id="cb65-94"><a href="#cb65-94" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> using_batch_norm:</span>
<span id="cb65-95"><a href="#cb65-95" aria-hidden="true" tabindex="-1"></a>        layers[<span class="op">-</span><span class="dv">1</span>].gamma <span class="op">*=</span> last_layer_confidence</span>
<span id="cb65-96"><a href="#cb65-96" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-97"><a href="#cb65-97" aria-hidden="true" tabindex="-1"></a>    <span class="co"># other layers: apply gain</span></span>
<span id="cb65-98"><a href="#cb65-98" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb65-99"><a href="#cb65-99" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Linear):</span>
<span id="cb65-100"><a href="#cb65-100" aria-hidden="true" tabindex="-1"></a>            <span class="co"># change this gain (default 5/3)</span></span>
<span id="cb65-101"><a href="#cb65-101" aria-hidden="true" tabindex="-1"></a>            layer.weight <span class="op">*=</span> gain</span>
<span id="cb65-102"><a href="#cb65-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-103"><a href="#cb65-103" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C] <span class="op">+</span> [p <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">for</span> p <span class="kw">in</span> layer.parameters()]</span>
<span id="cb65-104"><a href="#cb65-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span>
<span id="cb65-105"><a href="#cb65-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-106"><a href="#cb65-106" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb65-107"><a href="#cb65-107" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>47024</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># same optimization as last time</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>max_steps <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>lossi <span class="op">=</span> []</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>update_to_data_ratio <span class="op">=</span> []</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_steps):</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, Xtr.shape[<span class="dv">0</span>], (batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    Xb, Yb <span class="op">=</span> Xtr[ix], Ytr[ix]</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward pass</span></span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    emb <span class="op">=</span> C[Xb] <span class="co"># embed characters into vector space</span></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> emb.view((emb.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)) <span class="co"># flatten</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> layer(x)</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute loss</span></span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(x, Yb)</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># backward pass</span></span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layers:</span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>        layer.out.retain_grad()</span>
<span id="cb67-22"><a href="#cb67-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb67-23"><a href="#cb67-23" aria-hidden="true" tabindex="-1"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-24"><a href="#cb67-24" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb67-25"><a href="#cb67-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-26"><a href="#cb67-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update</span></span>
<span id="cb67-27"><a href="#cb67-27" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="fl">0.1</span> <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">10000</span> <span class="cf">else</span> <span class="fl">0.01</span></span>
<span id="cb67-28"><a href="#cb67-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb67-29"><a href="#cb67-29" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">-=</span> lr <span class="op">*</span> p.grad</span>
<span id="cb67-30"><a href="#cb67-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-31"><a href="#cb67-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># track stats</span></span>
<span id="cb67-32"><a href="#cb67-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb67-33"><a href="#cb67-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> loss </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-34"><a href="#cb67-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-35"><a href="#cb67-35" aria-hidden="true" tabindex="-1"></a>    lossi.append(loss.item())</span>
<span id="cb67-36"><a href="#cb67-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-37"><a href="#cb67-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># update to data ratio: How great is the update compared to the data itself?</span></span>
<span id="cb67-38"><a href="#cb67-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb67-39"><a href="#cb67-39" aria-hidden="true" tabindex="-1"></a>        update_to_data_ratio.append([(lr <span class="op">*</span> p.grad.std() <span class="op">/</span> p.data.std()).log10().item() <span class="cf">for</span> p <span class="kw">in</span> parameters <span class="cf">if</span> p.grad <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>])</span>
<span id="cb67-40"><a href="#cb67-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-41"><a href="#cb67-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">1000</span>:</span>
<span id="cb67-42"><a href="#cb67-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>step 0 loss 3.287036418914795</code></pre>
</div>
</div>
</section>
<section id="visualizations" class="level1">
<h1>7- Visualizations</h1>
<section id="forward-pass-activations-stats" class="level2">
<h2 class="anchored" data-anchor-id="forward-pass-activations-stats">7.1- Forward Pass Activations stats</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>legends <span class="op">=</span> []</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(layers[:<span class="op">-</span><span class="dv">1</span>]):</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Tanh):</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> layer.out</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> mean </span><span class="sc">{</span>t<span class="sc">.</span>mean()<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss"> std </span><span class="sc">{</span>t<span class="sc">.</span>std()<span class="sc">.</span>item()<span class="sc">:.2f}</span><span class="ss"> saturated </span><span class="sc">{</span>((t.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.97</span>).<span class="bu">float</span>().mean().item()) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>        hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>        plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>        legends.append(<span class="ss">f"layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>layer<span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>plt.legend(legends)</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"activation distribution"</span>)</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>layer 2 mean -0.00 std 0.64 saturated 2.56%
layer 5 mean 0.00 std 0.66 saturated 2.00%
layer 8 mean 0.00 std 0.66 saturated 1.47%
layer 11 mean 0.00 std 0.66 saturated 1.22%
layer 14 mean -0.00 std 0.66 saturated 0.72%</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-30-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="backward-pass-gradient-statistics" class="level2">
<h2 class="anchored" data-anchor-id="backward-pass-gradient-statistics">7.2- Backward pass Gradient Statistics</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>legends <span class="op">=</span> []</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(layers[:<span class="op">-</span><span class="dv">1</span>]):</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(layer, Tanh):</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> layer.out.grad</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> mean </span><span class="sc">{</span>t<span class="sc">.</span>mean()<span class="sc">.</span>item()<span class="sc">:.5f}</span><span class="ss"> std </span><span class="sc">{</span>t<span class="sc">.</span>std()<span class="sc">.</span>item()<span class="sc">:.5f}</span><span class="ss"> saturated </span><span class="sc">{</span>((t.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.97</span>).<span class="bu">float</span>().mean().item()) <span class="op">*</span> <span class="dv">100</span><span class="sc">:.8f}</span><span class="ss">%"</span>)</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>        hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>        plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        legends.append(<span class="ss">f"layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>layer<span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>plt.legend(legends)</span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"gradient distribution"</span>)</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>layer 2 mean -0.00000 std 0.00125 saturated 0.00000000%
layer 5 mean -0.00000 std 0.00110 saturated 0.00000000%
layer 8 mean -0.00000 std 0.00095 saturated 0.00000000%
layer 11 mean 0.00000 std 0.00094 saturated 0.00000000%
layer 14 mean -0.00000 std 0.00105 saturated 0.00000000%</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-31-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>comment: Gradients are not vanishing or exploding</p>
</section>
<section id="parameter-activation-and-gradient-statistics" class="level2">
<h2 class="anchored" data-anchor-id="parameter-activation-and-gradient-statistics">7.3- Parameter Activation and Gradient Statistics</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>legends <span class="op">=</span> []</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,p <span class="kw">in</span> <span class="bu">enumerate</span>(parameters):</span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> p.grad</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># neglect bias and batch norm parameters (weights only)</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"weight shape </span><span class="sc">{</span><span class="bu">tuple</span>(p.shape)<span class="sc">}</span><span class="ss"> | mean </span><span class="sc">{</span>t<span class="sc">.</span>mean()<span class="sc">.</span>item()<span class="sc">:.5f}</span><span class="ss"> | std </span><span class="sc">{</span>t<span class="sc">.</span>std()<span class="sc">.</span>item()<span class="sc">:.5f}</span><span class="ss"> | grad:data ratio </span><span class="sc">{</span>t<span class="sc">.</span>std() <span class="op">/</span> p<span class="sc">.</span>std()<span class="sc">:.2e}</span><span class="ss">"</span>)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>        hy, hx <span class="op">=</span> torch.histogram(t, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>        plt.plot(hx[:<span class="op">-</span><span class="dv">1</span>].detach(), hy.detach())</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>        legends.append(<span class="ss">f"weight </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>p<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>plt.legend(legends)</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Weights Gradient distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>weight shape (27, 10) | mean 0.00000 | std 0.00737 | grad:data ratio 7.37e-03
weight shape (30, 100) | mean 0.00031 | std 0.01392 | grad:data ratio 2.29e-01
weight shape (100, 100) | mean 0.00003 | std 0.00601 | grad:data ratio 2.01e-01
weight shape (100, 100) | mean -0.00002 | std 0.00522 | grad:data ratio 1.91e-01
weight shape (100, 100) | mean -0.00005 | std 0.00506 | grad:data ratio 1.92e-01
weight shape (100, 100) | mean -0.00011 | std 0.00522 | grad:data ratio 2.00e-01
weight shape (100, 27) | mean 0.00002 | std 0.01235 | grad:data ratio 2.86e-01</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Text(0.5, 1.0, 'Weights Gradient distribution')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-32-output-3.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="update-data-raito-over-time" class="level2">
<h2 class="anchored" data-anchor-id="update-data-raito-over-time">7.4- update: data raito over time</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>legends <span class="op">=</span> []</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(parameters):</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> p</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># neglect bias and batch norm parameters (weights only)</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p.ndim <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>        plt.plot([update_to_data_ratio[j][i] <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(update_to_data_ratio))])</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        legends.append(<span class="ss">f"weight </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a><span class="co"># rough guide for what it should be: 1e-3</span></span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="dv">0</span>, <span class="bu">len</span>(update_to_data_ratio)], [<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">3</span>], color<span class="op">=</span><span class="st">"black"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>plt.legend(legends)</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Update to Data ratio"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Text(0.5, 1.0, 'Update to Data ratio')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="02_bn_files/figure-html/cell-33-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Conclusion: Batchnorm makes the model more robust</p>
<ul>
<li>Changing the gain (when using batchnorm) doesn’t change too much except the Update to Data Ratio</li>
<li>Removing fan_it doesn’t change much too (when using batchnorm)</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "c_code_gen",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "c_code_gen",
    "section": "Install",
    "text": "Install\npip install c_code_gen"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "c_code_gen",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\nimport torch\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n    src_pad_idx = 0 # index of the padding token in source vocabulary\n    trg_pad_idx = 0 # index of the padding token in target vocabulary\n    src_vocab_size = 10 # number of unique tokens in source vocabulary\n    trg_vocab_size = 10 # number of unique tokens in target vocabulary\n    \n    print(f\"Input shape: {x.shape}\")\n    print(f\"Target shape: {trg.shape}\")\n    print(f\"Device available: {device}\")\n    \n    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n    out = model(x, trg[:, :-1])\n    print(f\"Output shape: {out.shape}\")\n    print(f\"Output: {out}\")\n\nInput shape: torch.Size([2, 9])\nTarget shape: torch.Size([2, 8])\nDevice available: cuda\nOutput shape: torch.Size([2, 7, 10])\nOutput: tensor([[[ 0.4464, -0.5610, -0.7114,  0.9363,  0.6822, -1.5460, -0.5701,\n          -0.0960,  0.4808,  0.1313],\n         [-0.7031,  0.6683, -0.6762, -0.0794, -0.2506, -0.9622, -0.1848,\n          -0.7650, -0.2054, -0.5386],\n         [-0.0112,  0.4172, -0.1490, -1.0593,  0.2641, -0.8530, -0.3859,\n          -0.3926, -0.3144, -0.1417],\n         [ 0.4123,  0.3738,  0.6268,  0.8212,  1.1357, -1.1602, -0.0434,\n          -1.7120,  0.1721, -0.5142],\n         [-0.5740,  0.6748,  0.4170,  1.0975, -0.0173, -0.5885, -1.8482,\n           0.1379,  0.7698, -0.3862],\n         [ 0.6030, -0.1450, -0.4451,  1.1064,  0.1838, -1.0696, -0.4320,\n           0.0764,  0.5091, -0.2963],\n         [ 0.0264,  0.1590, -0.4393,  0.9079,  0.7149, -1.4549,  0.1765,\n           0.3150,  0.3267, -0.9601]],\n\n        [[ 0.5317, -0.5054, -0.6930,  0.9477,  0.7169, -1.3674, -0.5864,\n          -0.1622,  0.5145,  0.1502],\n         [-0.5181,  1.1593, -0.3028,  0.6865, -0.1220, -0.7017, -1.0549,\n          -0.4249, -0.0154, -0.7563],\n         [ 0.0823,  0.9191, -0.1109,  0.1114,  0.0602, -1.0653, -0.8787,\n           0.1198, -0.4894, -0.1040],\n         [ 0.1353,  0.4007,  0.1736,  0.0703,  1.2294, -1.2375, -0.2426,\n          -1.0955,  0.2159, -0.7532],\n         [ 0.1648,  0.3638, -0.1407, -0.5300,  0.3209, -0.3451, -1.0195,\n           0.1148,  0.8064,  0.1274],\n         [-0.6308,  0.8116, -0.6778,  0.9686,  0.0346, -0.8795, -0.4404,\n          -0.3469,  0.3887, -0.2115],\n         [ 0.1550,  0.5274, -0.3766,  0.5200, -0.2350, -1.2167, -0.4607,\n           0.2098, -0.1927, -0.5351]]], device='cuda:0',\n       grad_fn=&lt;ViewBackward0&gt;)"
  },
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "This is the diagram of the Transformer network presented in the Attention is All You Need paper. We will go through all the different pieces of this network throughout this notebook."
  },
  {
    "objectID": "transformers.html#transformers-explained-step-by-step",
    "href": "transformers.html#transformers-explained-step-by-step",
    "title": "Transformers",
    "section": "Transformers Explained Step by Step",
    "text": "Transformers Explained Step by Step\n\nTokenization\nThe first step in processing text is to cut it into pieces called tokens. There are many variations of how to do it, and we won’t go into details, but BERT uses WordPiece tokenization. This means that tokens correspond roughly to words and punctuation, although a word can also be split into several tokens if it contains a common prefix or suffix. These are called sub-word tokens and usually contain ## characters. Words can even be spelled out if they have never been seen before. \n\n\n\n\n\nEmbedding\nThe second step is to associate each token with an embedding, which is nothing more than a vector of real numbers. Again, there are many ways to create embedding vectors. Fortunately, already trained embeddings are often provided by research groups, and we can just use an existing dictionary to convert the WordPiece tokens into embedding vectors. \n\n\n\n The embedding of tokens into vectors is an achievement in itself. The values inside an embedding carry information about the meaning of the token, but they are also arranged in such a way that one can perform mathematical operations on them, which correspond to semantic changes, like changing the gender of a noun, or the tense of a verb, or even the homeland of a city.\n\n\n\n\n\nContext\nHowever, embeddings are associated with tokens by a straight dictionary look-up, which means that the same token always gets the same embedding, regardless of its context. This is where the attention mechanism comes in, and specifically for BERT, the scaled dot-product self-attention. Attention transforms the default embeddings by analyzing the whole sequence of tokens, so that the values are more representative of the token they represent in the context of the sentence.\n\n\n\n\n\nSelf Attention Mechanism\nLet’s have a look at this process with the sequence of tokens walk, by, river, bank. Each token is initially replaced by its default embedding, which in this case is a vector with 768 components. \n\n\n\n\nLet’s color the embedding of the first token to follow what happens to it. We start by calculating the scalar product between pairs of embeddings. Here we have the first embedding with itself. When the two vectors are more correlated, or aligned, meaning that they are generally more similar, the scalar product is higher (darker in image), and we consider that they have a strong relationship. If they had less similar content, the scalar product would be lower (brighter in the image) and we would consider that they don’t relate to each other.\n\n\n\n\n\nWe go on and calculate the scalar product for every possible pair of embedding vectors in the input sequence. The values obtained are usually scaled down to avoid getting large values, which improves the numerical behavior. That’s done here by dividing by the square root of 768, which is the size of the vectors. \n\n\n\n\nThen comes the only non-linear operation in the attention mechanism: The scaled values are passed through a softmax activation function, by groups corresponding to each input token. So in this illustration, we apply the softmax column by column. What the softmax does is to exponentially amplify large values, while crushing low and negative values towards zero. It also does normalization, so that each column sums up to 1.\n\n\n\n\n\nFinally, we create a new embedding vector for each token by linear combination of the input embeddings, in proportions given by the softmax results. We can say that the new embedding vectors are contextualized, since they contain a fraction of every input embedding for this particular sequence of tokens. In particular, if a token has a strong relationship with another one, a large fraction of its new contextualized embedding will be made of the related embedding. If a token doesn’t relate much to any other, as measured by the scalar product between their input embeddings, its contextualized embedding will be nearly identical to the input embedding.\n\n\n\n\n\nFor instance, one can imagine that the vector space has a direction that corresponds to the idea of nature. The input embeddings of the tokens river and bank should both have large values in that direction, so that they are more similar and have a strong relationship. As a result, the new contextualized embeddings of the river and bank tokens would combine both input embeddings in roughly equal parts. On the other hand, the preposition by sounds quite neutral, so that its embedding should have a weak relationship with every other one and little modification of its embedding vector would occur. So there we have the mechanism that lets the scaled dot-product attention utilize context.\nTo recap: 1. First, it determines how much the input embedding vectors relate to each other using the scalar product. 2. The results are then scaled down, and the softmax activation function is applied, which normalizes these results in a non-linear way. 3. New contextualized embeddings are finally created for every token by linear combination of all the input embeddings, using the softmax proportions as coefficients \n\n\n\n\n\n\nKeys, Queries and Values\nHowever, that’s not the whole story. Most importantly, we don’t have to use the input embedding vectors as is. We can first project them using 3 linear projections to create the so-called Key, Query, and Value vectors. Typically, the projections are also mapping the input embeddings onto a space of lower dimension. In the case of BERT, the Key, Query, and Value vectors all have 64 components. \n\n\n\n\nEach projection can be thought of as focusing on different directions of the vector space, which would represent different semantic aspects. One can imagine that a Key is the projection of an embedding onto the direction of “prepositions”, and a Query is the projection of an embedding along the direction of “locations”. In this case, the Key of the token by should have a strong relationship with every other Query, since by should have strong components in the direction of “prepositions”, and every other token should have strong components in the direction of “locations”. The Values can come from yet another projection that is relevant, for example the direction of physical places. It’s these values that are combined to create the contextualized embeddings In practice, the meaning of each projection may not be so clear, and the model is free to learn whatever projections allow it to solve language tasks the most efficiently.\n\n\nMulti-head Attention\nIn addition, the same process can be repeated many times with different Key, Query, and Value projections, forming what is called a multi-head attention. Each head can focus on different projections of the input embeddings. For instance, one head could calculate the preposition/location relationships, while another head could calculate subject/verb relationships, simply by using different projections to create the Key, Query, and Value vectors. The outputs from each head are concatenated back in a large vector. BERT uses 12 such heads, which means that the final output contains one 768-component contextualized embedding vector per token, equally long with the input. \n\n\n\n\n\n\nPositional Encoding\nWe can also kickstart the process by adding the input embeddings to positional embeddings. Positional embeddings are vectors that contain information about a position in the sequence, rather than about the meaning of a token. This adds information about the sequence even before attention is applied, and it allows attention to calculate relationships knowing the relative order of the tokens.\n\n\n\n\n\nA detailed explanation of how it works can be found here, but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won’t really need to understand:\n\\[\\begin{equation}\n  p_{i,j} = \\left\\{\n  \\begin{array}{@{}ll@{}}\n    \\sin \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=even \\\\\n    \\cos \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=odd \\\\\n  \\end{array}\\right.\n\\end{equation}\n\\]\n\n\nBERT\nFinally, thanks to the non-linearity introduced by the softmax function, we can achieve even more complex transformations of the embeddings by applying attention again and again, with a couple of helpful steps between each application. A complete model like BERT uses 12 layers of attention, each with its own set of projections So when you search for suggestions for a “walk by the river bank”, the computer doesn’t only get a chance to recognize the keyword “river”, but even the numerical values given to “bank” indicate that you’re interested in enjoying the waterside, and not in need of the nearest cash machine."
  },
  {
    "objectID": "transformers.html#importing-libraries",
    "href": "transformers.html#importing-libraries",
    "title": "Transformers",
    "section": "Importing Libraries",
    "text": "Importing Libraries"
  },
  {
    "objectID": "transformers.html#multi-head-attention-1",
    "href": "transformers.html#multi-head-attention-1",
    "title": "Transformers",
    "section": "Multi Head Attention",
    "text": "Multi Head Attention\nAttention is a mechanism that allows neural networks to assign a different amount of weight or attention to each element in a sequence. For text sequences, the elements are token embeddings, where each token is mapped to a vector of some fixed dimension. For example, in BERT each token is represented as a 768-dimensional vector. The “self” part of self-attention refers to the fact that these weights are computed for all hidden states in the same set—for example, all the hidden states of the encoder. By contrast, the attention mechanism associated with recurrent models involves computing the relevance of each encoder hidden state to the decoder hidden state at a given decoding timestep.\nThe main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a weighted average of each embedding. Another way to formulate this is to say that given a sequence of token embeddings \\(x_{1}, x_{2}, ..., x_{n}\\), self-attention produces a sequence of new embeddings \\(x^{'}_{1}, x^{'}_{2}, ..., x^{'}_{n}\\) where each \\(x^{'}_{i}\\) is a linear combination of all the \\(x_{j}\\):\n\\[x^{'}_{i} = \\sum^{n}_{j=1} w_{ji} . x_{j}\\]\n\n\nMultiHeadAttention\n\n MultiHeadAttention (embed_size, heads)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "transformers.html#encoder-layer",
    "href": "transformers.html#encoder-layer",
    "title": "Transformers",
    "section": "Encoder Layer",
    "text": "Encoder Layer\n\n\n\n\n\nWe will be referring to the encoder layer. The encoder layer/block consists of: 1. Multi-Head Attention 2. Add & Norm 3. Feed Forward 4. Add & Norm again.\n\nnn.LayerNorm()\nThe forward_expansion is a parameter in the “Attention is All You Need” paper which simply adds nodes to the Linear Layer. Since it’s used in two different layers in the end it doesn’t affect the shape of the output (same as input) it just add some extra computation. Its default value is 4.\n\n\n\nTransformerLayer\n\n TransformerLayer (embed_size, heads, dropout, forward_expansion=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "transformers.html#encoder",
    "href": "transformers.html#encoder",
    "title": "Transformers",
    "section": "Encoder",
    "text": "Encoder\n\n\n\n\n\nWe will be referring to the transformer block. The transformer block consists of: 1. Embedding 2. Positional Encoding 3. Transformer Block\n\n\nEncoder\n\n Encoder (src_vocab_size, embed_size, num_layers, heads, device,\n          forward_expansion, dropout, max_length)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "langmodels.html",
    "href": "langmodels.html",
    "title": "Language Models",
    "section": "",
    "text": "import string\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nwith open('./names.txt') as f:\n    names = f.read().splitlines()\nTotal number of names\nlen(names)\n\n32033\nProblem with bigrams is that we were only looking the previous character to predict the next character, the more characters we look back, the bigger the matrix gets, with 2 characters then 27**2 sized matrix.\nInstead we use this:"
  },
  {
    "objectID": "langmodels.html#a-neural-probabilistic-language-model-paper",
    "href": "langmodels.html#a-neural-probabilistic-language-model-paper",
    "title": "Language Models",
    "section": "A Neural Probabilistic Language Model: paper",
    "text": "A Neural Probabilistic Language Model: paper\nIn the paper, they’ve 17000 words in the vocabulary and for each word they assign a feature vector of dimension (30,). So all the 17000 words are embedded into a 30-dimensional space.\nInitially the feature vectors are random and on training the feature vectors are updated…words with similar meanings are closer to each other in the vector space and converse is true.\nModelling approach: maximize log likelihood.\n\nModel Generalization: example (from the paper):\nIn the training set we might have a sentence like “the cat is walking in the bedroom”, “the dog is running in a room”.\nDue to the feature vectors, (a,the), (cat,dog), (room,bedroom), (is,was), (running,walking) might be closer together in the embedding space.\nThis allows the model to predict stuff like “the dog is walking in a ________” -&gt; bedroom, “a cat is running the the _______” -&gt; room\n\n\nDiagram\ninput: 3 previous words, output: 4th word. For each word there’s a one-hot index vector, and then there’s the embedding vector 17000X30. So a dot product will result in the feature vector of that word.\n# with 10 words vocab and 3-dimensional embedding feature vector\nx = torch.tensor([0,0,1,0,0,0,0,0,0,0]) # word index vector\ny = torch.randn(10,3) # embedding vector\ny[2,:], x.float() @ y\n\n&gt;&gt;&gt; (tensor([1.2606, 0.1792, 0.1153]), tensor([1.2606, 0.1792, 0.1153]))\nso for the 3 words, 90 neurons: 30 from each word\nhidden layer: size is the hyperparameter, fully connected to the 90 input neurons with tanh activation.\noutput layer: (17000,) layer with softmax which will give probability distribution for the next word in sequence. with argmax(output), we can get the index of the next word.\n\n\n\nmodel"
  },
  {
    "objectID": "uml_diagrams.html",
    "href": "uml_diagrams.html",
    "title": "UML Diagrams",
    "section": "",
    "text": "import base64\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\n\ndef mm(graph):\n    graphbytes = graph.encode(\"utf8\")\n    base64_bytes = base64.b64encode(graphbytes)\n    base64_string = base64_bytes.decode(\"ascii\")\n    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
  },
  {
    "objectID": "uml_diagrams.html#sequence-diagram",
    "href": "uml_diagrams.html#sequence-diagram",
    "title": "UML Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nThe sequence diagram that follows shows how the developer, IDE, model, and training process interact with one another. The process begins with the developer writing code in the IDE, the IDE sending the model the current code snippet, and the model using the machine learning model it has been trained to produce recommendations. After that, the developer can engage with the recommendations and offer input that helps the model get better over time by periodically retraining it. As the developer integrates the recommendations into their code, the process proceeds.\n\nmm(\"\"\"\nsequenceDiagram\n    participant Developer\n    participant IDE\n    participant Model\n    participant Dataset\n    participant MLTraining\n\n    Developer-&gt;&gt;IDE: Starts coding in IDE\n    IDE-&gt;&gt;Model: Developer's current code snippet\n    Model-&gt;&gt;MLTraining: Retrieve context from model\n    MLTraining--&gt;&gt;Model: Trains machine learning model\n    Model--&gt;&gt;IDE: Trained model\n\n    alt Code Completion Activated\n        Developer-&gt;&gt;IDE: Continues typing\n        IDE-&gt;&gt;Model: Current code snippet\n        Model-&gt;&gt;Model: Generate code suggestions\n        Model--&gt;&gt;IDE: List of suggestions\n    else User Interaction\n        Developer-&gt;&gt;IDE: Reviews and selects suggestions\n        IDE-&gt;&gt;Model: User feedback\n        Model--&gt;&gt;MLTraining: Update model with feedback\n        MLTraining--&gt;&gt;Model: Retrain model\n    end\n\n    Developer--&gt;&gt;IDE: Continues coding with suggestions\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#class-diagram",
    "href": "uml_diagrams.html#class-diagram",
    "title": "UML Diagrams",
    "section": "Class Diagram",
    "text": "Class Diagram\nThe primary components and their interactions in an intelligent code completion system are depicted in this class diagram, which also emphasizes the information flow between the training component, the machine learning model, the integrated development environment, and the developer. However they might now reflect same in the code as we might change them according to the need.\n\nmm(\"\"\"\nclassDiagram\n  class Developer {\n    +writeCode()\n  }\n\n  class IDE {\n    +sendCodeSnippet()\n  }\n\n  class Model {\n    +generateCodeSuggestions()\n    +receiveUserFeedback()\n  }\n\n  class MLTraining {\n    +trainModel()\n    +updateModel()\n  }\n\n  Developer --&gt; IDE: Uses\n  IDE --&gt; Model: Sends code snippet\n  Model --&gt; MLTraining: Trains on dataset\n  Model --&gt; IDE: Sends suggestions\n  Model --&gt; MLTraining: Receives user feedback\n  MLTraining --&gt; Model: Updates model\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#state-machine-diagram",
    "href": "uml_diagrams.html#state-machine-diagram",
    "title": "UML Diagrams",
    "section": "State Machine Diagram",
    "text": "State Machine Diagram\nThe system is shown in the state machine diagram as “Coding” upon startup. It changes to the “Code Suggestions” state when code completion is enabled. In the event that the user interacts with the suggestions, the system can either move to the “User Interaction” stage or revert to the “Coding” state. The “User Interaction” state permits the developer to type further or approve/disapprove recommendations.\n\nmm(\"\"\"\nstateDiagram\n  state \"Coding\" as Coding\n  state \"Code Suggestions\" as Suggestions\n  state \"User Interaction\" as Interaction\n\n  [*] --&gt; Coding\n\n  Coding --&gt; Suggestions: Code Completion Activated\n  Suggestions --&gt; Coding: Suggestions Rejected\n  Suggestions --&gt; Interaction: User Interaction\n\n  Interaction --&gt; Suggestions: Continue Typing\n  Interaction --&gt; Coding: Suggestions Accepted\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#user-journey-diagram",
    "href": "uml_diagrams.html#user-journey-diagram",
    "title": "UML Diagrams",
    "section": "User Journey Diagram",
    "text": "User Journey Diagram\nThe steps a developer takes to use the intelligent code completion system and get started with coding in the IDE are shown in this user journey diagram. Code completion, suggestion creation, user interaction, and the loop of continual improvement via user feedback and model retraining are all included.\n\nmm(\"\"\"\njourney\n  title Developer's Journey with Intelligent Code Completion\n\n  section Getting Started\n    Developer --&gt; IDE: Starts coding in IDE\n\n  section Code Completion Activated\n    IDE --&gt; Model: Developer's current code snippet\n    Model --&gt; MLTraining: Retrieve context from 70GB C code dataset\n    MLTraining --&gt;&gt; Model: Train machine learning model\n    Model --&gt;&gt; IDE: Trained model\n    Developer --&gt; IDE: Continues typing\n\n  section Suggestions\n    IDE --&gt; Model: Current code snippet\n    Model --&gt; Model: Generate code suggestions\n    Model --&gt;&gt; IDE: List of suggestions\n    Developer --&gt; IDE: Reviews and selects suggestions\n\n  section User Interaction\n    IDE --&gt; Model: User feedback\n    Model --&gt;&gt; MLTraining: Update model with feedback\n    MLTraining --&gt;&gt; Model: Retrain model\n    Model --&gt;&gt; IDE: Updated model\n    Developer --&gt; IDE: Continues coding with suggestions\n\n  section Conclusion\n    Developer --&gt; IDE: Finishes coding\n\n\"\"\")"
  }
]
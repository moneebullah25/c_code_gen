[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "c_code_gen",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "c_code_gen",
    "section": "Install",
    "text": "Install\npip install c_code_gen"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "c_code_gen",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\nimport torch\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n    src_pad_idx = 0 # index of the padding token in source vocabulary\n    trg_pad_idx = 0 # index of the padding token in target vocabulary\n    src_vocab_size = 10 # number of unique tokens in source vocabulary\n    trg_vocab_size = 10 # number of unique tokens in target vocabulary\n    \n    print(f\"Input shape: {x.shape}\")\n    print(f\"Target shape: {trg.shape}\")\n    print(f\"Device available: {device}\")\n    \n    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n    out = model(x, trg[:, :-1])\n    print(f\"Output shape: {out.shape}\")\n    print(f\"Output: {out}\")\n\nInput shape: torch.Size([2, 9])\nTarget shape: torch.Size([2, 8])\nDevice available: cuda\nOutput shape: torch.Size([2, 7, 10])\nOutput: tensor([[[ 0.4464, -0.5610, -0.7114,  0.9363,  0.6822, -1.5460, -0.5701,\n          -0.0960,  0.4808,  0.1313],\n         [-0.7031,  0.6683, -0.6762, -0.0794, -0.2506, -0.9622, -0.1848,\n          -0.7650, -0.2054, -0.5386],\n         [-0.0112,  0.4172, -0.1490, -1.0593,  0.2641, -0.8530, -0.3859,\n          -0.3926, -0.3144, -0.1417],\n         [ 0.4123,  0.3738,  0.6268,  0.8212,  1.1357, -1.1602, -0.0434,\n          -1.7120,  0.1721, -0.5142],\n         [-0.5740,  0.6748,  0.4170,  1.0975, -0.0173, -0.5885, -1.8482,\n           0.1379,  0.7698, -0.3862],\n         [ 0.6030, -0.1450, -0.4451,  1.1064,  0.1838, -1.0696, -0.4320,\n           0.0764,  0.5091, -0.2963],\n         [ 0.0264,  0.1590, -0.4393,  0.9079,  0.7149, -1.4549,  0.1765,\n           0.3150,  0.3267, -0.9601]],\n\n        [[ 0.5317, -0.5054, -0.6930,  0.9477,  0.7169, -1.3674, -0.5864,\n          -0.1622,  0.5145,  0.1502],\n         [-0.5181,  1.1593, -0.3028,  0.6865, -0.1220, -0.7017, -1.0549,\n          -0.4249, -0.0154, -0.7563],\n         [ 0.0823,  0.9191, -0.1109,  0.1114,  0.0602, -1.0653, -0.8787,\n           0.1198, -0.4894, -0.1040],\n         [ 0.1353,  0.4007,  0.1736,  0.0703,  1.2294, -1.2375, -0.2426,\n          -1.0955,  0.2159, -0.7532],\n         [ 0.1648,  0.3638, -0.1407, -0.5300,  0.3209, -0.3451, -1.0195,\n           0.1148,  0.8064,  0.1274],\n         [-0.6308,  0.8116, -0.6778,  0.9686,  0.0346, -0.8795, -0.4404,\n          -0.3469,  0.3887, -0.2115],\n         [ 0.1550,  0.5274, -0.3766,  0.5200, -0.2350, -1.2167, -0.4607,\n           0.2098, -0.1927, -0.5351]]], device='cuda:0',\n       grad_fn=&lt;ViewBackward0&gt;)"
  },
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "This is the diagram of the Transformer network presented in the Attention is All You Need paper. We will go through all the different pieces of this network throughout this notebook."
  },
  {
    "objectID": "transformers.html#transformers-explained-step-by-step",
    "href": "transformers.html#transformers-explained-step-by-step",
    "title": "Transformers",
    "section": "Transformers Explained Step by Step",
    "text": "Transformers Explained Step by Step\n\nTokenization\nThe first step in processing text is to cut it into pieces called tokens. There are many variations of how to do it, and we won’t go into details, but BERT uses WordPiece tokenization. This means that tokens correspond roughly to words and punctuation, although a word can also be split into several tokens if it contains a common prefix or suffix. These are called sub-word tokens and usually contain ## characters. Words can even be spelled out if they have never been seen before. \n\n\n\n\n\nEmbedding\nThe second step is to associate each token with an embedding, which is nothing more than a vector of real numbers. Again, there are many ways to create embedding vectors. Fortunately, already trained embeddings are often provided by research groups, and we can just use an existing dictionary to convert the WordPiece tokens into embedding vectors. \n\n\n\n The embedding of tokens into vectors is an achievement in itself. The values inside an embedding carry information about the meaning of the token, but they are also arranged in such a way that one can perform mathematical operations on them, which correspond to semantic changes, like changing the gender of a noun, or the tense of a verb, or even the homeland of a city.\n\n\n\n\n\nContext\nHowever, embeddings are associated with tokens by a straight dictionary look-up, which means that the same token always gets the same embedding, regardless of its context. This is where the attention mechanism comes in, and specifically for BERT, the scaled dot-product self-attention. Attention transforms the default embeddings by analyzing the whole sequence of tokens, so that the values are more representative of the token they represent in the context of the sentence.\n\n\n\n\n\nSelf Attention Mechanism\nLet’s have a look at this process with the sequence of tokens walk, by, river, bank. Each token is initially replaced by its default embedding, which in this case is a vector with 768 components. \n\n\n\n\nLet’s color the embedding of the first token to follow what happens to it. We start by calculating the scalar product between pairs of embeddings. Here we have the first embedding with itself. When the two vectors are more correlated, or aligned, meaning that they are generally more similar, the scalar product is higher (darker in image), and we consider that they have a strong relationship. If they had less similar content, the scalar product would be lower (brighter in the image) and we would consider that they don’t relate to each other.\n\n\n\n\n\nWe go on and calculate the scalar product for every possible pair of embedding vectors in the input sequence. The values obtained are usually scaled down to avoid getting large values, which improves the numerical behavior. That’s done here by dividing by the square root of 768, which is the size of the vectors. \n\n\n\n\nThen comes the only non-linear operation in the attention mechanism: The scaled values are passed through a softmax activation function, by groups corresponding to each input token. So in this illustration, we apply the softmax column by column. What the softmax does is to exponentially amplify large values, while crushing low and negative values towards zero. It also does normalization, so that each column sums up to 1.\n\n\n\n\n\nFinally, we create a new embedding vector for each token by linear combination of the input embeddings, in proportions given by the softmax results. We can say that the new embedding vectors are contextualized, since they contain a fraction of every input embedding for this particular sequence of tokens. In particular, if a token has a strong relationship with another one, a large fraction of its new contextualized embedding will be made of the related embedding. If a token doesn’t relate much to any other, as measured by the scalar product between their input embeddings, its contextualized embedding will be nearly identical to the input embedding.\n\n\n\n\n\nFor instance, one can imagine that the vector space has a direction that corresponds to the idea of nature. The input embeddings of the tokens river and bank should both have large values in that direction, so that they are more similar and have a strong relationship. As a result, the new contextualized embeddings of the river and bank tokens would combine both input embeddings in roughly equal parts. On the other hand, the preposition by sounds quite neutral, so that its embedding should have a weak relationship with every other one and little modification of its embedding vector would occur. So there we have the mechanism that lets the scaled dot-product attention utilize context.\nTo recap: 1. First, it determines how much the input embedding vectors relate to each other using the scalar product. 2. The results are then scaled down, and the softmax activation function is applied, which normalizes these results in a non-linear way. 3. New contextualized embeddings are finally created for every token by linear combination of all the input embeddings, using the softmax proportions as coefficients \n\n\n\n\n\n\nKeys, Queries and Values\nHowever, that’s not the whole story. Most importantly, we don’t have to use the input embedding vectors as is. We can first project them using 3 linear projections to create the so-called Key, Query, and Value vectors. Typically, the projections are also mapping the input embeddings onto a space of lower dimension. In the case of BERT, the Key, Query, and Value vectors all have 64 components. \n\n\n\n\nEach projection can be thought of as focusing on different directions of the vector space, which would represent different semantic aspects. One can imagine that a Key is the projection of an embedding onto the direction of “prepositions”, and a Query is the projection of an embedding along the direction of “locations”. In this case, the Key of the token by should have a strong relationship with every other Query, since by should have strong components in the direction of “prepositions”, and every other token should have strong components in the direction of “locations”. The Values can come from yet another projection that is relevant, for example the direction of physical places. It’s these values that are combined to create the contextualized embeddings In practice, the meaning of each projection may not be so clear, and the model is free to learn whatever projections allow it to solve language tasks the most efficiently.\n\n\nMulti-head Attention\nIn addition, the same process can be repeated many times with different Key, Query, and Value projections, forming what is called a multi-head attention. Each head can focus on different projections of the input embeddings. For instance, one head could calculate the preposition/location relationships, while another head could calculate subject/verb relationships, simply by using different projections to create the Key, Query, and Value vectors. The outputs from each head are concatenated back in a large vector. BERT uses 12 such heads, which means that the final output contains one 768-component contextualized embedding vector per token, equally long with the input. \n\n\n\n\n\n\nPositional Encoding\nWe can also kickstart the process by adding the input embeddings to positional embeddings. Positional embeddings are vectors that contain information about a position in the sequence, rather than about the meaning of a token. This adds information about the sequence even before attention is applied, and it allows attention to calculate relationships knowing the relative order of the tokens.\n\n\n\n\n\nA detailed explanation of how it works can be found here, but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won’t really need to understand:\n\\[\\begin{equation}\n  p_{i,j} = \\left\\{\n  \\begin{array}{@{}ll@{}}\n    \\sin \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=even \\\\\n    \\cos \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=odd \\\\\n  \\end{array}\\right.\n\\end{equation}\n\\]\n\n\nBERT\nFinally, thanks to the non-linearity introduced by the softmax function, we can achieve even more complex transformations of the embeddings by applying attention again and again, with a couple of helpful steps between each application. A complete model like BERT uses 12 layers of attention, each with its own set of projections So when you search for suggestions for a “walk by the river bank”, the computer doesn’t only get a chance to recognize the keyword “river”, but even the numerical values given to “bank” indicate that you’re interested in enjoying the waterside, and not in need of the nearest cash machine."
  },
  {
    "objectID": "transformers.html#importing-libraries",
    "href": "transformers.html#importing-libraries",
    "title": "Transformers",
    "section": "Importing Libraries",
    "text": "Importing Libraries"
  },
  {
    "objectID": "transformers.html#multi-head-attention-1",
    "href": "transformers.html#multi-head-attention-1",
    "title": "Transformers",
    "section": "Multi Head Attention",
    "text": "Multi Head Attention\nAttention is a mechanism that allows neural networks to assign a different amount of weight or attention to each element in a sequence. For text sequences, the elements are token embeddings, where each token is mapped to a vector of some fixed dimension. For example, in BERT each token is represented as a 768-dimensional vector. The “self” part of self-attention refers to the fact that these weights are computed for all hidden states in the same set—for example, all the hidden states of the encoder. By contrast, the attention mechanism associated with recurrent models involves computing the relevance of each encoder hidden state to the decoder hidden state at a given decoding timestep.\nThe main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a weighted average of each embedding. Another way to formulate this is to say that given a sequence of token embeddings \\(x_{1}, x_{2}, ..., x_{n}\\), self-attention produces a sequence of new embeddings \\(x^{'}_{1}, x^{'}_{2}, ..., x^{'}_{n}\\) where each \\(x^{'}_{i}\\) is a linear combination of all the \\(x_{j}\\):\n\\[x^{'}_{i} = \\sum^{n}_{j=1} w_{ji} . x_{j}\\]\n\n\nMultiHeadAttention\n\n MultiHeadAttention (embed_size, heads)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "transformers.html#encoder-layer",
    "href": "transformers.html#encoder-layer",
    "title": "Transformers",
    "section": "Encoder Layer",
    "text": "Encoder Layer\n\n\n\n\n\nWe will be referring to the encoder layer. The encoder layer/block consists of: 1. Multi-Head Attention 2. Add & Norm 3. Feed Forward 4. Add & Norm again.\n\nnn.LayerNorm()\nThe forward_expansion is a parameter in the “Attention is All You Need” paper which simply adds nodes to the Linear Layer. Since it’s used in two different layers in the end it doesn’t affect the shape of the output (same as input) it just add some extra computation. Its default value is 4.\n\n\n\nTransformerLayer\n\n TransformerLayer (embed_size, heads, dropout, forward_expansion=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "transformers.html#encoder",
    "href": "transformers.html#encoder",
    "title": "Transformers",
    "section": "Encoder",
    "text": "Encoder\n\n\n\n\n\nWe will be referring to the transformer block. The transformer block consists of: 1. Embedding 2. Positional Encoding 3. Transformer Block\n\n\nEncoder\n\n Encoder (src_vocab_size, embed_size, num_layers, heads, device,\n          forward_expansion, dropout, max_length)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "mlp.html",
    "href": "mlp.html",
    "title": "Language Models",
    "section": "",
    "text": "import string\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nwith open('./names.txt') as f:\n    names = f.read().splitlines()\nTotal number of names\nlen(names)\n\n32033\nProblem with bigrams is that we were only looking the previous character to predict the next character, the more characters we look back, the bigger the matrix gets, with 2 characters then 27**2 sized matrix.\nInstead we use this:"
  },
  {
    "objectID": "mlp.html#a-neural-probabilistic-language-model-paper",
    "href": "mlp.html#a-neural-probabilistic-language-model-paper",
    "title": "Language Models",
    "section": "A Neural Probabilistic Language Model: paper",
    "text": "A Neural Probabilistic Language Model: paper\nIn the paper, they’ve 17000 words in the vocabulary and for each word they assign a feature vector of dimension (30,). So all the 17000 words are embedded into a 30-dimensional space.\nInitially the feature vectors are random and on training the feature vectors are updated…words with similar meanings are closer to each other in the vector space and converse is true.\nModelling approach: maximize log likelihood.\n\nModel Generalization: example (from the paper):\nIn the training set we might have a sentence like “the cat is walking in the bedroom”, “the dog is running in a room”.\nDue to the feature vectors, (a,the), (cat,dog), (room,bedroom), (is,was), (running,walking) might be closer together in the embedding space.\nThis allows the model to predict stuff like “the dog is walking in a ________” -&gt; bedroom, “a cat is running the the _______” -&gt; room\n\n\nDiagram\ninput: 3 previous words, output: 4th word. For each word there’s a one-hot index vector, and then there’s the embedding vector 17000X30. So a dot product will result in the feature vector of that word.\n# with 10 words vocab and 3-dimensional embedding feature vector\nx = torch.tensor([0,0,1,0,0,0,0,0,0,0]) # word index vector\ny = torch.randn(10,3) # embedding vector\ny[2,:], x.float() @ y\n\n&gt;&gt;&gt; (tensor([1.2606, 0.1792, 0.1153]), tensor([1.2606, 0.1792, 0.1153]))\nso for the 3 words, 90 neurons: 30 from each word\nhidden layer: size is the hyperparameter, fully connected to the 90 input neurons with tanh activation.\noutput layer: (17000,) layer with softmax which will give probability distribution for the next word in sequence. with argmax(output), we can get the index of the next word.\n\n\n\nmodel"
  },
  {
    "objectID": "bigram.html",
    "href": "bigram.html",
    "title": "Bigram Character Level Language Model",
    "section": "",
    "text": "import torch\nimport string\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "bigram.html#introduction",
    "href": "bigram.html#introduction",
    "title": "Bigram Character Level Language Model",
    "section": "Introduction",
    "text": "Introduction\nReading the names from the names.txt which contains the names in lowercase, separated by new line. Dataset is downloaded from https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n\nwith open('names.txt','r') as f:\n    names = f.read().splitlines()\n\n\nlen(names)\n\n32033\n\n\nTo determine the minimum number of characters and maximum number of characters we use the following code\n\nmin(len(w) for w in names), max(len(w) for w in names)\n\n(2, 15)\n\n\nWe initially used &lt;S&gt; and &lt;E&gt; as the start and end token respectively. But this approach is not useful enough because we can’t have a character that starts before &lt;S&gt; so in the N lookup table there will be a complete row which have 0’s in it.\nHowever instead of using &lt;S&gt; and &lt;E&gt;, substituting it for a single . character which indicates both the start and the end and make slight improvement in our code.\n\n# tokens = ['&lt;S&gt;','&lt;E&gt;'] + list(string.ascii_lowercase)\ntokens = ['.'] + list(string.ascii_lowercase)\n\n\ntokens\n\n['.',\n 'a',\n 'b',\n 'c',\n 'd',\n 'e',\n 'f',\n 'g',\n 'h',\n 'i',\n 'j',\n 'k',\n 'l',\n 'm',\n 'n',\n 'o',\n 'p',\n 'q',\n 'r',\n 's',\n 't',\n 'u',\n 'v',\n 'w',\n 'x',\n 'y',\n 'z']\n\n\nNote string.ascii_lowercase return lowercase alphabets in a single string which we will later use to create a list and append . character. Now len(tokens) will have 27 characters.\n\nstring.ascii_lowercase, len(tokens)\n\n('abcdefghijklmnopqrstuvwxyz', 27)\n\n\nNow we can’t pass string data to our model we need to convert the characters to the number. Why characters? because it is a character level language model Bigram which means we will feed one character to our model and it will try to predict the next character in a sequence. We can also pass multiple character to our model but this is not the architecture of Bigram.\n\nstoi = {s:i for i,s in enumerate(tokens)} # alphabet as key, integer as value\nitos = {i:s for s,i in stoi.items()} # integer as key, alphabet as value\n\n\nstoi\n\n{'.': 0,\n 'a': 1,\n 'b': 2,\n 'c': 3,\n 'd': 4,\n 'e': 5,\n 'f': 6,\n 'g': 7,\n 'h': 8,\n 'i': 9,\n 'j': 10,\n 'k': 11,\n 'l': 12,\n 'm': 13,\n 'n': 14,\n 'o': 15,\n 'p': 16,\n 'q': 17,\n 'r': 18,\n 's': 19,\n 't': 20,\n 'u': 21,\n 'v': 22,\n 'w': 23,\n 'x': 24,\n 'y': 25,\n 'z': 26}\n\n\n\nitos\n\n{0: '.',\n 1: 'a',\n 2: 'b',\n 3: 'c',\n 4: 'd',\n 5: 'e',\n 6: 'f',\n 7: 'g',\n 8: 'h',\n 9: 'i',\n 10: 'j',\n 11: 'k',\n 12: 'l',\n 13: 'm',\n 14: 'n',\n 15: 'o',\n 16: 'p',\n 17: 'q',\n 18: 'r',\n 19: 's',\n 20: 't',\n 21: 'u',\n 22: 'v',\n 23: 'w',\n 24: 'x',\n 25: 'y',\n 26: 'z'}\n\n\nSample Bigram Language Model – Now basically A bigram language model is a type of language model that predicts the probability of a word in a sequence based on the previous word. Same is true for the character.\nIn the word case our vocabulary can be 17000 words or 100000 words based on the size of the problem, which in this case each word is assigned a index to be feed in to the model. But in the character level language model our vocabulary size is total number of character used in our whole dataset which are in this case 26 characters and we append . at the start and end of each name so total of we have 27 characters in our problem so our vocabulary size is 27 characters.\n\nfor word in names[:3]:\n    word = ['.'] + list(word) + ['.']\n    for ch1, ch2 in zip(word,word[1:]):\n        print(ch1,ch2)\n\n. e\ne m\nm m\nm a\na .\n. o\no l\nl i\ni v\nv i\ni a\na .\n. a\na v\nv a\na .\n\n\nWe are declaring a tensor which we will use to store the counts of our bigrams\n\nN = torch.zeros((27, 27), dtype=torch.int32)\n\nIn the below code we are counting out of total 27*27 pairs of bigrams, how many times each bigram appeared in our names dataset. Also note than we append . character at the start and at the end of each name.\n\nfor w in names:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    N[ix1, ix2] += 1\n\nThen we just get top most appearing bigrams in our dataset\n\ntorch.topk is a PyTorch function that returns the top k values and their indices along a specified dimension of a tensor.\nIn this case, N.flatten() is used to flatten the 2D tensor N into a 1D tensor, and torch.topk is applied to find the top 5 values and their indices in this flattened tensor.\ntop_values contains the top 5 values, and top_indices contains their corresponding indices in the flattened tensor.\n\n\ntop_values, top_indices = torch.topk(N.flatten(), 5)\nprint(top_values, top_indices)\n\ntensor([6763, 6640, 5438, 4410, 3983], dtype=torch.int32) tensor([378,  27,  41,   1, 135])\n\n\n\nnp.unravel_index is a NumPy function that converts flat indices into multi-dimensional (2D in this case) indices.\ntop_indices are the flat indices obtained from torch.topk.\nN.shape is the shape of the original 2D tensor N.\ntop_indices_2d contains the 2D indices corresponding to the flat indices.\n\n\ntop_indices_2d = np.unravel_index(top_indices, N.shape)\nprint(top_indices_2d)\n\n(array([14,  1,  1,  0,  5]), array([ 0,  0, 14,  1,  0]))\n\n\n\nThe below line creates a list of tuples where each tuple contains a string in the specified format and the corresponding value as an integer.\nitos[top_indices_2d[0][i]] and itos[top_indices_2d[1][i]] are used to get the characters corresponding to the first and second indices in the 2D indices, respectively.\nint(top_values[i]) is used to convert the values to integers.\n\n\ntop_results = [(f\"('{itos[top_indices_2d[0][i]]}', '{itos[top_indices_2d[1][i]]}')\", int(top_values[i])) for i in range(5)]\n\nprint(top_results)\n\n[(\"('n', '.')\", 6763), (\"('a', '.')\", 6640), (\"('a', 'n')\", 5438), (\"('.', 'a')\", 4410), (\"('e', '.')\", 3983)]\n\n\nPlotting graph\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(27):\n    for j in range(27):\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\nplt.axis('off');\n\n\n\n\n\nN[0]\n\ntensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n         134,  535,  929], dtype=torch.int32)\n\n\nUsing the p as probability distribution we will use it with torch.multinomial to draw samples from p based on the probability\ntorch.rand Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0,1)\n\ngen = torch.Generator().manual_seed(2147483647)\np = torch.rand(3, generator=gen)\np /= p.sum()\np\n\ntensor([0.6064, 0.3033, 0.0903])\n\n\ntorch.multinomial Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor p.\n\n# then with the generator, we can sample ints using multinomial\n# here ~60% will be 0, ~30% will be 1 and ~10% will be 2\ntorch.multinomial(p,num_samples=20, replacement=True,generator=gen)\n\ntensor([1, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1])\n\n\n\n# normalize: this gives a probability distribution across the row, and sum=1\np = N[0].float()\np /= p.sum()\np, p.sum()\n\n(tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n         0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n         0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290]),\n tensor(1.))\n\n\n\nidx = torch.multinomial(p,num_samples=1, replacement=True,generator=gen).item()\nidx, itos[idx]\n\n(11, 'k')\n\n\n\n# for the next character, go to row 'e' and so on\nseed = 2147483647\ngen = torch.Generator().manual_seed(seed)\nidx = 0 # start token\n\nfor i in range(25):\n    name = ''\n    while True:\n        \"\"\"\n        what if every first character were equally likely?\n        p = torch.ones(27) / 27.0\n        this is gonna be garbage indicating untrained.\n        \"\"\"\n        p = N[idx].float()\n        p /= p.sum()\n        idx = torch.multinomial(p,num_samples=1, replacement=True,generator=gen).item()\n        name += itos[idx]\n        if idx == 0:\n            break\n    print(name)\n\njunide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias.\ndedainrwieta.\nssonielylarte.\nfaveumerifontume.\nphynslenaruani.\ncore.\nyaenon.\nka.\njabdinerimikimaynin.\nanaasn.\nssorionsush.\ndgossmitan.\nil.\nle.\npann.\nthat.\n\n\nNow when calculating the log likelihood if any of our probability is 0 our loss will be very much high. So what we will do we will add a very small amount to our N model so that each biagram will atleast be contributing a little to the output\nAlso instead of calculating p for each row we could calculate it separate\n\np = (N+1).float()\np /= p.sum(1, keepdims=True)\n\n\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(5):\n  \n  out = []\n  ix = 0\n  while True:\n    P = p[ix]\n    ix = torch.multinomial(P, num_samples=1, replacement=True, generator=g).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n\njunide.\njanasah.\np.\ncony.\na.\n\n\nIn language models, the negative log-likelihood (NLL) is commonly used as a loss function during training. The goal of a language model is to predict the probability distribution of the next word in a sequence given the context of preceding words.\nThe NLL measures the difference between the predicted probability distribution and the actual distribution of the next word. Minimizing the NLL during training encourages the model to assign higher probabilities to the correct words. Mathematically, maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood.\n\nlog_likelihood = 0.0\nn = 0\n\nfor w in names:\n#for w in [\"andrejq\"]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    prob = p[ix1, ix2]\n    logprob = torch.log(prob)\n    log_likelihood += logprob\n    n += 1\n    #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n\nprint(f'{log_likelihood=}')\nnll = -log_likelihood\nprint(f'{nll=}')\nprint(f'{nll/n}')\n\nlog_likelihood=tensor(-559951.5625)\nnll=tensor(559951.5625)\n2.4543561935424805"
  },
  {
    "objectID": "bigram.html#creating-training-dataset-for-bigrams",
    "href": "bigram.html#creating-training-dataset-for-bigrams",
    "title": "Bigram Character Level Language Model",
    "section": "Creating Training Dataset for Bigrams",
    "text": "Creating Training Dataset for Bigrams\nys is basically the shifted version of the xs. Which means training set will contains the previous character and the next character in the name\n\n# create the training set of bigrams (x,y)\nxs, ys = [], []\n\nfor w in names[:1]:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    print(ch1, ch2)\n    xs.append(ix1)\n    ys.append(ix2)\n    \nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n\n. e\ne m\nm m\nm a\na .\n\n\n\nxs\n\ntensor([ 0,  5, 13, 13,  1])\n\n\n\nys\n\ntensor([ 5, 13, 13,  1,  0])\n\n\n\nimport torch.nn.functional as F\nxenc = F.one_hot(xs, num_classes=27).float()\nxenc\n\ntensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\nxenc.shape\n\ntorch.Size([5, 27])\n\n\n\nplt.imshow(xenc)\n\n&lt;matplotlib.image.AxesImage at 0x7f4cb165bb90&gt;\n\n\n\n\n\n\nxenc.dtype\n\ntorch.float32\n\n\n\nW = torch.randn((27, 27))\nxenc @ W\n\ntensor([[-0.6121,  0.0792, -0.6813, -1.4389,  0.3476, -0.7554, -0.0311, -0.4285,\n         -0.9945, -0.3620,  1.1535,  1.0019,  0.1942, -1.2011, -0.3320, -0.7448,\n          0.2188,  0.1101, -1.1942,  0.2223, -0.3493, -0.1568,  0.1647,  0.0068,\n         -1.4091,  1.0483,  1.4028],\n        [-0.9926,  0.2868, -0.4810,  0.2572,  1.1087,  1.4877,  0.5212,  0.7704,\n          0.1191,  0.4504,  1.4064, -0.8934, -0.1128, -0.0778,  0.5568, -0.5619,\n         -0.5935, -0.3566, -0.0470, -0.5065,  1.1515, -0.3277, -1.1372, -0.8816,\n         -1.1635, -1.0060, -0.4768],\n        [ 0.6945,  1.3994,  0.1112,  0.3696, -2.0318,  0.8638,  0.3667,  0.7193,\n          0.4077, -0.3856, -0.1961, -1.0752,  0.9568, -1.6473, -2.1890,  0.6441,\n          0.3363, -2.1826, -1.2316, -0.8693, -0.3052, -1.0200,  0.0095, -0.1961,\n          0.6888,  0.7063, -0.9632],\n        [ 0.6945,  1.3994,  0.1112,  0.3696, -2.0318,  0.8638,  0.3667,  0.7193,\n          0.4077, -0.3856, -0.1961, -1.0752,  0.9568, -1.6473, -2.1890,  0.6441,\n          0.3363, -2.1826, -1.2316, -0.8693, -0.3052, -1.0200,  0.0095, -0.1961,\n          0.6888,  0.7063, -0.9632],\n        [-0.5563, -0.5197,  0.3712,  0.0050,  1.8375,  1.0533, -0.8577,  1.3217,\n          0.1840,  0.9304, -0.6296,  0.1094, -1.2615, -1.0253, -0.1580,  1.1305,\n          1.7722,  0.8831, -0.6096,  0.2606,  0.0242, -0.9650, -1.0171,  0.9944,\n          1.0160,  0.9089,  0.2120]])\n\n\n\nlogits = xenc @ W # log-counts\ncounts = logits.exp() # equivalent N\n\nprobs = counts / counts.sum(1, keepdims=True)\nprobs, logits.shape\n\n(tensor([[0.0179, 0.0356, 0.0167, 0.0078, 0.0466, 0.0155, 0.0319, 0.0215, 0.0122,\n          0.0229, 0.1044, 0.0897, 0.0400, 0.0099, 0.0236, 0.0156, 0.0410, 0.0368,\n          0.0100, 0.0411, 0.0232, 0.0282, 0.0388, 0.0332, 0.0080, 0.0940, 0.1339],\n         [0.0106, 0.0381, 0.0177, 0.0370, 0.0867, 0.1266, 0.0482, 0.0618, 0.0322,\n          0.0449, 0.1167, 0.0117, 0.0256, 0.0265, 0.0499, 0.0163, 0.0158, 0.0200,\n          0.0273, 0.0172, 0.0905, 0.0206, 0.0092, 0.0118, 0.0089, 0.0105, 0.0178],\n         [0.0619, 0.1254, 0.0346, 0.0448, 0.0041, 0.0734, 0.0446, 0.0635, 0.0465,\n          0.0210, 0.0254, 0.0106, 0.0805, 0.0060, 0.0035, 0.0589, 0.0433, 0.0035,\n          0.0090, 0.0130, 0.0228, 0.0112, 0.0312, 0.0254, 0.0616, 0.0627, 0.0118],\n         [0.0619, 0.1254, 0.0346, 0.0448, 0.0041, 0.0734, 0.0446, 0.0635, 0.0465,\n          0.0210, 0.0254, 0.0106, 0.0805, 0.0060, 0.0035, 0.0589, 0.0433, 0.0035,\n          0.0090, 0.0130, 0.0228, 0.0112, 0.0312, 0.0254, 0.0616, 0.0627, 0.0118],\n         [0.0119, 0.0124, 0.0302, 0.0209, 0.1308, 0.0597, 0.0088, 0.0781, 0.0250,\n          0.0528, 0.0111, 0.0232, 0.0059, 0.0075, 0.0178, 0.0645, 0.1225, 0.0504,\n          0.0113, 0.0270, 0.0213, 0.0079, 0.0075, 0.0563, 0.0575, 0.0517, 0.0257]]),\n torch.Size([5, 27]))\n\n\n\nprobs[0]\n\ntensor([0.0179, 0.0356, 0.0167, 0.0078, 0.0466, 0.0155, 0.0319, 0.0215, 0.0122,\n        0.0229, 0.1044, 0.0897, 0.0400, 0.0099, 0.0236, 0.0156, 0.0410, 0.0368,\n        0.0100, 0.0411, 0.0232, 0.0282, 0.0388, 0.0332, 0.0080, 0.0940, 0.1339])\n\n\n\nprobs[0].shape\n\ntorch.Size([27])\n\n\n\nprobs[0].sum()\n\ntensor(1.)"
  },
  {
    "objectID": "bigram.html#summary",
    "href": "bigram.html#summary",
    "title": "Bigram Character Level Language Model",
    "section": "Summary",
    "text": "Summary\n\nxs\n\ntensor([ 0,  5, 13, 13,  1])\n\n\n\nys\n\ntensor([ 5, 13, 13,  1,  0])\n\n\n\n# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g)\n\n\nxenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\nlogits = xenc @ W # predict log-counts\ncounts = logits.exp() # counts, equivalent to N\nprobs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n# btw: the last 2 lines here are together called a 'softmax'\n\n\nprobs.shape\n\ntorch.Size([5, 27])\n\n\n\nnlls = torch.zeros(5)\nfor i in range(5):\n  # i-th bigram:\n  x = xs[i].item() # input character index\n  y = ys[i].item() # label character index\n  print('--------')\n  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n  print('input to the neural net:', x)\n  print('output probabilities from the neural net:', probs[i])\n  print('label (actual next character):', y)\n  p = probs[i, y]\n  print('probability assigned by the net to the the correct character:', p.item())\n  logp = torch.log(p)\n  print('log likelihood:', logp.item())\n  nll = -logp\n  print('negative log likelihood:', nll.item())\n  nlls[i] = nll\n\nprint('=========')\nprint('average negative log likelihood, i.e. loss =', nlls.mean().item())\n\n--------\nbigram example 1: .e (indexes 0,5)\ninput to the neural net: 0\noutput probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\nlabel (actual next character): 5\nprobability assigned by the net to the the correct character: 0.01228625513613224\nlog likelihood: -4.399273872375488\nnegative log likelihood: 4.399273872375488\n--------\nbigram example 2: em (indexes 5,13)\ninput to the neural net: 5\noutput probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.018050700426101685\nlog likelihood: -4.014570713043213\nnegative log likelihood: 4.014570713043213\n--------\nbigram example 3: mm (indexes 13,13)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 13\nprobability assigned by the net to the the correct character: 0.026691533625125885\nlog likelihood: -3.623408794403076\nnegative log likelihood: 3.623408794403076\n--------\nbigram example 4: ma (indexes 13,1)\ninput to the neural net: 13\noutput probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\nlabel (actual next character): 1\nprobability assigned by the net to the the correct character: 0.07367686182260513\nlog likelihood: -2.6080665588378906\nnegative log likelihood: 2.6080665588378906\n--------\nbigram example 5: a. (indexes 1,0)\ninput to the neural net: 1\noutput probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\nlabel (actual next character): 0\nprobability assigned by the net to the the correct character: 0.014977526850998402\nlog likelihood: -4.201204299926758\nnegative log likelihood: 4.201204299926758\n=========\naverage negative log likelihood, i.e. loss = 3.7693049907684326"
  },
  {
    "objectID": "bigram.html#optimization",
    "href": "bigram.html#optimization",
    "title": "Bigram Character Level Language Model",
    "section": "Optimization",
    "text": "Optimization\n\nxs, ys = [], []\nfor w in names:\n  chs = ['.'] + list(w) + ['.']\n  for ch1, ch2 in zip(chs, chs[1:]):\n    ix1 = stoi[ch1]\n    ix2 = stoi[ch2]\n    xs.append(ix1)\n    ys.append(ix2)\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint('number of examples: ', num)\n\n# initialize the 'network'\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27, 27), generator=g, requires_grad=True)\n\nnumber of examples:  228146\n\n\n\nxs\n\ntensor([ 0,  5, 13,  ..., 25, 26, 24])\n\n\n\nxs.shape\n\ntorch.Size([228146])\n\n\n\nys\n\ntensor([ 5, 13, 13,  ..., 26, 24,  0])\n\n\n\nys.shape\n\ntorch.Size([228146])\n\n\nxenc = F.one_hot(xs, num_classes=27).float()\n\nF.one_hot(xs, num_classes=27) converts the input sequence xs into a one-hot encoded representation. Each element in xs is replaced by a one-hot vector with a length of 27 (assuming 27 classes or tokens).\n.float() converts the one-hot encoded tensor to floating-point format, which is often required for further operations.\n\nlogits = xenc @ W  # predict log-counts - @ is the matrix multiplication operator. It calculates the dot product of the one-hot encoded input xenc and the weight matrix W. - logits represents the predicted log-counts for each class.\ncounts = logits.exp()  # counts, equivalent to N - logits.exp() exponentiates the predicted log-counts, converting them into counts. This step is common in models where the output is interpreted as log-probabilities.\nprobs = counts / counts.sum(1, keepdims=True) - counts.sum(1, keepdims=True) computes the sum of counts along the second dimension, ensuring that the result has the same shape as counts. - probs is the probability distribution over the classes for the next character, obtained by normalizing the counts.\nloss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W ** 2).mean() - The first part computes the negative log-likelihood loss. It selects the log-probabilities corresponding to the true labels ys and computes their negative mean. - The second part adds a regularization term, penalizing large values in the weight matrix W.\n# backward pass\nW.grad = None  # set to zero the gradient\nloss.backward()\n\nW.grad = None initializes the gradient of the weight matrix to zero before computing the backward pass.\nloss.backward() computes the gradients of the loss with respect to the parameters using backpropagation.\n\n# update\nW.data += -50 * W.grad\n\nThis performs a gradient descent update. It subtracts a multiple of the gradient from the current weight values to update them.\nThe learning rate is represented by the value -50. The negative sign indicates that it’s a gradient descent step.\n\n\n# gradient descent\nfor k in range(100):\n  \n  # forward pass\n  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n  logits = xenc @ W # predict log-counts\n  counts = logits.exp() # counts, equivalent to N\n  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n  print(loss.item())\n  \n  # backward pass\n  W.grad = None # set to zero the gradient\n  loss.backward()\n  \n  # update\n  W.data += -50 * W.grad\n\n3.768618583679199\n3.3788068294525146\n3.161090850830078\n3.0271859169006348\n2.9344842433929443\n2.867231607437134\n2.8166542053222656\n2.777146577835083\n2.745253801345825\n2.7188305854797363\n2.696505308151245\n2.6773719787597656\n2.6608052253723145\n2.6463515758514404\n2.633665084838867\n2.622471570968628\n2.6125476360321045\n2.6037068367004395\n2.595794916152954\n2.5886807441711426\n2.5822560787200928\n2.576429843902588\n2.5711236000061035\n2.566272735595703\n2.5618226528167725\n2.5577261447906494\n2.5539441108703613\n2.550442695617676\n2.5471930503845215\n2.5441699028015137\n2.5413522720336914\n2.538722038269043\n2.536262035369873\n2.5339579582214355\n2.531797409057617\n2.529768228530884\n2.527860164642334\n2.5260636806488037\n2.5243704319000244\n2.522773265838623\n2.52126407623291\n2.519836664199829\n2.5184857845306396\n2.5172054767608643\n2.515990734100342\n2.5148372650146484\n2.5137407779693604\n2.512697696685791\n2.511704921722412\n2.5107579231262207\n2.509855031967163\n2.5089924335479736\n2.5081679821014404\n2.507380485534668\n2.5066258907318115\n2.5059030055999756\n2.5052103996276855\n2.5045459270477295\n2.503908157348633\n2.503295421600342\n2.5027060508728027\n2.5021398067474365\n2.501594305038452\n2.5010695457458496\n2.500563383102417\n2.500075578689575\n2.4996049404144287\n2.499150514602661\n2.4987120628356934\n2.49828839302063\n2.4978787899017334\n2.4974827766418457\n2.4970996379852295\n2.4967293739318848\n2.496370315551758\n2.4960227012634277\n2.4956860542297363\n2.4953596591949463\n2.4950432777404785\n2.494736433029175\n2.494438886642456\n2.494149684906006\n2.4938690662384033\n2.4935965538024902\n2.4933321475982666\n2.493075132369995\n2.4928252696990967\n2.492582321166992\n2.4923462867736816\n2.492116689682007\n2.4918932914733887\n2.491675853729248\n2.491464376449585\n2.491258382797241\n2.491057872772217\n2.4908623695373535\n2.4906723499298096\n2.4904870986938477\n2.4903063774108887\n2.4901304244995117\n\n\nNow to get sample from the model we start from the 0 as index which is . character and pass it to our model and use torch.multinomial to draw sample from the distribution calculated by the last layer of our nerual network model\n\n# finally, sample from the 'neural net' model\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(5):\n  \n  out = []\n  ix = 0\n  while True:\n    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n    logits = xenc @ W # predict log-counts\n    counts = logits.exp() # counts, equivalent to N\n    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n    \n    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n    out.append(itos[ix])\n    if ix == 0:\n      break\n  print(''.join(out))\n\njunide.\njanasah.\np.\ncfay.\na."
  },
  {
    "objectID": "wavenet.html",
    "href": "wavenet.html",
    "title": "Wavenet Documentation",
    "section": "",
    "text": "This notebook is a practical follow-up to Andrej Karpathy’s “Building makemore Part 5: Building a WaveNet” lecture. Check out the full lecture here.\nHere’s what we’ll cover:\n\nModel Basics: Introduction to a multi-layer perceptron character-level language model.\nModel Enhancement: Expanding the architecture and input characters for better results.\nWaveNet Overview: Understand WaveNet’s hierarchical structure and its predictions.\nBatch Normalization: Dive into the BatchNorm layer and its challenges.\nPyTorch Containers: A look at how PyTorch structures its layers.\nDataset Expansion: Increase the context length for performance improvement.\nForward Pass: Visualization of tensor transformations in the network.\nBatchNorm1D Bug: Addressing an implementation bug.\nDevelopment Insights: Best practices in deep neural network development.\nOptimizing WaveNet: Suggestions and strategies for better performance.\n\nThis notebook aims to provide a clear understanding of WaveNet’s development and optimization process.\n\n\n\n\n\n\n\nNature of the Model: WaveNet is a fully probabilistic and autoregressive model. This means that when predicting any given audio sample, it considers all the previous samples.\nEfficiency: It can be trained efficiently on very high-resolution audio data (e.g., data with tens of thousands of samples per second).\nPerformance: For text-to-speech tasks, human listeners rated the outputs of WaveNet as more natural sounding than other leading methods. Additionally, it can switch between different speakers by conditioning on the speaker’s identity. WaveNet can also generate musical fragments that sound realistic.\n\n\n\n\n\nGenerative Model for Audio: WaveNet operates directly on raw audio, predicting the probability of each audio sample based on the previous ones. The model’s structure is inspired by PixelCNN, which was designed for images.\nDilated Causal Convolutions: To ensure that predictions for any timestep don’t depend on future timesteps, the model uses causal convolutions. “Dilated” convolutions are introduced to effectively increase the receptive field (the portion of the input data the model “sees”) without significantly increasing computational cost.\nSoftmax Distributions: Instead of using a mixture model, the paper employs a softmax distribution for modeling audio samples. To manage the high-resolution of raw audio, a µ-law companding transformation is applied to the data before quantizing it.\nGated Activation Units: The paper uses a specific type of activation function for the neural network, which was found to work particularly well for audio signals.\nResidual and Skip Connections: These are techniques to help train deeper neural networks more effectively. They help in faster convergence and enable deeper model architectures.\nConditional WaveNets: WaveNet can be conditioned on additional inputs, which allows it to generate audio with specific characteristics. For example, by conditioning on a speaker’s identity, WaveNet can produce audio in that speaker’s voice. The paper distinguishes between global conditioning (affecting the whole audio) and local conditioning (affecting specific parts of the audio).\nContext Stacks: To increase the receptive field size, the paper introduces the concept of context stacks. These are separate smaller networks that process longer parts of the audio signal and condition the primary WaveNet model.\n\n\n\n\n\nText-to-Speech (TTS): WaveNet can produce very natural-sounding speech, surpassing other state-of-the-art systems.\nVoice Modulation: A single WaveNet model can mimic many different speakers.\nMusic Generation: WaveNet can generate realistic musical fragments.\nOther Audio Tasks: The model is also promising for tasks like speech enhancement, voice conversion, and source separation.\n\nIn essence, WaveNet is a breakthrough in audio generation, offering a versatile and powerful model for a range of audio-related tasks.\n\n\n\n\nThe finished model is inspired by WaveNet, which is a deep learning architecture designed for generating raw audio waveforms.\nInnovation: 1. Hierarchical Fusion of Information: Instead of squashing all the character information into a single layer right at the beginning, the new model aims for a more hierarchical approach. This is akin to WaveNet’s methodology where information from previous contexts gets fused progressively as the network gets deeper. It’s a departure from the original network that was more linear in its approach. 2. FlattenConsecutive Layer: This new layer is essentially reshaping the data by grouping consecutive embeddings, which helps in retaining more granularity of information for longer sequences. 3. Increased Depth with Batch Normalization: The model has added depth, with multiple hidden layers interspersed with BatchNorm layers. Batch Normalization helps in stabilizing and accelerating the training of deeper networks.\nIntuition: 1. Preserving Contextual Information: By not immediately squashing all characters into a single layer, the network retains more of the raw, granular information from the input. This is crucial when predicting the next character based on a sequence of prior characters. The more original context the model has, the better its predictive capability. 2. Progressive Fusion of Information: Just as our human cognition processes information hierarchically (from letters to words to sentences to paragraphs), the model is designed to gradually combine information. It first understands pairs of characters, then bigger chunks, and so on. This allows the model to capture both short-term and long-term dependencies in the data. 3. Stability with Batch Normalization: Deep networks can suffer from internal covariate shift where the distribution of layer inputs changes during training. Batch normalization standardizes the inputs of a layer, making training more stable and faster. 4. Embedding Layer: It’s a look-up table that maps from integer indices (representing specific words or characters) to dense vectors (their embeddings). These vectors are trainable and can capture the semantic relationship between words or characters. By using embeddings, the model can capture richer representations of the input data.\nIn summary, the hierarchical approach is inspired by WaveNet’s methodology of processing audio signals, where the prediction for the next audio sample depends on a gradually fused context of previous samples. By applying a similar approach to character prediction, the model aims to capture richer contextual information, leading to better predictions."
  },
  {
    "objectID": "wavenet.html#wavenet-implementation-based-on-andrej-karpathys-lecture",
    "href": "wavenet.html#wavenet-implementation-based-on-andrej-karpathys-lecture",
    "title": "Wavenet Documentation",
    "section": "",
    "text": "This notebook is a practical follow-up to Andrej Karpathy’s “Building makemore Part 5: Building a WaveNet” lecture. Check out the full lecture here.\nHere’s what we’ll cover:\n\nModel Basics: Introduction to a multi-layer perceptron character-level language model.\nModel Enhancement: Expanding the architecture and input characters for better results.\nWaveNet Overview: Understand WaveNet’s hierarchical structure and its predictions.\nBatch Normalization: Dive into the BatchNorm layer and its challenges.\nPyTorch Containers: A look at how PyTorch structures its layers.\nDataset Expansion: Increase the context length for performance improvement.\nForward Pass: Visualization of tensor transformations in the network.\nBatchNorm1D Bug: Addressing an implementation bug.\nDevelopment Insights: Best practices in deep neural network development.\nOptimizing WaveNet: Suggestions and strategies for better performance.\n\nThis notebook aims to provide a clear understanding of WaveNet’s development and optimization process."
  },
  {
    "objectID": "wavenet.html#wavenet",
    "href": "wavenet.html#wavenet",
    "title": "Wavenet Documentation",
    "section": "",
    "text": "Nature of the Model: WaveNet is a fully probabilistic and autoregressive model. This means that when predicting any given audio sample, it considers all the previous samples.\nEfficiency: It can be trained efficiently on very high-resolution audio data (e.g., data with tens of thousands of samples per second).\nPerformance: For text-to-speech tasks, human listeners rated the outputs of WaveNet as more natural sounding than other leading methods. Additionally, it can switch between different speakers by conditioning on the speaker’s identity. WaveNet can also generate musical fragments that sound realistic.\n\n\n\n\n\nGenerative Model for Audio: WaveNet operates directly on raw audio, predicting the probability of each audio sample based on the previous ones. The model’s structure is inspired by PixelCNN, which was designed for images.\nDilated Causal Convolutions: To ensure that predictions for any timestep don’t depend on future timesteps, the model uses causal convolutions. “Dilated” convolutions are introduced to effectively increase the receptive field (the portion of the input data the model “sees”) without significantly increasing computational cost.\nSoftmax Distributions: Instead of using a mixture model, the paper employs a softmax distribution for modeling audio samples. To manage the high-resolution of raw audio, a µ-law companding transformation is applied to the data before quantizing it.\nGated Activation Units: The paper uses a specific type of activation function for the neural network, which was found to work particularly well for audio signals.\nResidual and Skip Connections: These are techniques to help train deeper neural networks more effectively. They help in faster convergence and enable deeper model architectures.\nConditional WaveNets: WaveNet can be conditioned on additional inputs, which allows it to generate audio with specific characteristics. For example, by conditioning on a speaker’s identity, WaveNet can produce audio in that speaker’s voice. The paper distinguishes between global conditioning (affecting the whole audio) and local conditioning (affecting specific parts of the audio).\nContext Stacks: To increase the receptive field size, the paper introduces the concept of context stacks. These are separate smaller networks that process longer parts of the audio signal and condition the primary WaveNet model.\n\n\n\n\n\nText-to-Speech (TTS): WaveNet can produce very natural-sounding speech, surpassing other state-of-the-art systems.\nVoice Modulation: A single WaveNet model can mimic many different speakers.\nMusic Generation: WaveNet can generate realistic musical fragments.\nOther Audio Tasks: The model is also promising for tasks like speech enhancement, voice conversion, and source separation.\n\nIn essence, WaveNet is a breakthrough in audio generation, offering a versatile and powerful model for a range of audio-related tasks."
  },
  {
    "objectID": "wavenet.html#our-model-gets-improved-using-ideas-from-wavenet",
    "href": "wavenet.html#our-model-gets-improved-using-ideas-from-wavenet",
    "title": "Wavenet Documentation",
    "section": "",
    "text": "The finished model is inspired by WaveNet, which is a deep learning architecture designed for generating raw audio waveforms.\nInnovation: 1. Hierarchical Fusion of Information: Instead of squashing all the character information into a single layer right at the beginning, the new model aims for a more hierarchical approach. This is akin to WaveNet’s methodology where information from previous contexts gets fused progressively as the network gets deeper. It’s a departure from the original network that was more linear in its approach. 2. FlattenConsecutive Layer: This new layer is essentially reshaping the data by grouping consecutive embeddings, which helps in retaining more granularity of information for longer sequences. 3. Increased Depth with Batch Normalization: The model has added depth, with multiple hidden layers interspersed with BatchNorm layers. Batch Normalization helps in stabilizing and accelerating the training of deeper networks.\nIntuition: 1. Preserving Contextual Information: By not immediately squashing all characters into a single layer, the network retains more of the raw, granular information from the input. This is crucial when predicting the next character based on a sequence of prior characters. The more original context the model has, the better its predictive capability. 2. Progressive Fusion of Information: Just as our human cognition processes information hierarchically (from letters to words to sentences to paragraphs), the model is designed to gradually combine information. It first understands pairs of characters, then bigger chunks, and so on. This allows the model to capture both short-term and long-term dependencies in the data. 3. Stability with Batch Normalization: Deep networks can suffer from internal covariate shift where the distribution of layer inputs changes during training. Batch normalization standardizes the inputs of a layer, making training more stable and faster. 4. Embedding Layer: It’s a look-up table that maps from integer indices (representing specific words or characters) to dense vectors (their embeddings). These vectors are trainable and can capture the semantic relationship between words or characters. By using embeddings, the model can capture richer representations of the input data.\nIn summary, the hierarchical approach is inspired by WaveNet’s methodology of processing audio signals, where the prediction for the next audio sample depends on a gradually fused context of previous samples. By applying a similar approach to character prediction, the model aims to capture richer contextual information, leading to better predictions."
  },
  {
    "objectID": "wavenet.html#wavenet-implementation-and-tensor-management",
    "href": "wavenet.html#wavenet-implementation-and-tensor-management",
    "title": "Wavenet Documentation",
    "section": "WaveNet Implementation and Tensor Management",
    "text": "WaveNet Implementation and Tensor Management\n\nForward Pass Visualization\nThe lecturer is working on a neural network implementation of WaveNet. To ensure understanding and correct functioning, they visualize the forward pass by observing tensor shapes at each stage. This helps in understanding data transformations as it progresses through the network.\n\n\nInput Batch and Shape\nA batch of 4 random examples is created for debugging. The shape of the batch (referred to as ( xB )) is ($ 4 $) due to having 4 examples and a block size of 8.\n\n\nEmbedding Layer\nThe first layer is the embedding layer. When the integer tensor ( xB ) is passed through this layer, the output shape becomes ( $4 $). Here, each character has a 10-dimensional vector representation. The embedding layer takes the integers and converts them into these 10-dimensional vectors.\n\n\nFlattening and Concatenation\nThe flattened layer views the ( $4 $) tensor as a ( $4 $) tensor. The effect is that the 10-dimensional embeddings for the 8 characters are lined up in a row, appearing as if they’ve been concatenated.\n\n\nLinear Layer and Matrix Multiplication\nThe linear layer is responsible for transforming the shape from ( $4 \\(\\) to \\(\\) 4 $ ). This is achieved through matrix multiplication. The lecturer emphasizes that in PyTorch, the matrix multiplication operator is versatile and can handle higher-dimensional tensors, treating earlier dimensions as batch dimensions.\n\n\nRestructuring Input\nA key insight is that instead of flattening the entire input, we can group and process parts of it. For instance, the lecturer suggests grouping every two consecutive elements for processing in parallel. This results in a tensor shape of ($ 4 $).\n\n\nFlattening Consecutively\nTo achieve the desired restructuring, the lecturer introduces a new method called “Flatten Consecutive”. This method differs from the regular flattening by allowing for flattening only a specified number of consecutive elements, leading to multi-dimensional outputs rather than fully flattened ones.\n\n\nModel Layers and Parameter Count\nThe lecturer moves on to demonstrate how the neural network layers are organized. They ensure that the number of parameters remains consistent as the model architecture evolves, emphasizing the importance of maintaining model capacity.\n\n\nWaveNet’s Performance\nAfter restructuring the neural network, the lecturer observes that the validation loss remains nearly identical to the original, simpler model. This suggests that, at least in this instance, the added complexity doesn’t yield performance benefits.\n\n\nPotential Issues with BatchNorm1D\nThe lecturer points out that while the model runs, there might still be issues, specifically with the BatchNorm1D layer. A thorough review of this layer is necessary to ensure it’s functioning correctly.\n\nKey Takeaways: 1. Visualizing the forward pass: This helps in understanding data transformations in a neural network. 2. Embeddings: Convert categorical data (like characters) into continuous vectors. 3. Flattening and Reshaping: Managing tensor shapes is crucial, especially when designing custom architectures. 4. Matrix Multiplication in Neural Networks: PyTorch’s matrix multiplication can handle multi-dimensional tensors, treating earlier dimensions as batch dimensions. 5. Model Capacity: When altering a neural network architecture, it’s essential to keep an eye on the number of parameters to ensure model capacity remains consistent. 6. Debugging and Validation: Always ensure that each layer of the neural network is functioning as expected, especially when introducing custom layers or functions."
  },
  {
    "objectID": "wavenet.html#fixing-batchnorm1d-bug",
    "href": "wavenet.html#fixing-batchnorm1d-bug",
    "title": "Wavenet Documentation",
    "section": "Fixing BatchNorm1D Bug",
    "text": "Fixing BatchNorm1D Bug\nThe lecturer begins by discussing a bug related to the BatchNorm1D implementation.\n\nIssue: The current BatchNorm1D implementation assumes a two-dimensional input, but the actual input is three-dimensional. This discrepancy leads to improper calculations.\nCurrent Behavior: The BatchNorm receives an input with dimensions 32x4x68. Although this shape allows the code to run without errors due to broadcasting, it doesn’t work as intended.\nDesired Behavior: The BatchNorm should be modified to consider both the zeroth and first dimensions as batch dimensions. Instead of averaging over 32 numbers, the average should be over (\\(32 \\times 4\\)) numbers for each of the 68 channels.\nSolution: The lecturer suggests using the torch.mean function, which can reduce over multiple dimensions at the same time. By passing in a tuple (0,1) as dimensions, the mean is calculated over both the zeroth and first dimensions, leading to a 1x1x68 shape."
  },
  {
    "objectID": "wavenet.html#deviation-from-pytorch-api",
    "href": "wavenet.html#deviation-from-pytorch-api",
    "title": "Wavenet Documentation",
    "section": "Deviation from PyTorch API",
    "text": "Deviation from PyTorch API\nThere’s a highlighted difference between the lecturer’s implementation and PyTorch’s BatchNorm1D:\n\nPyTorch’s BatchNorm1D: Assumes that when input is three-dimensional, it should be in the form of nxCxL (with C being the number of features or channels).\nLecturer’s Implementation: Assumes the input to be in the form of nxLxC."
  },
  {
    "objectID": "wavenet.html#development-process-of-building-deep-neural-nets",
    "href": "wavenet.html#development-process-of-building-deep-neural-nets",
    "title": "Wavenet Documentation",
    "section": "Development Process of Building Deep Neural Nets",
    "text": "Development Process of Building Deep Neural Nets\nThe lecturer provides insights into the typical process of building and refining deep neural networks:\n\nReference to Documentation: It’s essential to frequently refer to the documentation to understand the various layers, their expected input shapes, and functionalities. However, the lecturer notes that PyTorch documentation can sometimes be misleading or incomplete.\nShape Management: A significant amount of time is spent ensuring tensor shapes are compatible. This involves reshaping tensors, understanding expected input and output shapes, and sometimes prototyping to ensure shapes align.\nPrototyping: The lecturer emphasizes the utility of Jupyter notebooks for prototyping. Once satisfied with the prototype, the code is transferred to a more permanent codebase.\nUse of Convolutions: Convolutions are introduced as a means for efficiency. Instead of processing inputs individually, convolutions allow the model to process multiple inputs simultaneously by sliding filters over the input sequence. This concept connects with future topics, like Convolutional Neural Networks (CNNs)."
  },
  {
    "objectID": "wavenet.html#improving-wavenets-performance",
    "href": "wavenet.html#improving-wavenets-performance",
    "title": "Wavenet Documentation",
    "section": "Improving WaveNet’s Performance",
    "text": "Improving WaveNet’s Performance\nThe lecturer emphasizes the potential for improving the performance of WaveNet:\n\nCurrent Performance: The model’s performance has improved from a loss of 2.1 to 1.993.\nChallenges: The lecturer points out that the current approach lacks an experimental harness, meaning they’re mostly making educated guesses without a systematic way to evaluate changes.\nPotential Improvements: Suggestions include re-allocating channels, tweaking the number of dimensions for embeddings, or even reverting to a simpler network structure. The WaveNet paper itself might also have additional strategies or layers worth implementing."
  },
  {
    "objectID": "wavenet.html#call-to-action",
    "href": "wavenet.html#call-to-action",
    "title": "Wavenet Documentation",
    "section": "Call to Action",
    "text": "Call to Action\nLastly, the lecturer encourages listeners to try and improve upon the WaveNet’s loss of 1.993, pointing out that there’s a lot of potential to optimize the network further."
  },
  {
    "objectID": "wavenet.html#update-embedding-layer",
    "href": "wavenet.html#update-embedding-layer",
    "title": "Wavenet Documentation",
    "section": "Update Embedding Layer",
    "text": "Update Embedding Layer\n\n1. Current Model State:\nThe current model has training and validation losses that are close to each other. This suggests that the model isn’t overfitting. In such cases, a common approach to improve performance is to expand the model: increase its capacity by adding more neurons or layers.\n\n\n2. Problem with Current Architecture:\nRight now, the model takes in a sequence of characters, processes them through a single layer, and predicts the next character. This is somewhat akin to trying to understand a sentence by reading all its words at once. While you can add more layers, you’re still compressing all the information at the very beginning, which might be suboptimal.\n\n\n3. Inspiration from WaveNet:\nWaveNet offers a different approach. Instead of compressing all characters at once, it processes the input in a hierarchical manner. Imagine trying to understand a sentence not word by word, but by understanding two words at a time, then four words, then eight, and so on. This allows the model to capture relationships and patterns at different scales.\n\n\n4. Progressive Fusion:\nThe key idea is to combine (or “fuse”) input data progressively. Start by combining pairs of characters (bigrams). Then, combine pairs of bigrams to form four-character chunks, and so on. This slow fusion ensures that the model has a more refined understanding of the input data at various levels of granularity.\n\n\n5. Dilated Causal Convolutions:\nWhile it sounds complex, the core idea is about efficiency and preserving information. In standard convolutions, each layer can only see a limited portion of the input. By using dilated convolutions, each layer can see a wider range of input, allowing the model to capture longer-term dependencies without needing extremely deep architectures. The “causal” part ensures that the prediction at any time step is only based on past and current data, not future data.\n\n\nConclusion:\nIn essence, the lecturer is suggesting moving from a simplistic model that quickly compresses input information to a more sophisticated architecture that understands the input in a layered and hierarchical manner. This approach, inspired by WaveNet, allows the model to capture patterns and relationships at different scales, potentially leading to better performance. The implementation details, like dilated causal convolutions, are there to ensure efficiency and respect the temporal nature of the data.\n# Layer inspector tool\n\nfor layer in model.layers:\n  print(layer.__class__.__name__, ':', tuple(layer.out.shape) )\n\nEmbedding : (1, 8, 10)\nFlattenConsecutive : (1, 4, 20)\nLinear : (1, 4, 86)\nBatchNorm1d : (1, 4, 86)\nTanh : (1, 4, 86)\nFlattenConsecutive : (1, 2, 172)\nLinear : (1, 2, 86)\nBatchNorm1d : (1, 2, 86)\nTanh : (1, 2, 86)\nFlattenConsecutive : (1, 172)\nLinear : (1, 86)\nBatchNorm1d : (1, 86)\nTanh : (1, 86)\nLinear : (1, 27)\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n\n  # minibatch construct\n  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n  # forward pass\n  logits = model(Xb)\n  loss = F.cross_entropy(logits, Yb) # loss function\n\n  # backward pass\n  for p in parameters:\n    p.grad = None\n  loss.backward()\n\n  # update: simple SGD\n  lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n  for p in parameters:\n    p.data += -lr * p.grad\n\n  # track stats\n  if i % 10000 == 0: # print every once in a while\n    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n  lossi.append(loss.log10().item())\n\n  break\n\n      0/ 200000: 2.0798\n\n\n\nfor layer in model.layers:\n  print(layer.__class__.__name__, ':', tuple(layer.out.shape) )\n\nEmbedding : (32, 8, 10)\nFlattenConsecutive : (32, 4, 20)\nLinear : (32, 4, 86)\nBatchNorm1d : (32, 4, 86)\nTanh : (32, 4, 86)\nFlattenConsecutive : (32, 2, 172)\nLinear : (32, 2, 86)\nBatchNorm1d : (32, 2, 86)\nTanh : (32, 2, 86)\nFlattenConsecutive : (32, 172)\nLinear : (32, 86)\nBatchNorm1d : (32, 86)\nTanh : (32, 86)\nLinear : (32, 27)"
  },
  {
    "objectID": "wavenet.html#random-seeding-in-the-context-of-pytorch-and-neural-network-training.",
    "href": "wavenet.html#random-seeding-in-the-context-of-pytorch-and-neural-network-training.",
    "title": "Wavenet Documentation",
    "section": "Random seeding in the context of PyTorch and neural network training.",
    "text": "Random seeding in the context of PyTorch and neural network training.\n\n1. Purpose of Seeding:\nIn machine learning, especially in neural networks, we often initialize weights and biases randomly. Moreover, when you’re dealing with stochastic processes like dropout, sampling, and other random transformations, the behavior can differ from one run to another due to the randomness. By setting a seed for these random operations, we ensure that the randomness is consistent across multiple runs, making experiments reproducible.\n\n\n2. torch.manual_seed() vs. torch.Generator():\n\ntorch.manual_seed(seed): This sets the seed for the default global generator in PyTorch. Every time you call a function that involves randomness without specifying a generator, it uses the global generator. When you set a manual seed, you’re setting the seed for this global generator. It’s a straightforward way to ensure consistent randomness throughout your program.\ntorch.Generator(): This creates an independent random number generator. You can manually set the seed for this generator and use it for specific operations, keeping it separate from the global generator. This is particularly useful when you want different parts of your code to have different random behaviors, but still want each of those behaviors to be reproducible.\n\n\n\n3. Why not always use torch.manual_seed()?:\nIn many cases, using torch.manual_seed() is sufficient, especially for simpler projects and experiments. However, as your projects grow in complexity, there might be reasons to maintain different seeds:\n\nFine-grained Control: You might want different parts of your code to operate with different seeds. For example, if you’re doing multi-task learning with multiple neural networks, you might want to initialize each network with a different seed, but still want each initialization to be reproducible.\nParallelism: When running operations in parallel, having separate generators can prevent potential synchronization issues and ensure that each parallel operation is consistent across runs.\nIsolation: By using different generators for different parts of your code, you can change one part of your code without affecting the randomness in another part.\n\n\n\nConclusion:\nWhile torch.manual_seed() is a quick and effective method for most use cases, as your projects become more complex, you might find situations where the granularity and control offered by torch.Generator() become necessary. Knowing when and how to use each method appropriately can make your experiments more organized and your results more reliable."
  },
  {
    "objectID": "wavenet.html#high-level-hierarchical-view-of-pytorch-api",
    "href": "wavenet.html#high-level-hierarchical-view-of-pytorch-api",
    "title": "Wavenet Documentation",
    "section": "High-level hierarchical view of PyTorch API",
    "text": "High-level hierarchical view of PyTorch API\nThe PyTorch API is extensive, but I’ll provide a high-level hierarchical view of its core components, which should give you a roadmap for diving deeper:\n\nTensors\n\nCore data structure in PyTorch, similar to NumPy arrays but with GPU support.\ntorch.Tensor class and its various methods.\nCreation: torch.empty(), torch.rand(), torch.zeros(), torch.ones(), torch.tensor(), etc.\nOperations: Mathematical, Reduction, Comparison, Matrix, etc.\nIndexing, Slicing, Joining, Mutating ops: torch.cat(), torch.stack(), etc.\n\nAutograd\n\nAutomatic differentiation library.\ntorch.autograd module.\nVariable: Deprecated, but historically important. All Tensors now have requires_grad attribute.\nFunction: Defines a forward and backward operation. Links to Variable to build a computation graph.\n\nNeural Networks\n\ntorch.nn module.\nLayers: Pre-defined layers like nn.Linear, nn.Conv2d, nn.ReLU, etc.\nLoss functions: nn.CrossEntropyLoss, nn.MSELoss, etc.\nOptimizers: Located in torch.optim, e.g., optim.Adam, optim.SGD.\nUtilities: nn.functional for stateless functions like activation functions.\nnn.Module: Base class for all neural network modules, aiding in organizing code and parameters.\nnn.Sequential: A sequential container for stacking layers.\n\nUtilities\n\nTensor transformations: torchvision.transforms.\nData handling for NN training: torch.utils.data.Dataset, torch.utils.data.DataLoader.\n\nOptimization\n\ntorch.optim module.\nOptimization algorithms like SGD, Adam, RMSProp, etc.\nLearning rate schedulers: Adjust LR on-the-fly during training.\n\nSerialization\n\nSave and load models: torch.save(), torch.load(), nn.Module.load_state_dict(), etc.\n\nDistributed Training\n\ntorch.distributed: For multi-GPU and distributed training.\nBackend support for different communication protocols.\n\nOther Libraries & Extensions\n\ntorchvision: Datasets, models, and image transformations for computer vision.\ntorchaudio: Audio processing tools and datasets.\ntorchtext: NLP data utilities and models.\n\nDevice & CUDA\n\nTensor operations on different devices: CPU, GPU.\nCUDA Tensors: Tensors transferred to GPU.\nDevice management: torch.cuda, torch.device.\n\nJIT Compiler\n\ntorch.jit: Just-In-Time compiler to convert PyTorch models to a representation that can be optimized and run in non-Python environments.\n\nQuantization\n\nReduce the size of models and increase runtime performance.\ntorch.quantization: Contains utilities for model quantization.\n\n\nStart with Tensors and Autograd to get a solid grasp on the basics. Then, you can delve into neural networks with the torch.nn module. After mastering these, choose specialized topics based on your interests and needs."
  },
  {
    "objectID": "wavenet.html#torch.nn.embedding.",
    "href": "wavenet.html#torch.nn.embedding.",
    "title": "Wavenet Documentation",
    "section": "torch.nn.Embedding.",
    "text": "torch.nn.Embedding.\n\n1. The Concept of Embeddings:\nEmbeddings are a powerful tool in the world of deep learning, especially when dealing with categorical data, like words in a language. Instead of representing words or other categorical variables as discrete values (like integers), embeddings represent them as continuous vectors. These vectors capture more information and relationships between different words or categories.\n\n\n2. torch.nn.Embedding:\ntorch.nn.Embedding is PyTorch’s module to create an embedding layer. Essentially, it’s a lookup table that maps from integer indices (representing specific words or categories) to dense vectors (their embeddings).\n\n\n3. Parameters:\n\nnum_embeddings: Total number of distinct categories/words.\nembedding_dim: The size of each embedding vector, i.e., the number of units each embedding should have.\n\n\n\n4. Why Use Embeddings?:\n\nDimensionality Reduction: One-hot encoded vectors can be massive (imagine a vector of length 50,000 for a moderate-sized vocabulary, with all zeros except for a single one). Embeddings condense this information into a much smaller dimension, like 300 for word embeddings.\nCapture Relationships: Embeddings are learned from data. This means that words or categories that have similar meanings or behaviors can have embeddings that are close to each other in the vector space.\nFlexibility: Embeddings can be fine-tuned during training. This means that as a model learns a task, it can also adjust the embeddings to capture any task-specific insights.\n\n\n\n5. Usage:\nAn embedding layer is typically initialized with random weights and will learn an embedding for all the words in the training dataset. It is a flexible layer that can be used in a variety of ways, such as:\n\nPre-trained Embeddings: Sometimes, embeddings are pre-trained on a larger dataset and then fine-tuned on a specific task. Word2Vec, GloVe, and FastText are popular pre-trained word embeddings.\nTask-specific Embeddings: For some tasks, it might be beneficial to let the embedding layer learn embeddings from scratch, tailored to the specific task.\n\n\n\n6. Under the Hood:\nAt its core, an embedding layer is a weight matrix. The rows of this matrix correspond to each category’s unique ID (like a word’s ID), and the columns correspond to the embedding dimensions. When you “pass” an integer to this layer, it returns the corresponding row of the weight matrix. This operation is essentially a lookup, making it efficient.\n\n\nConclusion:\ntorch.nn.Embedding provides an efficient and straightforward way to handle categorical data in neural networks. By converting discrete categorical values into continuous vectors, embeddings enable models to capture intricate relationships in the data and improve performance on a variety of tasks."
  },
  {
    "objectID": "wavenet.html#torch.nn.flatten.",
    "href": "wavenet.html#torch.nn.flatten.",
    "title": "Wavenet Documentation",
    "section": "torch.nn.Flatten.",
    "text": "torch.nn.Flatten.\n\n1. The Basic Idea:\nWhen working with neural networks, especially convolutional neural networks (CNNs), we often deal with multi-dimensional data (like images). After passing this data through several convolutional and pooling layers, we often want to use the resulting multi-dimensional feature maps in fully connected layers (dense layers). However, fully connected layers expect a 1D input. Here’s where torch.nn.Flatten comes in: it’s used to transform multi-dimensional data into a one-dimensional format.\n\n\n2. torch.nn.Flatten:\ntorch.nn.Flatten is a layer provided by PyTorch that reshapes its input into a one-dimensional tensor. It’s effectively a ‘flattening’ operation.\n\n\n3. Parameters:\n\nstart_dim: Dimension to start the flattening. Typically, for a batch of images, the data shape might be [batch_size, channels, height, width]. If we want to flatten the channel, height, and width dimensions, we’d start the flattening from dimension 1 (0-based indexing for dimensions). By default, start_dim is 1.\nend_dim: Dimension to end the flattening. By default, it’s -1, meaning it will flatten all dimensions from start_dim to the last dimension.\n\n\n\n4. Why Use Flatten?:\n\nTransitioning in Architectures: It’s common in CNNs to have convolutional layers followed by dense layers. The flatten layer acts as a bridge between these two, reshaping the output of the convolutional layers to a format that dense layers can work with.\nSimplicity: Instead of manually reshaping tensors using .view() or .reshape(), torch.nn.Flatten provides a more readable and explicit way to flatten data within a model architecture.\n\n\n\n5. Usage:\nImagine you have a batch of images with the shape [batch_size, channels, height, width]. After passing them through convolutional layers, you might get a shape like [batch_size, 64, 7, 7]. Before sending this to a fully connected layer, you’d use the flatten layer:\nflat_layer = torch.nn.Flatten()\nflattened_data = flat_layer(conv_output)\nNow, flattened_data will have a shape [batch_size, 64*7*7], ready to be passed to a fully connected layer.\n\n\n6. In Context:\nIf you’re familiar with other deep learning frameworks, you might recognize this as similar to TensorFlow’s tf.keras.layers.Flatten or Keras’s Flatten layer. It’s a staple in the toolkit of designing deep learning architectures.\n\n\nConclusion:\ntorch.nn.Flatten is a utility layer in PyTorch that streamlines the process of converting multi-dimensional tensors into a one-dimensional format, easing the transition from convolutional layers to fully connected layers in neural network architectures. It’s a straightforward yet crucial component for many deep learning models, particularly CNNs."
  },
  {
    "objectID": "wavenet.html#torch.nn.sequential.",
    "href": "wavenet.html#torch.nn.sequential.",
    "title": "Wavenet Documentation",
    "section": "torch.nn.Sequential.",
    "text": "torch.nn.Sequential.\n\n1. The Basic Idea:\nWhen building neural networks, we often create architectures that involve a series of layers or operations that process data in a specific order. torch.nn.Sequential is a container provided by PyTorch that allows us to encapsulate a sequence of modules or operations into a single module, streamlining both the definition and execution of such sequences.\n\n\n2. torch.nn.Sequential:\nAt its core, torch.nn.Sequential is essentially an ordered container of modules. Data passed to a Sequential module will traverse through each contained module in the order they were added, with the output of one module becoming the input to the next.\n\n\n3. Advantages:\n\nReadability: Architectures, especially simpler ones, become more readable and compact. Instead of defining and calling layers separately, you can consolidate them into a single Sequential block.\nModularity: It allows for easy reuse of certain sequences of operations across different architectures. If a specific sequence of layers gets used frequently, encapsulating it within a Sequential block makes it easier to plug into various models.\n\n\n\n4. Usage:\nSuppose you’re designing a simple feedforward neural network with two hidden layers and ReLU activations:\nWithout Sequential:\nself.fc1 = torch.nn.Linear(input_size, hidden_size)\nself.relu1 = torch.nn.ReLU()\nself.fc2 = torch.nn.Linear(hidden_size, hidden_size)\nself.relu2 = torch.nn.ReLU()\nself.fc3 = torch.nn.Linear(hidden_size, output_size)\nWith Sequential:\nself.layers = torch.nn.Sequential(\n    torch.nn.Linear(input_size, hidden_size),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_size, hidden_size),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_size, output_size)\n)\nThe latter is clearly more concise and readable.\n\n\n5. Points to Remember:\n\nWhile Sequential is convenient, it’s most suited for networks where the data flow is linear. For architectures with branches (like skip connections in ResNets) or multiple inputs/outputs, manual layer definition might be more appropriate.\nModules in Sequential are executed in the order they’re added, making the order crucial. Always ensure that layers are added in the intended sequence.\n\n\n\n6. In Context:\nIf you’re familiar with other deep learning frameworks, the concept might remind you of Keras’s Sequential model. The idea of simplifying linear stacks of layers is a common one across various deep learning libraries, given its convenience.\n\n\nConclusion:\ntorch.nn.Sequential is a convenient tool in the PyTorch library that helps in compactly defining and organizing linear sequences of operations in neural network architectures. While incredibly useful for straightforward, linear data flows, it’s essential to remember its limitations when dealing with more complex architectures.\n## torch.squeeze\nThe torch.squeeze function removes dimensions of size 1 from a tensor. It’s particularly useful when certain operations introduce unwanted singleton dimensions, and you want to revert back to a more compact shape.\nFunction signature:\ntorch.squeeze(input, dim=None, *, out=None)\n\ninput (Tensor): The input tensor.\ndim (int, optional): Specifies which dimension to squeeze. If not specified, all dimensions of size 1 will be squeezed.\nout (Tensor, optional): The output tensor.\n\n\n\nExamples:\n\nSqueezing all dimensions of size 1:\n\nimport torch\n\n# A tensor with shape [1, 3, 1, 2]\nx = torch.tensor([[[[1, 2]], [[3, 4]], [[5, 6]]]])\nprint(x.shape)  # torch.Size([1, 3, 1, 2])\n\ny = torch.squeeze(x)\nprint(y.shape)  # torch.Size([3, 2])\nHere, torch.squeeze removed the first and third dimensions, both of size 1.\n\nSqueezing a specific dimension:\n\nIf you only want to squeeze a specific dimension, you can specify it using the dim argument.\nz = torch.squeeze(x, dim=0)\nprint(z.shape)  # torch.Size([3, 1, 2])\nIn this case, only the first dimension of size 1 was squeezed.\n\nA tensor with no dimensions of size 1:\n\na = torch.tensor([[1, 2], [3, 4]])\nprint(a.shape)  # torch.Size([2, 2])\n\nb = torch.squeeze(a)\nprint(b.shape)  # torch.Size([2, 2])\nAs there were no dimensions of size 1, torch.squeeze had no effect on the tensor’s shape.\n\n\nNote:\nBe cautious when using torch.squeeze without specifying a dimension. In some cases, especially when your tensor might sometimes have singleton dimensions due to variable data sizes (e.g., batch size of 1 in deep learning models), unintended squeezing might lead to shape mismatches or other errors in subsequent operations.\n## torch.randint\n\n\n1. torch.randint:\ntorch.randint is a PyTorch function that returns a tensor filled with random integers generated uniformly between two specified integer values (low and high).\nThe function signature is:\ntorch.randint(low=0, high, size, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n\nlow (int, optional): Lowest integer to be drawn from the distribution. Default: 0.\nhigh (int): One above the highest integer to be drawn from the distribution.\nsize (tuple): The shape of the output tensor.\nAdditional arguments like dtype, device, and requires_grad allow you to further specify the nature of the returned tensor.\n\n\n\n2. Given Line:\nThis would produce a 1D tensor with 4 random integer values in the specified range.\nThis line aims to generate a tensor of random integer values between 0 (inclusive) and Xtr.shape[0] (exclusive).\nix = torch.randint(0, Xtr.shape[0], (4,))"
  },
  {
    "objectID": "wavenet.html#convolutional-neural-network-cnn-in-simple-terms",
    "href": "wavenet.html#convolutional-neural-network-cnn-in-simple-terms",
    "title": "Wavenet Documentation",
    "section": "Convolutional Neural Network (CNN) in simple terms",
    "text": "Convolutional Neural Network (CNN) in simple terms\nImagine you’re trying to identify objects in a large photograph, but instead of looking at the entire picture all at once, you use a small magnifying glass to focus on specific parts of the picture one at a time. As you move the magnifying glass over the picture, you notice different features: maybe the edge of a building, the curve of a car, or the pattern of a shirt.\nA Convolutional Neural Network (CNN) operates in a similar manner when processing images. Instead of analyzing the entire image in one go, it uses “filters” (akin to our magnifying glass) to scan through the image and detect specific features. These features could be edges, textures, colors, and more.\nAs the CNN processes the image through multiple layers: 1. Early layers might recognize simple patterns like lines and edges. 2. Middle layers might recognize more complex structures, like shapes or specific textures. 3. Deeper layers might recognize high-level features, such as a human face or a dog.\nTo continue with our analogy, after examining the entire photograph using the magnifying glass, you would then combine all the features you’ve noticed to determine what the entire image represents. Similarly, after detecting various features in an image, a CNN combines them in its later layers to determine the object in the image (e.g., “This is a picture of a cat”).\nIn summary, a CNN processes images by scanning them for features using filters and then combines these features in sophisticated ways to make decisions about what the image contains. This structure makes CNNs particularly good at understanding visual data."
  },
  {
    "objectID": "uml_diagrams.html",
    "href": "uml_diagrams.html",
    "title": "UML Diagrams",
    "section": "",
    "text": "import base64\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\n\ndef mm(graph):\n    graphbytes = graph.encode(\"utf8\")\n    base64_bytes = base64.b64encode(graphbytes)\n    base64_string = base64_bytes.decode(\"ascii\")\n    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
  },
  {
    "objectID": "uml_diagrams.html#sequence-diagram",
    "href": "uml_diagrams.html#sequence-diagram",
    "title": "UML Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nThe sequence diagram that follows shows how the developer, IDE, model, and training process interact with one another. The process begins with the developer writing code in the IDE, the IDE sending the model the current code snippet, and the model using the machine learning model it has been trained to produce recommendations. After that, the developer can engage with the recommendations and offer input that helps the model get better over time by periodically retraining it. As the developer integrates the recommendations into their code, the process proceeds.\n\nmm(\"\"\"\nsequenceDiagram\n    participant Developer\n    participant IDE\n    participant Model\n    participant Dataset\n    participant MLTraining\n\n    Developer-&gt;&gt;IDE: Starts coding in IDE\n    IDE-&gt;&gt;Model: Developer's current code snippet\n    Model-&gt;&gt;MLTraining: Retrieve context from model\n    MLTraining--&gt;&gt;Model: Trains machine learning model\n    Model--&gt;&gt;IDE: Trained model\n\n    alt Code Completion Activated\n        Developer-&gt;&gt;IDE: Continues typing\n        IDE-&gt;&gt;Model: Current code snippet\n        Model-&gt;&gt;Model: Generate code suggestions\n        Model--&gt;&gt;IDE: List of suggestions\n    else User Interaction\n        Developer-&gt;&gt;IDE: Reviews and selects suggestions\n        IDE-&gt;&gt;Model: User feedback\n        Model--&gt;&gt;MLTraining: Update model with feedback\n        MLTraining--&gt;&gt;Model: Retrain model\n    end\n\n    Developer--&gt;&gt;IDE: Continues coding with suggestions\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#class-diagram",
    "href": "uml_diagrams.html#class-diagram",
    "title": "UML Diagrams",
    "section": "Class Diagram",
    "text": "Class Diagram\nThe primary components and their interactions in an intelligent code completion system are depicted in this class diagram, which also emphasizes the information flow between the training component, the machine learning model, the integrated development environment, and the developer. However they might now reflect same in the code as we might change them according to the need.\n\nmm(\"\"\"\nclassDiagram\n  class Developer {\n    +writeCode()\n  }\n\n  class IDE {\n    +sendCodeSnippet()\n  }\n\n  class Model {\n    +generateCodeSuggestions()\n    +receiveUserFeedback()\n  }\n\n  class MLTraining {\n    +trainModel()\n    +updateModel()\n  }\n\n  Developer --&gt; IDE: Uses\n  IDE --&gt; Model: Sends code snippet\n  Model --&gt; MLTraining: Trains on dataset\n  Model --&gt; IDE: Sends suggestions\n  Model --&gt; MLTraining: Receives user feedback\n  MLTraining --&gt; Model: Updates model\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#state-machine-diagram",
    "href": "uml_diagrams.html#state-machine-diagram",
    "title": "UML Diagrams",
    "section": "State Machine Diagram",
    "text": "State Machine Diagram\nThe system is shown in the state machine diagram as “Coding” upon startup. It changes to the “Code Suggestions” state when code completion is enabled. In the event that the user interacts with the suggestions, the system can either move to the “User Interaction” stage or revert to the “Coding” state. The “User Interaction” state permits the developer to type further or approve/disapprove recommendations.\n\nmm(\"\"\"\nstateDiagram\n  state \"Coding\" as Coding\n  state \"Code Suggestions\" as Suggestions\n  state \"User Interaction\" as Interaction\n\n  [*] --&gt; Coding\n\n  Coding --&gt; Suggestions: Code Completion Activated\n  Suggestions --&gt; Coding: Suggestions Rejected\n  Suggestions --&gt; Interaction: User Interaction\n\n  Interaction --&gt; Suggestions: Continue Typing\n  Interaction --&gt; Coding: Suggestions Accepted\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#user-journey-diagram",
    "href": "uml_diagrams.html#user-journey-diagram",
    "title": "UML Diagrams",
    "section": "User Journey Diagram",
    "text": "User Journey Diagram\nThe steps a developer takes to use the intelligent code completion system and get started with coding in the IDE are shown in this user journey diagram. Code completion, suggestion creation, user interaction, and the loop of continual improvement via user feedback and model retraining are all included.\n\nmm(\"\"\"\njourney\n  title Developer's Journey with Intelligent Code Completion\n\n  section Getting Started\n    Developer --&gt; IDE: Starts coding in IDE\n\n  section Code Completion Activated\n    IDE --&gt; Model: Developer's current code snippet\n    Model --&gt; MLTraining: Retrieve context from 70GB C code dataset\n    MLTraining --&gt;&gt; Model: Train machine learning model\n    Model --&gt;&gt; IDE: Trained model\n    Developer --&gt; IDE: Continues typing\n\n  section Suggestions\n    IDE --&gt; Model: Current code snippet\n    Model --&gt; Model: Generate code suggestions\n    Model --&gt;&gt; IDE: List of suggestions\n    Developer --&gt; IDE: Reviews and selects suggestions\n\n  section User Interaction\n    IDE --&gt; Model: User feedback\n    Model --&gt;&gt; MLTraining: Update model with feedback\n    MLTraining --&gt;&gt; Model: Retrain model\n    Model --&gt;&gt; IDE: Updated model\n    Developer --&gt; IDE: Continues coding with suggestions\n\n  section Conclusion\n    Developer --&gt; IDE: Finishes coding\n\n\"\"\")"
  }
]
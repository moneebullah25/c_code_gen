[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "c_code_gen",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "c_code_gen",
    "section": "Install",
    "text": "Install\npip install c_code_gen"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "c_code_gen",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\nimport torch\n\n\nif __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n    trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)\n    src_pad_idx = 0 # index of the padding token in source vocabulary\n    trg_pad_idx = 0 # index of the padding token in target vocabulary\n    src_vocab_size = 10 # number of unique tokens in source vocabulary\n    trg_vocab_size = 10 # number of unique tokens in target vocabulary\n    \n    print(f\"Input shape: {x.shape}\")\n    print(f\"Target shape: {trg.shape}\")\n    print(f\"Device available: {device}\")\n    \n    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n    out = model(x, trg[:, :-1])\n    print(f\"Output shape: {out.shape}\")\n    print(f\"Output: {out}\")\n\nInput shape: torch.Size([2, 9])\nTarget shape: torch.Size([2, 8])\nDevice available: cuda\nOutput shape: torch.Size([2, 7, 10])\nOutput: tensor([[[ 0.4464, -0.5610, -0.7114,  0.9363,  0.6822, -1.5460, -0.5701,\n          -0.0960,  0.4808,  0.1313],\n         [-0.7031,  0.6683, -0.6762, -0.0794, -0.2506, -0.9622, -0.1848,\n          -0.7650, -0.2054, -0.5386],\n         [-0.0112,  0.4172, -0.1490, -1.0593,  0.2641, -0.8530, -0.3859,\n          -0.3926, -0.3144, -0.1417],\n         [ 0.4123,  0.3738,  0.6268,  0.8212,  1.1357, -1.1602, -0.0434,\n          -1.7120,  0.1721, -0.5142],\n         [-0.5740,  0.6748,  0.4170,  1.0975, -0.0173, -0.5885, -1.8482,\n           0.1379,  0.7698, -0.3862],\n         [ 0.6030, -0.1450, -0.4451,  1.1064,  0.1838, -1.0696, -0.4320,\n           0.0764,  0.5091, -0.2963],\n         [ 0.0264,  0.1590, -0.4393,  0.9079,  0.7149, -1.4549,  0.1765,\n           0.3150,  0.3267, -0.9601]],\n\n        [[ 0.5317, -0.5054, -0.6930,  0.9477,  0.7169, -1.3674, -0.5864,\n          -0.1622,  0.5145,  0.1502],\n         [-0.5181,  1.1593, -0.3028,  0.6865, -0.1220, -0.7017, -1.0549,\n          -0.4249, -0.0154, -0.7563],\n         [ 0.0823,  0.9191, -0.1109,  0.1114,  0.0602, -1.0653, -0.8787,\n           0.1198, -0.4894, -0.1040],\n         [ 0.1353,  0.4007,  0.1736,  0.0703,  1.2294, -1.2375, -0.2426,\n          -1.0955,  0.2159, -0.7532],\n         [ 0.1648,  0.3638, -0.1407, -0.5300,  0.3209, -0.3451, -1.0195,\n           0.1148,  0.8064,  0.1274],\n         [-0.6308,  0.8116, -0.6778,  0.9686,  0.0346, -0.8795, -0.4404,\n          -0.3469,  0.3887, -0.2115],\n         [ 0.1550,  0.5274, -0.3766,  0.5200, -0.2350, -1.2167, -0.4607,\n           0.2098, -0.1927, -0.5351]]], device='cuda:0',\n       grad_fn=&lt;ViewBackward0&gt;)"
  },
  {
    "objectID": "nano_gpt.html",
    "href": "nano_gpt.html",
    "title": "Nano GPT",
    "section": "",
    "text": "with open(\"tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n    text = f.read()\n\nprint(f\"length of dataset in characters: {len(text)}\")\n\nlength of dataset in characters: 1115394\n\n\n\nprint(f\"first 1000 characters of dataset:\\n{text[:1000]}\")\n\nfirst 1000 characters of dataset:\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n\n\n\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nprint(f\"all chars are:{''.join(chars)}\")\nprint(f\"vocab size: {vocab_size}\")\n\nall chars are:\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nvocab size: 65\n\n\n\nstoi = {ch:i for i, ch in enumerate(chars)}\nitos = {val:key for key, val in stoi.items()}\n\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\n\nprint(encode(\"Hello, world!\"))\nprint(decode(encode(\"Hello, world!\")))\n\n[20, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42, 2]\nHello, world!\n\n\n\nimport torch\ndata = torch.tensor(encode(text), dtype = torch.long)\n\nprint(f\"data shape: {data.shape}\")\nprint(f\"data type: {data.dtype}\")\n\nprint(\"-\"*50)\n\nprint(f\"first 100 characters of dataset:\\n{data[:100]}\")\nprint(f\"first 100 characters of dataset:\\n{decode(data[:100].tolist())}\")\n\ndata shape: torch.Size([1115394])\ndata type: torch.int64\n--------------------------------------------------\nfirst 100 characters of dataset:\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\nfirst 100 characters of dataset:\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n\n\n\n\n\n\nn = int(0.9 * len(data))\n\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\n\nblock_size = 8\ntrain_data[:block_size + 1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\n\nfor t in range(block_size):\n    context = x[:t + 1]\n    target = y[t]\n    print(f\"when input tensor is {context.tolist()}, target is {target}\")\n\nwhen input tensor is [18], target is 47\nwhen input tensor is [18, 47], target is 56\nwhen input tensor is [18, 47, 56], target is 57\nwhen input tensor is [18, 47, 56, 57], target is 58\nwhen input tensor is [18, 47, 56, 57, 58], target is 1\nwhen input tensor is [18, 47, 56, 57, 58, 1], target is 15\nwhen input tensor is [18, 47, 56, 57, 58, 1, 15], target is 47\nwhen input tensor is [18, 47, 56, 57, 58, 1, 15, 47], target is 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel\nblock_size = 8 # what is the maximum context length for predictions?\n\n# number of input_examples = batch_size * block_size (4 * 8 = 32)\n\ndef get_batch(split):\n    # Select the appropriate dataset based on the split parameter\n    data = train_data if split == \"train\" else val_data\n\n    # Generate a batch of random starting indices within the dataset\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n\n    # Select a block of text of size block_size starting from each random index\n    x = torch.stack([data[i:i+block_size] for i in ix])\n\n    # Shift the selected block of text by one character to the right to create the target sequence\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch(\"train\")\nprint(f\"inputs:\\nshape{xb.shape}\\ndata: {xb}\")\nprint(f\"targets:\\nshape{yb.shape}\\ndata: {yb}\")\n\ninputs:\nshapetorch.Size([4, 8])\ndata: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\nshapetorch.Size([4, 8])\ndata: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])"
  },
  {
    "objectID": "nano_gpt.html#opening-and-exploring-the-data",
    "href": "nano_gpt.html#opening-and-exploring-the-data",
    "title": "Nano GPT",
    "section": "",
    "text": "with open(\"tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n    text = f.read()\n\nprint(f\"length of dataset in characters: {len(text)}\")\n\nlength of dataset in characters: 1115394\n\n\n\nprint(f\"first 1000 characters of dataset:\\n{text[:1000]}\")\n\nfirst 1000 characters of dataset:\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge."
  },
  {
    "objectID": "nano_gpt.html#tokenization",
    "href": "nano_gpt.html#tokenization",
    "title": "Nano GPT",
    "section": "",
    "text": "chars = sorted(list(set(text)))\nvocab_size = len(chars)\n\nprint(f\"all chars are:{''.join(chars)}\")\nprint(f\"vocab size: {vocab_size}\")\n\nall chars are:\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nvocab size: 65\n\n\n\nstoi = {ch:i for i, ch in enumerate(chars)}\nitos = {val:key for key, val in stoi.items()}\n\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: ''.join([itos[i] for i in l])\n\nprint(encode(\"Hello, world!\"))\nprint(decode(encode(\"Hello, world!\")))\n\n[20, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42, 2]\nHello, world!\n\n\n\nimport torch\ndata = torch.tensor(encode(text), dtype = torch.long)\n\nprint(f\"data shape: {data.shape}\")\nprint(f\"data type: {data.dtype}\")\n\nprint(\"-\"*50)\n\nprint(f\"first 100 characters of dataset:\\n{data[:100]}\")\nprint(f\"first 100 characters of dataset:\\n{decode(data[:100].tolist())}\")\n\ndata shape: torch.Size([1115394])\ndata type: torch.int64\n--------------------------------------------------\nfirst 100 characters of dataset:\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\nfirst 100 characters of dataset:\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou"
  },
  {
    "objectID": "nano_gpt.html#train-validation-split",
    "href": "nano_gpt.html#train-validation-split",
    "title": "Nano GPT",
    "section": "",
    "text": "n = int(0.9 * len(data))\n\ntrain_data = data[:n]\nval_data = data[n:]"
  },
  {
    "objectID": "nano_gpt.html#create-dataloader",
    "href": "nano_gpt.html#create-dataloader",
    "title": "Nano GPT",
    "section": "",
    "text": "block_size = 8\ntrain_data[:block_size + 1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\nx = train_data[:block_size]\ny = train_data[1:block_size + 1]\n\nfor t in range(block_size):\n    context = x[:t + 1]\n    target = y[t]\n    print(f\"when input tensor is {context.tolist()}, target is {target}\")\n\nwhen input tensor is [18], target is 47\nwhen input tensor is [18, 47], target is 56\nwhen input tensor is [18, 47, 56], target is 57\nwhen input tensor is [18, 47, 56, 57], target is 58\nwhen input tensor is [18, 47, 56, 57, 58], target is 1\nwhen input tensor is [18, 47, 56, 57, 58, 1], target is 15\nwhen input tensor is [18, 47, 56, 57, 58, 1, 15], target is 47\nwhen input tensor is [18, 47, 56, 57, 58, 1, 15, 47], target is 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel\nblock_size = 8 # what is the maximum context length for predictions?\n\n# number of input_examples = batch_size * block_size (4 * 8 = 32)\n\ndef get_batch(split):\n    # Select the appropriate dataset based on the split parameter\n    data = train_data if split == \"train\" else val_data\n\n    # Generate a batch of random starting indices within the dataset\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n\n    # Select a block of text of size block_size starting from each random index\n    x = torch.stack([data[i:i+block_size] for i in ix])\n\n    # Shift the selected block of text by one character to the right to create the target sequence\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch(\"train\")\nprint(f\"inputs:\\nshape{xb.shape}\\ndata: {xb}\")\nprint(f\"targets:\\nshape{yb.shape}\\ndata: {yb}\")\n\ninputs:\nshapetorch.Size([4, 8])\ndata: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\nshapetorch.Size([4, 8])\ndata: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])"
  },
  {
    "objectID": "nano_gpt.html#creating-the-model",
    "href": "nano_gpt.html#creating-the-model",
    "title": "Nano GPT",
    "section": "Creating the Model",
    "text": "Creating the Model\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a loockup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets = None):\n        # idx and targets are both (B, T) tensor of ints\n        logits = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n        \n        if targets is None:\n            loss = None\n        else:\n            # note that F.cross_entropy accepts inputs in shape (B, C, T)\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T) # can be as targets = targets.view(-1)\n            \n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the logits for the next token\n            logits, loss = self(idx)\n            # focus only on the last time step\n            # (note that we are feeding the whole context each time, however we only care about the last prediction)\n            # (this make doesn't make sense now, but the function will be modified later)\n            logits = logits[:, -1, :] # Becomes (B, C) (get the last time step for each sequence)\n            # apply softmax to convert to probabilities\n            probs = F.softmax(logits, dim = -1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled token to the context\n            idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n        return idx\n    \n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\n\nprint(f\"logits shape: {logits.shape}\")\nprint(f\"loss: {loss} | we are expecting a loss of around {torch.log(torch.tensor(vocab_size))}\")\n\nlogits shape: torch.Size([32, 65])\nloss: 4.878634929656982 | we are expecting a loss of around 4.174387454986572\n\n\n\nidx = torch.zeros((1,1), dtype = torch.long)\ngenerated = m.generate(idx, 100) # shape (1, 101)\nprint(decode(generated[0].tolist()))\n\n\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3"
  },
  {
    "objectID": "nano_gpt.html#training-the-model",
    "href": "nano_gpt.html#training-the-model",
    "title": "Nano GPT",
    "section": "Training the Model",
    "text": "Training the Model\n\noptimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)\nbatch_size = 32\nlossi = []\n\nfor i in range(10000):\n    # sample a batch of training data\n    xb, yb = get_batch(\"train\")\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none = True)\n    loss.backward()\n    optimizer.step()\n\n    lossi.append(loss.item())\n\nprint(f\"loss: {loss.item()}\")\n\nloss: 2.5727508068084717\n\n\n\nplt.plot(lossi);\n\n\n\n\n\n# sampling from the model\nidx = torch.zeros((1,1), dtype = torch.long)\ngenerated = m.generate(idx, 400) # shape (1, 101)\nprint(decode(generated[0].tolist()))\n\n\nIyoteng h hasbe pave pirance\nRie hicomyonthar's\nPlinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\nKIN d pe wither vouprrouthercc.\nhathe; d!\nMy hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\nh hay.JUCle n prids, r loncave w hollular s O:\nHIs; ht anjx?\n\nDUThinqunt.\n\nLaZAnde.\nathave l.\nKEONH:\nARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenou"
  },
  {
    "objectID": "nano_gpt.html#porting-our-code-to-a-script",
    "href": "nano_gpt.html#porting-our-code-to-a-script",
    "title": "Nano GPT",
    "section": "Porting our code to a script",
    "text": "Porting our code to a script\ncheck bigram.py\n\n!python3 bigram.py -h\n\nusage: bigram.py [-h] [--batch_size BATCH_SIZE] [--block_size BLOCK_SIZE]\n                 [--max_iters MAX_ITERS] [--eval_interval EVAL_INTERVAL]\n                 [--learning_rate LEARNING_RATE] [--eval_iters EVAL_ITERS]\n                 [--n_embed N_EMBED] [--n_layer N_LAYER] [--n_head N_HEAD]\n                 [--dropout DROPOUT]\n\nProcess some integers.\n\noptions:\n  -h, --help            show this help message and exit\n  --batch_size BATCH_SIZE\n                        how many independent sequences will we process in\n                        parallel (default: 4)\n  --block_size BLOCK_SIZE\n                        what is the maximum context length for predictions?\n                        (default: 8)\n  --max_iters MAX_ITERS\n                        how many training iterations do we want? (default:\n                        3000)\n  --eval_interval EVAL_INTERVAL\n                        how often do we evaluate the loss on train and val?\n                        (default: 300)\n  --learning_rate LEARNING_RATE\n                        what is the learning rate for the optimizer? (default:\n                        0.001)\n  --eval_iters EVAL_ITERS\n                        how many batches we average the loss over for train\n                        and val (default: 200)\n  --n_embed N_EMBED     how many dimensions do we want to embed the tokens in?\n                        (default: 32)\n  --n_layer N_LAYER     how many layers of transformer blocks (default: 3)\n  --n_head N_HEAD       how many heads do we want to use? (default: 4)\n  --dropout DROPOUT     what dropout probability do we want to use? (default:\n                        0.1)\n\n\n\n!python3 bigram.py\n\n0.209729 M parameters\n step 0 | train loss: 4.4116 | val loss: 4.4048\n step 100 | train loss: 2.6539 | val loss: 2.6605\n step 200 | train loss: 2.5140 | val loss: 2.5161\n step 300 | train loss: 2.4039 | val loss: 2.4126\n step 400 | train loss: 2.3430 | val loss: 2.3506\n step 500 | train loss: 2.2918 | val loss: 2.3002\n step 600 | train loss: 2.2347 | val loss: 2.2517\n step 700 | train loss: 2.1930 | val loss: 2.2089\n step 800 | train loss: 2.1556 | val loss: 2.1793\n step 900 | train loss: 2.1211 | val loss: 2.1507\n step 1000 | train loss: 2.0768 | val loss: 2.1179\n step 1100 | train loss: 2.0615 | val loss: 2.1029\n step 1200 | train loss: 2.0300 | val loss: 2.0744\n step 1300 | train loss: 2.0145 | val loss: 2.0577\n step 1400 | train loss: 1.9936 | val loss: 2.0542\n step 1500 | train loss: 1.9759 | val loss: 2.0375\n step 1600 | train loss: 1.9503 | val loss: 2.0281\n step 1700 | train loss: 1.9273 | val loss: 2.0172\n step 1800 | train loss: 1.9151 | val loss: 2.0030\n step 1900 | train loss: 1.9089 | val loss: 2.0017\n step 2000 | train loss: 1.8805 | val loss: 1.9853\n step 2100 | train loss: 1.8700 | val loss: 1.9754\n step 2200 | train loss: 1.8716 | val loss: 1.9654\n step 2300 | train loss: 1.8520 | val loss: 1.9508\n step 2400 | train loss: 1.8433 | val loss: 1.9484\n step 2500 | train loss: 1.8300 | val loss: 1.9462\n step 2600 | train loss: 1.8120 | val loss: 1.9332\n step 2700 | train loss: 1.8117 | val loss: 1.9292\n step 2800 | train loss: 1.8059 | val loss: 1.9191\n step 2900 | train loss: 1.7842 | val loss: 1.8985\n step 3000 | train loss: 1.7776 | val loss: 1.8973\n step 3100 | train loss: 1.7803 | val loss: 1.9138\n step 3200 | train loss: 1.7770 | val loss: 1.9130\n step 3300 | train loss: 1.7566 | val loss: 1.8956\n step 3400 | train loss: 1.7586 | val loss: 1.8909\n step 3500 | train loss: 1.7417 | val loss: 1.8878\n step 3600 | train loss: 1.7340 | val loss: 1.8668\n step 3700 | train loss: 1.7352 | val loss: 1.8874\n step 3800 | train loss: 1.7179 | val loss: 1.8655\n step 3900 | train loss: 1.7237 | val loss: 1.8702\n step 4000 | train loss: 1.7193 | val loss: 1.8635\n step 4100 | train loss: 1.7079 | val loss: 1.8657\n step 4200 | train loss: 1.7085 | val loss: 1.8462\n step 4300 | train loss: 1.7024 | val loss: 1.8440\n step 4400 | train loss: 1.6880 | val loss: 1.8442\n step 4500 | train loss: 1.6986 | val loss: 1.8520\n step 4600 | train loss: 1.6957 | val loss: 1.8502\n step 4700 | train loss: 1.6892 | val loss: 1.8477\n step 4800 | train loss: 1.6831 | val loss: 1.8370\n step 4900 | train loss: 1.6708 | val loss: 1.8188\n\n\nClown:\nRorince, and is so blorbacest bobe to take On.\n\nMARCILIA:\nI'll bapitius have bury, delay ane away, what,\nDound my know\nYourself friance!\nNor linding, and if eye, hone; and maid overs, and the news\nAnd about liking tear.\nHis mild him speak; and allw youngs\nPause and at lives home.\nWho like again Wich to draugions to them,\nWill we hide honour more-forrument of thrupted so;\nAnging must with all of which Priently of.\n\nHENRY VI\n\nJOHN Y:\n\nSaday Warwick forth in couragain.\n\nCRINIUS:\n\nAnd forwic"
  },
  {
    "objectID": "nano_gpt.html#using-explicit-loops",
    "href": "nano_gpt.html#using-explicit-loops",
    "title": "Nano GPT",
    "section": "Using Explicit loops",
    "text": "Using Explicit loops\n\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 2 # batch size, time, channels\nx = torch.randn(B, T, C)\nprint(f\"x shape: {x.shape}\")\n\n# We want x[b, t] = mean_(i&lt;=t) x[b, i]\nxbow = torch.zeros((B, T, C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b, :t+1] # (t, C)\n        xbow[b, t] = torch.mean(xprev, dim = 0) # average over time dimension (t)\n\nx shape: torch.Size([4, 8, 2])\n\n\n\n# Let's Check the first Batch\nprint(f\"x[0]: {x[0]}\")\nprint(f\"xbow[0]: {xbow[0]}\")\n\n# the first row is the same \nprint(x[0, 0] == xbow[0, 0])\n# the second row is the average of the first two rows\nprint((x[0, 0] + x[0, 1]) / 2 == xbow[0, 1])\n# etc ...\n\nx[0]: tensor([[ 0.1808, -0.0700],\n        [-0.3596, -0.9152],\n        [ 0.6258,  0.0255],\n        [ 0.9545,  0.0643],\n        [ 0.3612,  1.1679],\n        [-1.3499, -0.5102],\n        [ 0.2360, -0.2398],\n        [-0.9211,  1.5433]])\nxbow[0]: tensor([[ 0.1808, -0.0700],\n        [-0.0894, -0.4926],\n        [ 0.1490, -0.3199],\n        [ 0.3504, -0.2238],\n        [ 0.3525,  0.0545],\n        [ 0.0688, -0.0396],\n        [ 0.0927, -0.0682],\n        [-0.0341,  0.1332]])\ntensor([True, True])\ntensor([True, True])"
  },
  {
    "objectID": "nano_gpt.html#using-matrix-multiplication",
    "href": "nano_gpt.html#using-matrix-multiplication",
    "title": "Nano GPT",
    "section": "Using Matrix Multiplication",
    "text": "Using Matrix Multiplication\n\nInstead of nested loops, we can make it using matrix multiplication\nThis can be done by multiplying the matrix with lower triangular matrix\n\n\ntorch.manual_seed(42)\n# lower triangular matrix of ones\na = torch.tril(torch.ones(3,3)) \n# make all rows sum to 1\na = a / torch.sum(a, 1, keepdim = True)\n# create a random matrix\nb = torch.randint(0, 10, (3, 2)).float() \n\nc = a @ b\nprint(f\"a (shape = {a.shape}) =\\n{a}\")\nprint(f\"b (shape = {b.shape}) =\\n{b}\")\nprint(f\"c (shape = {c.shape}) =\\n{c}\")\n\na (shape = torch.Size([3, 3])) =\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\nb (shape = torch.Size([3, 2])) =\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\nc (shape = torch.Size([3, 2])) =\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n# We want x[b, t] = mean_(i&lt;=t) x[b, i]\nwei = torch.tril(torch.ones(T, T)) # (T, T)\n# make all rows sum to 1\nwei = wei / torch.sum(wei, 1, keepdim = True) # (T, T)\nxbow2 = wei @ x # (T, T) @ (B, T, C) ----broadcasting----&gt; (B, T, T) @ (B, T, C) ➡️ (B, T, C)\n\n# check if xbow2 is the same as xbow\nprint(torch.allclose(xbow, xbow2, atol = 1e-7))\n\nTrue"
  },
  {
    "objectID": "nano_gpt.html#using-softmax",
    "href": "nano_gpt.html#using-softmax",
    "title": "Nano GPT",
    "section": "Using Softmax",
    "text": "Using Softmax\n\ntril = torch.tril(torch.ones(T, T))\n# we start with zeros, but later these will be replaced with data dependent values (affinities)\nwei = torch.zeros((T, T))\n# masked_fill: for all elements where tril == 0, replace with float(\"-inf\")\nwei = wei.masked_fill(tril == 0, float(\"-inf\"))\nprint(f\"wei:\\n{wei}\")\nwei = F.softmax(wei, dim = -1)\nprint(f\"wei:\\n{wei}\")\nxbow3 = wei @ x\n\n# check if xbow3 is the same as xbow\nprint(torch.allclose(xbow, xbow3, atol = 1e-7))\n\nwei:\ntensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\nwei:\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\nTrue"
  },
  {
    "objectID": "nano_gpt.html#adding-variable-embedding-size",
    "href": "nano_gpt.html#adding-variable-embedding-size",
    "title": "Nano GPT",
    "section": "Adding variable embedding size",
    "text": "Adding variable embedding size\n\nRemoving vocab_size from the constructor of BigramLanguageModel class, since it’s already defiend above\nModifying the embedding layer to has an output size of n_embed instead of vocab_size\nAdding a linear layer with vocab_size outputs after the embedding layer\n\n\nclass BigramLanguageModel(nn.Module):\n    # no need to pass vocab_size as an argument, since it is a global variable in this file\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a loockup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        # the output layer is a linear layer with vocab_size outputs\n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets = None):\n        # idx and targets are both (B, T) tensor of ints\n        token_emb = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n        logits = self.lm_head(token_emb) # (B, T, vocab_size) = (4, 8, vocab_size)\n    \n    # rest of the code .."
  },
  {
    "objectID": "nano_gpt.html#positional-encoding",
    "href": "nano_gpt.html#positional-encoding",
    "title": "Nano GPT",
    "section": "Positional Encoding",
    "text": "Positional Encoding\n\nAdding a positional encoding layer to the model self.position_embedding_table\nAdding the positional encoding to the input embeddings x = token_emb + pos_emb\n\n\nclass BigramLanguageModel(nn.Module):\n    # no need to pass vocab_size as an argument, since it is a global variable in this file\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a loockup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        # each position is also associated with an embedding vector\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        # the output layer is a linear layer with vocab_size outputs\n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets = None):\n        B, T = idx.shape\n        # idx and targets are both (B, T) tensor of ints\n        token_emb = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n        pos_emb = self.position_embedding_table(torch.arange(T, device = idx.device)) # (T, C) = (8, vocab_size)\n        # x has the token identities + the position embeddings\n        x = token_emb + pos_emb # (B, T, C) = (4, 8, vocab_size)\n        logits = self.lm_head(x) # (B, T, vocab_size) = (4, 8, vocab_size)\n    \n    # rest of the code .."
  },
  {
    "objectID": "nano_gpt.html#previous-code-from-part-3",
    "href": "nano_gpt.html#previous-code-from-part-3",
    "title": "Nano GPT",
    "section": "Previous code from part 3",
    "text": "Previous code from part 3\n\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32 # batch, time, channels\nx = torch.randn(B, T, C)\n\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float(\"-inf\"))\nwei = F.softmax(wei, dim = -1)\n\nout = wei @ x\n\nprint(f\"tril:\\n{tril}\")\nprint(f\"wei:\\n{wei}\")\nprint(f\"out.shape:\\n{out.shape}\")\n\ntril:\ntensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])\nwei:\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\nout.shape:\ntorch.Size([4, 8, 32])"
  },
  {
    "objectID": "nano_gpt.html#building-the-self-attention",
    "href": "nano_gpt.html#building-the-self-attention",
    "title": "Nano GPT",
    "section": "Building the Self-Attention",
    "text": "Building the Self-Attention\nEach token (can be called node too) at each position emmits 2 vectors: 1. Query: What I’m looking for? 2. Key: What do I contain? 3. Value: What I will tell you or the information I have in this head?\n\nAffinities between tokens wei = my Query @ all Keys\nIf key and query are aligned ➡️ high value ➡️ learn more about this sequence\nInstead of multiplying wei with tokens directly, we multiply it with values (which is the information we want to learn about)\n\n\ntorch.manual_seed(1337)\nB, T, C = 4, 8, 32 # batch, time, channels\n\n# x is private information of each token\nx = torch.randn(B, T, C)\n\n# single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias = False)\nquery = nn.Linear(C, head_size, bias = False)\nvalue = nn.Linear(C, head_size, bias = False)\n\n\nk = key(x) # (B, T, head_size) = (4, 8, 16)\nq = query(x) # (B, T, head_size) = (4, 8, 16)\n\n# now every token in every batch is associated with a key and a query (in parallel), no communication between tokens has happened yet\n\nwei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\ntril = torch.tril(torch.ones(T, T))\n# wei are no longer zeros, but data dependent values (affinities)\n# wei = torch.zeros((T, T))\nwei = wei.masked_fill(tril == 0, float(\"-inf\"))\nwei = F.softmax(wei, dim = -1)\n\nprint(f\"wei[0]: {wei[0]}\")\n\n# multiply with value instead of x\nv = value(x) # (B, T, head_size) = (4, 8, 16)\nout = wei @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n# out = wei @ x\n\nwei[0]: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)"
  },
  {
    "objectID": "nano_gpt.html#notes-about-self-attention",
    "href": "nano_gpt.html#notes-about-self-attention",
    "title": "Nano GPT",
    "section": "Notes About Self Attention",
    "text": "Notes About Self Attention\n\nAttention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\nThere is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\nEach example across batch dimension is of course processed completely independently and never “talk” to each other\nIn an encoder attention block just delete the single line that does masking with tril, allowing all tokens to communicate, it can be used for some applications like translation and sentiment analysis. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\nself-attention just means that the keys and values are produced from the same source as queries. In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\nScaled Dot-Product Attention: \\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\)\nScaled attention additional divides wei by \\(\\frac{1}{\\sqrt{\\text{head\\_size}}}\\). This makes it so when input Q, K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. it’s important especially in initialization.\nif the variance is very high, softmax will converge to one-hot vector\n\n\n# Scaled Attention\nk = torch.randn(B, T, head_size)\nq = torch.randn(B, T, head_size)\n\nprint(\"Unscaled Attention\")\nwei = q @ k.transpose(-2, -1)\nprint(f\"var(k) = {torch.var(k)}\")\nprint(f\"var(q) = {torch.var(q)}\")\nprint(f\"var(wei) = {torch.var(wei)}\")\n\nprint(\"\\nScaled Attention\")\nwei = q @ k.transpose(-2, -1) * (head_size ** -0.5)\nprint(f\"var(k) = {torch.var(k)}\")\nprint(f\"var(q) = {torch.var(q)}\")\nprint(f\"var(wei) = {torch.var(wei)}\")\n\nUnscaled Attention\nvar(k) = 1.044861912727356\nvar(q) = 1.0700464248657227\nvar(wei) = 17.46897315979004\n\nScaled Attention\nvar(k) = 1.044861912727356\nvar(q) = 1.0700464248657227\nvar(wei) = 1.0918108224868774"
  },
  {
    "objectID": "nano_gpt.html#adding-single-self-attention-head-to-the-bigram-language-model",
    "href": "nano_gpt.html#adding-single-self-attention-head-to-the-bigram-language-model",
    "title": "Nano GPT",
    "section": "Adding single Self Attention Head to the Bigram Language Model",
    "text": "Adding single Self Attention Head to the Bigram Language Model\n\nMaking new Head class\n\n# Making the Head Class\nclass Head(nn.Module):\n    \"\"\" one head of self attention \"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embed, head_size, bias = False)\n        self.query = nn.Linear(n_embed, head_size, bias = False)\n        self.value = nn.Linear(n_embed, head_size, bias = False)\n        # since tril isn't a parameter, we register it as a buffer\n        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n    \n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x) # (B, T, C)\n        q = self.query(x) # (B, T, C)\n\n        # compute attention scores (affinities)\n        wei = q @ k.transpose(-2, -1) * (C ** -0.5) # (B, T, C) @ (B, C, T) = (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0 , float(\"-inf\")) # (B, T, T)\n        wei = F.softmax(wei, dim = -1) # (B, T, T)\n\n        # perform weighted aggregation of the values\n        v = self.value(x) # (B, T, C)\n        out = wei @ v # (B, T, T) @ (B, T, C) = (B, T, C)\n        return out\n\n\n\nModifying BigramLanguageModel class\n\nAdding Head to the BigramLanguageModel class\nAdding Head to the BigramLanguageModel forward pass\nFor generate function, we need crop idx to keep idx.shape &lt;= block_size, since we are using positional embedding\n\n\n# Adding Head to the BigramLanguageModel\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    # no need to pass vocab_size as an argument, since it is a global variable in this file\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a loockup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        # each position is also associated with an embedding vector\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        # a single head of self attention\n        self.sa_head = Head(n_embed)\n        # the output layer is a linear layer with vocab_size outputs\n        self.lm_head = nn.Linear(n_embed, vocab_size)\n\n    def forward(self, idx, targets = None):\n        B, T = idx.shape\n        # idx and targets are both (B, T) tensor of ints\n        token_emb = self.token_embedding_table(idx) # (B, T, C) = (4, 8 , vocab_size)\n        pos_emb = self.position_embedding_table(torch.arange(T, device = idx.device)) # (T, C) = (8, vocab_size)\n        # x has the token identities + the position embeddings\n        x = token_emb + pos_emb # (B, T, C) = (4, 8, vocab_size)\n        # feed the input to the self attention head\n        x = self.sa_head(x) # (B, T, C) = (4, 8, vocab_size)\n        logits = self.lm_head(x) # (B, T, vocab_size) = (4, 8, vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            # note that F.cross_entropy accepts inputs in shape (B, C, T)\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T) # can be as targets = targets.view(-1)\n            \n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:] # (B, T)\n            # get the logits for the next token\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            # (note that we are feeding the whole context each time, however we only care about the last prediction)\n            # (this make doesn't make sense now, but the function will be modified later)\n            logits = logits[:, -1, :] # Becomes (B, C) (get the last time step for each sequence)\n            # apply softmax to convert to probabilities\n            probs = F.softmax(logits, dim = -1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled token to the context\n            idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n        return idx\n\n\n\nTesting\n(loss is 2.54 instead of 2.57)\n\n!python3 bigram.py --eval_interval 2999\n\n0.209729 M parameters\n step 0 | train loss: 4.4116 | val loss: 4.4048\n step 2999 | train loss: 1.7879 | val loss: 1.9086\n\nAnd they bride.\n\nNUTHORD IV:\nKind Petice you each graves the can\nhe art third wear he. Warwithry ane away, my feans\nacut onour\nYourself fittice of my helige, at mirters,\nI in latiHer drove to does me none\nAnd justs like die; like us, courmby:\nAughtainst, why. Here, she royal elsed whom\nIs would that\nAnd insun her evices to thee, and The chiress poor\nof his burder hands thy fis are\nIn the flownd male of would Prive my of.\n\nHENRY BOLON:\nPrisabardand the Eart to uncles.\n\nDUCHASS III:\nMy hand for hi"
  },
  {
    "objectID": "nano_gpt.html#multi-head-attention",
    "href": "nano_gpt.html#multi-head-attention",
    "title": "Nano GPT",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\n\nMake the new MultiHeadAttention class\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_head, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n    \n    def forward(self, x):\n        # concatenate them into the channel dimension\n        return torch.cat([h(x) for h in self.heads], dim = -1)\n\nthen add it to BigramLanguageModel class\npreviously:\nself.sa_head = Head(n_embed)\nnow:\nself.sa_heads = MultiHeadAttention(num_head = 4, head_size = n_embed // 4)\n\nTesting\n(loss is 2.51 instead of 2.54)\n\n!python3 bigram.py --eval_interval 2999\n\n0.209729 M parameters\n step 0 | train loss: 4.4116 | val loss: 4.4048\n step 2999 | train loss: 1.7879 | val loss: 1.9086\n\nAnd they bride.\n\nNUTHORD IV:\nKind Petice you each graves the can\nhe art third wear he. Warwithry ane away, my feans\nacut onour\nYourself fittice of my helige, at mirters,\nI in latiHer drove to does me none\nAnd justs like die; like us, courmby:\nAughtainst, why. Here, she royal elsed whom\nIs would that\nAnd insun her evices to thee, and The chiress poor\nof his burder hands thy fis are\nIn the flownd male of would Prive my of.\n\nHENRY BOLON:\nPrisabardand the Eart to uncles.\n\nDUCHASS III:\nMy hand for hi"
  },
  {
    "objectID": "nano_gpt.html#test",
    "href": "nano_gpt.html#test",
    "title": "Nano GPT",
    "section": "Test",
    "text": "Test\n(loss is 2.46 instead of 2.51)\n\n!python3 bigram.py --eval_interval 2999\n\n0.209729 M parameters\n step 0 | train loss: 4.4116 | val loss: 4.4048\n step 2999 | train loss: 1.7879 | val loss: 1.9086\n\nAnd they bride.\n\nNUTHORD IV:\nKind Petice you each graves the can\nhe art third wear he. Warwithry ane away, my feans\nacut onour\nYourself fittice of my helige, at mirters,\nI in latiHer drove to does me none\nAnd justs like die; like us, courmby:\nAughtainst, why. Here, she royal elsed whom\nIs would that\nAnd insun her evices to thee, and The chiress poor\nof his burder hands thy fis are\nIn the flownd male of would Prive my of.\n\nHENRY BOLON:\nPrisabardand the Eart to uncles.\n\nDUCHASS III:\nMy hand for hi"
  },
  {
    "objectID": "nano_gpt.html#stacking-the-blocks",
    "href": "nano_gpt.html#stacking-the-blocks",
    "title": "Nano GPT",
    "section": "Stacking the Blocks",
    "text": "Stacking the Blocks\n\nclass Block(nn.Module):\n    \"\"\" Transformer Block: Communication followed by Computation \"\"\"\n\n    def __init__(self, n_embed, n_head):\n        \"\"\" n_embed: embedding dimension\n            n_head: number of heads in the multi-head attention\n        \"\"\"\n        super().__init__()\n        head_size = n_embed // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embed)\n    \n    def forward(self, x):\n        x = self.sa(x)\n        x = self.ffwd(x)\n        return x\n\n# previous code ..\n# each token directly reads off the logits for the next token from a loockup table\nself.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n# each position is also associated with an embedding vector\nself.position_embedding_table = nn.Embedding(block_size, n_embed)\n# transformer blocks\nself.blocks = nn.Sequential(\n        Block(n_embed, n_head = 4),\n        Block(n_embed, n_head = 4),\n        Block(n_embed, n_head = 4),\n)\nself.lm_head = nn.Linear(n_embed, vocab_size)\n# rest of the code ..\nAdd them in the forward pass\n# previous code ..\nx = token_emb + pos_emb # (B, T, C) = (4, 8, vocab_size)\n# feed the input to the self attention head\nx = self.blocks(x) # (B, T, C) = (4, 8, vocab_size)\nlogits = self.lm_head(x) # (B, T, vocab_size) = (4, 8, vocab_size)\n# rest of the code ..\n\nTest\n(loss is 2.81 instead of 2.46) ➡️ WORSE\n\n!python3 bigram.py --eval_interval 2999\n\n0.209729 M parameters\n step 0 | train loss: 4.4116 | val loss: 4.4048\n step 2999 | train loss: 1.7879 | val loss: 1.9086\n\nAnd they bride.\n\nNUTHORD IV:\nKind Petice you each graves the can\nhe art third wear he. Warwithry ane away, my feans\nacut onour\nYourself fittice of my helige, at mirters,\nI in latiHer drove to does me none\nAnd justs like die; like us, courmby:\nAughtainst, why. Here, she royal elsed whom\nIs would that\nAnd insun her evices to thee, and The chiress poor\nof his burder hands thy fis are\nIn the flownd male of would Prive my of.\n\nHENRY BOLON:\nPrisabardand the Eart to uncles.\n\nDUCHASS III:\nMy hand for hi"
  },
  {
    "objectID": "nano_gpt.html#adding-residual-connections-skip-connections",
    "href": "nano_gpt.html#adding-residual-connections-skip-connections",
    "title": "Nano GPT",
    "section": "Adding Residual Connections (Skip Connections)",
    "text": "Adding Residual Connections (Skip Connections)\n\nTransformer Block\n\nclass Block(nn.Module):\n    \"\"\" Transformer Block: Communication followed by Computation \"\"\"\n\n    def __init__(self, n_embed, n_head):\n        \"\"\" n_embed: embedding dimension\n            n_head: number of heads in the multi-head attention\n        \"\"\"\n        super().__init__()\n        head_size = n_embed // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embed)\n    \n    def forward(self, x):\n        # residual connection (add the input to the output)\n        x = x + self.sa(x)\n        x = x + self.ffwd(x)\n        return x\n\n\n\nMulti-Head Attention\n\n# Multi Head Attention Class\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_head, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n        # linear transformation to the output of the multi-head attention as projection back to the residual pathway\n        self.proj = nn.Linear(n_embed, n_embed)\n    \n    def forward(self, x):\n        # out is the outptu of the multi-head attention\n        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n        # apply a linear layer to the concatenated output\n        out = self.proj(out)\n        return out\n\n\n\nFeedForward Layer\n\n# Feed Forward Class\nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    def __init__(self, n_embed):\n        super().__init__()\n        self.net = nn.Sequential(\n            # multiply by 4 to follow the original implementation\n            nn.Linear(n_embed, 4 * n_embed),\n            nn.ReLU(),\n            nn.Linear(n_embed * 4, n_embed),\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\n\n\nTest\n(loss is 2.33 instead of 2.81 and 2.46 before it)\n\n!python3 bigram.py --eval_interval 2999\n\n0.209729 M parameters\n step 0 | train loss: 4.4116 | val loss: 4.4048\n step 2999 | train loss: 1.7879 | val loss: 1.9086\n\nAnd they bride.\n\nNUTHORD IV:\nKind Petice you each graves the can\nhe art third wear he. Warwithry ane away, my feans\nacut onour\nYourself fittice of my helige, at mirters,\nI in latiHer drove to does me none\nAnd justs like die; like us, courmby:\nAughtainst, why. Here, she royal elsed whom\nIs would that\nAnd insun her evices to thee, and The chiress poor\nof his burder hands thy fis are\nIn the flownd male of would Prive my of.\n\nHENRY BOLON:\nPrisabardand the Eart to uncles.\n\nDUCHASS III:\nMy hand for hi"
  },
  {
    "objectID": "nano_gpt.html#batchnorm1d-from-makemore-part-3",
    "href": "nano_gpt.html#batchnorm1d-from-makemore-part-3",
    "title": "Nano GPT",
    "section": "BatchNorm1d from makemore part 3",
    "text": "BatchNorm1d from makemore part 3\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n        # buffers (trained while running `momentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        if self.training:\n            # batch mean\n            xmean = x.mean(0, keepdim= True)\n            # batch variance\n            xvar = x.var(0, keepdim= True)\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n\n        # update the buffers in training\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        \n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)\nx = module(x)\nx.shape\n\n# columns are normalized\nprint(f\"mean of first column: {x[:, 0].mean():.4f} | std of first column: {x[:, 0].std():.4f}\")\n# rows are not normalized ➡️ we need to normalize the rows instead\nprint(f\"mean of first row: {x[0, :].mean():.4f} | std of first row: {x[0, :].std():.4f}\")\n\nmean of first column: 0.0000 | std of first column: 1.0000\nmean of first row: 0.0411 | std of first row: 1.0431\n\n\n\n# after normalizing the rows (and removing the buffers too)\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n        self.eps = eps\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        xmean = x.mean(1, keepdim= True)\n        xvar = x.var(1, keepdim= True)\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = BatchNorm1d(100)\nx = torch.randn(32, 100)\nx = module(x)\nx.shape\n\n# columns are not normalized now\nprint(f\"mean of first column: {x[:, 0].mean():.4f} | std of first column: {x[:, 0].std():.4f}\")\n# rows are normalized now\nprint(f\"mean of first row: {x[0, :].mean():.4f} | std of first row: {x[0, :].std():.4f}\")\n\nmean of first column: 0.1469 | std of first column: 0.8803\nmean of first row: -0.0000 | std of first row: 1.0000"
  },
  {
    "objectID": "nano_gpt.html#adding-layernorm",
    "href": "nano_gpt.html#adding-layernorm",
    "title": "Nano GPT",
    "section": "Adding LayerNorm",
    "text": "Adding LayerNorm\n\nIn the Transformer Blocks\n\nclass Block(nn.Module):\n    \"\"\" Transformer Block: Communication followed by Computation \"\"\"\n\n    def __init__(self, n_embed, n_head):\n        \"\"\" n_embed: embedding dimension\n            n_head: number of heads in the multi-head attention\n        \"\"\"\n        super().__init__()\n        head_size = n_embed // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embed)\n        # ln1 is applied directly on input before the multi-head attention\n        self.ln1 = nn.LayerNorm(n_embed)\n        # ln2 is applied directly on the output of the multi-head attention before the feed-forward layer\n        self.ln2 = nn.LayerNorm(n_embed)\n    \n    def forward(self, x):\n        # residual connection (add the input to the output)\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n\n\nAfter all blocks (before last linear layer)\n\nclass BigramLanguageModel(nn.Module):\n    # no need to pass vocab_size as an argument, since it is a global variable in this file\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a loockup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        # each position is also associated with an embedding vector\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        # transformer blocks\n        self.blocks = nn.Sequential(\n                Block(n_embed, n_head = 4),\n                Block(n_embed, n_head = 4),\n                Block(n_embed, n_head = 4),\n                # add layernorm here\n                nn.LayerNorm(n_embed),\n        )\n        self.lm_head = nn.Linear(n_embed, vocab_size)"
  },
  {
    "objectID": "nano_gpt.html#adding-n_layer-variable",
    "href": "nano_gpt.html#adding-n_layer-variable",
    "title": "Nano GPT",
    "section": "Adding n_layer variable",
    "text": "Adding n_layer variable\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n    # no need to pass vocab_size as an argument, since it is a global variable in this file\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a loockup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n        # each position is also associated with an embedding vector\n        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n        # transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_embed, n_head = 4) for _ in range(n_layer)])\n        # Remember to add it in forward too\n        self.ln_f = nn.LayerNorm(n_embed)\n        self.lm_head = nn.Linear(n_embed, vocab_size)"
  },
  {
    "objectID": "nano_gpt.html#adding-dropouts",
    "href": "nano_gpt.html#adding-dropouts",
    "title": "Nano GPT",
    "section": "Adding Dropouts",
    "text": "Adding Dropouts\n\nIn Head after calculating wei\nIn MultiHeadAttention after self.proj\nIn FeedForward after last linear"
  },
  {
    "objectID": "nano_gpt.html#try",
    "href": "nano_gpt.html#try",
    "title": "Nano GPT",
    "section": "Try",
    "text": "Try\n\n!python3 bigram.py\n\n0.209729 M parameters\n step 0 | train loss: 4.4116 | val loss: 4.4048\n step 100 | train loss: 2.6539 | val loss: 2.6605\n step 200 | train loss: 2.5140 | val loss: 2.5161\n step 300 | train loss: 2.4039 | val loss: 2.4126\n step 400 | train loss: 2.3430 | val loss: 2.3506\n step 500 | train loss: 2.2918 | val loss: 2.3002\n step 600 | train loss: 2.2347 | val loss: 2.2517\n step 700 | train loss: 2.1930 | val loss: 2.2089\n step 800 | train loss: 2.1556 | val loss: 2.1793\n step 900 | train loss: 2.1211 | val loss: 2.1507\n step 1000 | train loss: 2.0768 | val loss: 2.1179\n step 1100 | train loss: 2.0615 | val loss: 2.1029\n step 1200 | train loss: 2.0300 | val loss: 2.0744\n step 1300 | train loss: 2.0145 | val loss: 2.0577\n step 1400 | train loss: 1.9936 | val loss: 2.0542\n step 1500 | train loss: 1.9759 | val loss: 2.0375\n step 1600 | train loss: 1.9503 | val loss: 2.0281\n step 1700 | train loss: 1.9273 | val loss: 2.0172\n step 1800 | train loss: 1.9151 | val loss: 2.0030\n step 1900 | train loss: 1.9089 | val loss: 2.0017\n step 2000 | train loss: 1.8805 | val loss: 1.9853\n step 2100 | train loss: 1.8700 | val loss: 1.9754\n step 2200 | train loss: 1.8716 | val loss: 1.9654\n step 2300 | train loss: 1.8520 | val loss: 1.9508\n step 2400 | train loss: 1.8433 | val loss: 1.9484\n step 2500 | train loss: 1.8300 | val loss: 1.9462\n step 2600 | train loss: 1.8120 | val loss: 1.9332\n step 2700 | train loss: 1.8117 | val loss: 1.9292\n step 2800 | train loss: 1.8059 | val loss: 1.9191\n step 2900 | train loss: 1.7842 | val loss: 1.8985\n step 3000 | train loss: 1.7776 | val loss: 1.8973\n step 3100 | train loss: 1.7803 | val loss: 1.9138\n step 3200 | train loss: 1.7770 | val loss: 1.9130\n step 3300 | train loss: 1.7566 | val loss: 1.8956\n step 3400 | train loss: 1.7586 | val loss: 1.8909\n step 3500 | train loss: 1.7417 | val loss: 1.8878\n step 3600 | train loss: 1.7340 | val loss: 1.8668\n step 3700 | train loss: 1.7352 | val loss: 1.8874\n step 3800 | train loss: 1.7179 | val loss: 1.8655\n step 3900 | train loss: 1.7237 | val loss: 1.8702\n step 4000 | train loss: 1.7193 | val loss: 1.8635\n step 4100 | train loss: 1.7079 | val loss: 1.8657\n step 4200 | train loss: 1.7085 | val loss: 1.8462\n step 4300 | train loss: 1.7024 | val loss: 1.8440\n step 4400 | train loss: 1.6880 | val loss: 1.8442\n step 4500 | train loss: 1.6986 | val loss: 1.8520\n step 4600 | train loss: 1.6957 | val loss: 1.8502\n step 4700 | train loss: 1.6892 | val loss: 1.8477\n step 4800 | train loss: 1.6831 | val loss: 1.8370\n step 4900 | train loss: 1.6708 | val loss: 1.8188\n\n\nClown:\nRorince, and is so blorbacest bobe to take On.\n\nMARCILIA:\nI'll bapitius have bury, delay ane away, what,\nDound my know\nYourself friance!\nNor linding, and if eye, hone; and maid overs, and the news\nAnd about liking tear.\nHis mild him speak; and allw youngs\nPause and at lives home.\nWho like again Wich to draugions to them,\nWill we hide honour more-forrument of thrupted so;\nAnging must with all of which Priently of.\n\nHENRY VI\n\nJOHN Y:\n\nSaday Warwick forth in couragain.\n\nCRINIUS:\n\nAnd forwic"
  },
  {
    "objectID": "wavenet.html",
    "href": "wavenet.html",
    "title": "Wavenet",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport math"
  },
  {
    "objectID": "wavenet.html#wavenet-implementation-based-on-andrej-karpathys-lecture",
    "href": "wavenet.html#wavenet-implementation-based-on-andrej-karpathys-lecture",
    "title": "Wavenet",
    "section": "WaveNet Implementation: Based on Andrej Karpathy’s Lecture",
    "text": "WaveNet Implementation: Based on Andrej Karpathy’s Lecture\nThis notebook is a practical follow-up to Andrej Karpathy’s “Building makemore Part 5: Building a WaveNet” lecture. Check out the full lecture here.\nHere’s what we’ll cover:\n\nModel Basics: Introduction to a multi-layer perceptron character-level language model.\nModel Enhancement: Expanding the architecture and input characters for better results.\nWaveNet Overview: Understand WaveNet’s hierarchical structure and its predictions.\nBatch Normalization: Dive into the BatchNorm layer and its challenges.\nPyTorch Containers: A look at how PyTorch structures its layers.\nDataset Expansion: Increase the context length for performance improvement.\nForward Pass: Visualization of tensor transformations in the network.\nBatchNorm1D Bug: Addressing an implementation bug.\nDevelopment Insights: Best practices in deep neural network development.\nOptimizing WaveNet: Suggestions and strategies for better performance.\n\nThis notebook aims to provide a clear understanding of WaveNet’s development and optimization process."
  },
  {
    "objectID": "wavenet.html#wavenet",
    "href": "wavenet.html#wavenet",
    "title": "Wavenet",
    "section": "WaveNet",
    "text": "WaveNet\n\nWaveNet Overview:\n\nNature of the Model: WaveNet is a fully probabilistic and autoregressive model. This means that when predicting any given audio sample, it considers all the previous samples.\nEfficiency: It can be trained efficiently on very high-resolution audio data (e.g., data with tens of thousands of samples per second).\nPerformance: For text-to-speech tasks, human listeners rated the outputs of WaveNet as more natural sounding than other leading methods. Additionally, it can switch between different speakers by conditioning on the speaker’s identity. WaveNet can also generate musical fragments that sound realistic.\n\n\n\nTechnical Insights:\n\nGenerative Model for Audio: WaveNet operates directly on raw audio, predicting the probability of each audio sample based on the previous ones. The model’s structure is inspired by PixelCNN, which was designed for images.\nDilated Causal Convolutions: To ensure that predictions for any timestep don’t depend on future timesteps, the model uses causal convolutions. “Dilated” convolutions are introduced to effectively increase the receptive field (the portion of the input data the model “sees”) without significantly increasing computational cost.\nSoftmax Distributions: Instead of using a mixture model, the paper employs a softmax distribution for modeling audio samples. To manage the high-resolution of raw audio, a µ-law companding transformation is applied to the data before quantizing it.\nGated Activation Units: The paper uses a specific type of activation function for the neural network, which was found to work particularly well for audio signals.\nResidual and Skip Connections: These are techniques to help train deeper neural networks more effectively. They help in faster convergence and enable deeper model architectures.\nConditional WaveNets: WaveNet can be conditioned on additional inputs, which allows it to generate audio with specific characteristics. For example, by conditioning on a speaker’s identity, WaveNet can produce audio in that speaker’s voice. The paper distinguishes between global conditioning (affecting the whole audio) and local conditioning (affecting specific parts of the audio).\nContext Stacks: To increase the receptive field size, the paper introduces the concept of context stacks. These are separate smaller networks that process longer parts of the audio signal and condition the primary WaveNet model.\n\n\n\nApplications:\n\nText-to-Speech (TTS): WaveNet can produce very natural-sounding speech, surpassing other state-of-the-art systems.\nVoice Modulation: A single WaveNet model can mimic many different speakers.\nMusic Generation: WaveNet can generate realistic musical fragments.\nOther Audio Tasks: The model is also promising for tasks like speech enhancement, voice conversion, and source separation.\n\nIn essence, WaveNet is a breakthrough in audio generation, offering a versatile and powerful model for a range of audio-related tasks.\n\nwords = open(\"names.txt\", \"r\").read().splitlines()\nprint(words[:8])\nprint(len(words))\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n32033\n\n\n\nchars = sorted(list(set(\"\".join(words))))\nstoi = {s: i + 1 for i, s in enumerate(chars)}\nstoi[\".\"] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(stoi)\n\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\nblock_size = 3\ndef build_dataset(words):  \n\n  X, Y = [], []\n  \n  for w in words:\n\n    context = [0] * block_size\n    for ch in w + '.':\n\n      ix = stoi[ch]\n\n      X.append(context)\n      Y.append(ix)\n\n      context = context[1:] + [ix]\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\nfor x, y in zip(Xtr[:15], Ytr[:15]):\n  print(\"\".join(itos[ix.item()] for ix in x), \"----&gt;\", itos[y.item()])\n\n... ----&gt; y\n..y ----&gt; u\n.yu ----&gt; h\nyuh ----&gt; e\nuhe ----&gt; n\nhen ----&gt; g\neng ----&gt; .\n... ----&gt; d\n..d ----&gt; i\n.di ----&gt; o\ndio ----&gt; n\nion ----&gt; d\nond ----&gt; r\nndr ----&gt; e\ndre ----&gt; ."
  },
  {
    "objectID": "wavenet.html#high-level-hierarchical-view-of-pytorch-api",
    "href": "wavenet.html#high-level-hierarchical-view-of-pytorch-api",
    "title": "Wavenet",
    "section": "High-level hierarchical view of PyTorch API",
    "text": "High-level hierarchical view of PyTorch API\nThe PyTorch API is extensive, but I’ll provide a high-level hierarchical view of its core components, which should give you a roadmap for diving deeper:\n\nTensors\n\nCore data structure in PyTorch, similar to NumPy arrays but with GPU support.\ntorch.Tensor class and its various methods.\nCreation: torch.empty(), torch.rand(), torch.zeros(), torch.ones(), torch.tensor(), etc.\nOperations: Mathematical, Reduction, Comparison, Matrix, etc.\nIndexing, Slicing, Joining, Mutating ops: torch.cat(), torch.stack(), etc.\n\nAutograd\n\nAutomatic differentiation library.\ntorch.autograd module.\nVariable: Deprecated, but historically important. All Tensors now have requires_grad attribute.\nFunction: Defines a forward and backward operation. Links to Variable to build a computation graph.\n\nNeural Networks\n\ntorch.nn module.\nLayers: Pre-defined layers like nn.Linear, nn.Conv2d, nn.ReLU, etc.\nLoss functions: nn.CrossEntropyLoss, nn.MSELoss, etc.\nOptimizers: Located in torch.optim, e.g., optim.Adam, optim.SGD.\nUtilities: nn.functional for stateless functions like activation functions.\nnn.Module: Base class for all neural network modules, aiding in organizing code and parameters.\nnn.Sequential: A sequential container for stacking layers.\n\nUtilities\n\nTensor transformations: torchvision.transforms.\nData handling for NN training: torch.utils.data.Dataset, torch.utils.data.DataLoader.\n\nOptimization\n\ntorch.optim module.\nOptimization algorithms like SGD, Adam, RMSProp, etc.\nLearning rate schedulers: Adjust LR on-the-fly during training.\n\nSerialization\n\nSave and load models: torch.save(), torch.load(), nn.Module.load_state_dict(), etc.\n\nDistributed Training\n\ntorch.distributed: For multi-GPU and distributed training.\nBackend support for different communication protocols.\n\nOther Libraries & Extensions\n\ntorchvision: Datasets, models, and image transformations for computer vision.\ntorchaudio: Audio processing tools and datasets.\ntorchtext: NLP data utilities and models.\n\nDevice & CUDA\n\nTensor operations on different devices: CPU, GPU.\nCUDA Tensors: Tensors transferred to GPU.\nDevice management: torch.cuda, torch.device.\n\nJIT Compiler\n\ntorch.jit: Just-In-Time compiler to convert PyTorch models to a representation that can be optimized and run in non-Python environments.\n\nQuantization\n\nReduce the size of models and increase runtime performance.\ntorch.quantization: Contains utilities for model quantization.\n\n\nStart with Tensors and Autograd to get a solid grasp on the basics. Then, you can delve into neural networks with the torch.nn module. After mastering these, choose specialized topics based on your interests and needs.\n\n# Layers made in part 3\nclass Linear:\n    def __init__(self, fan_in, fan_out, bias = True):\n        self.weight = torch.randn((fan_in, fan_out))\n        self.weight /= fan_in ** 0.5\n        self.bias = torch.zeros((fan_out)) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n\n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n        # buffers (trained while running `momentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        if self.training:\n            # batch mean\n            xmean = x.mean(0, keepdim= True)\n            # batch variance\n            xvar = x.var(0, keepdim= True)\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n\n        # update the buffers in training\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        \n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n    \n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []"
  },
  {
    "objectID": "wavenet.html#random-seeding-in-the-context-of-pytorch-and-neural-network-training.",
    "href": "wavenet.html#random-seeding-in-the-context-of-pytorch-and-neural-network-training.",
    "title": "Wavenet",
    "section": "Random seeding in the context of PyTorch and neural network training.",
    "text": "Random seeding in the context of PyTorch and neural network training.\n\n1. Purpose of Seeding:\nIn machine learning, especially in neural networks, we often initialize weights and biases randomly. Moreover, when you’re dealing with stochastic processes like dropout, sampling, and other random transformations, the behavior can differ from one run to another due to the randomness. By setting a seed for these random operations, we ensure that the randomness is consistent across multiple runs, making experiments reproducible.\n\n\n2. torch.manual_seed() vs. torch.Generator():\n\ntorch.manual_seed(seed): This sets the seed for the default global generator in PyTorch. Every time you call a function that involves randomness without specifying a generator, it uses the global generator. When you set a manual seed, you’re setting the seed for this global generator. It’s a straightforward way to ensure consistent randomness throughout your program.\ntorch.Generator(): This creates an independent random number generator. You can manually set the seed for this generator and use it for specific operations, keeping it separate from the global generator. This is particularly useful when you want different parts of your code to have different random behaviors, but still want each of those behaviors to be reproducible.\n\n\n\n3. Why not always use torch.manual_seed()?:\nIn many cases, using torch.manual_seed() is sufficient, especially for simpler projects and experiments. However, as your projects grow in complexity, there might be reasons to maintain different seeds:\n\nFine-grained Control: You might want different parts of your code to operate with different seeds. For example, if you’re doing multi-task learning with multiple neural networks, you might want to initialize each network with a different seed, but still want each initialization to be reproducible.\nParallelism: When running operations in parallel, having separate generators can prevent potential synchronization issues and ensure that each parallel operation is consistent across runs.\nIsolation: By using different generators for different parts of your code, you can change one part of your code without affecting the randomness in another part.\n\n\n\nConclusion:\nWhile torch.manual_seed() is a quick and effective method for most use cases, as your projects become more complex, you might find situations where the granularity and control offered by torch.Generator() become necessary. Knowing when and how to use each method appropriately can make your experiments more organized and your results more reliable.\n\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x7f228c1dc9b0&gt;\n\n\n\nn_embd = 10\nn_hidden = 200\n\nC = torch.randn((vocab_size, n_embd))\nlayers = [\n    Linear(n_embd * block_size, n_hidden, bias= False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n    Linear(n_hidden, vocab_size)\n]\n\nwith torch.no_grad():\n    layers[-1].weight *= 0.1\n\nparameters = [C] + [p for l in layers for p in l.parameters()]\nprint(f\"num parameters: {sum(p.numel() for p in parameters)}\")\n\nfor p in parameters:\n    p.requires_grad_()\n\nnum parameters: 12097\n\n\n## torch.randint\n\n\n1. torch.randint:\ntorch.randint is a PyTorch function that returns a tensor filled with random integers generated uniformly between two specified integer values (low and high).\nThe function signature is:\ntorch.randint(low=0, high, size, *, dtype=None, layout=torch.strided, device=None, requires_grad=False)\n\nlow (int, optional): Lowest integer to be drawn from the distribution. Default: 0.\nhigh (int): One above the highest integer to be drawn from the distribution.\nsize (tuple): The shape of the output tensor.\nAdditional arguments like dtype, device, and requires_grad allow you to further specify the nature of the returned tensor.\n\n\n\n2. Given Line:\nThis would produce a 1D tensor with 4 random integer values in the specified range.\nThis line aims to generate a tensor of random integer values between 0 (inclusive) and Xtr.shape[0] (exclusive).\nix = torch.randint(0, Xtr.shape[0], (4,))\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n\n    # forward pass\n    emb = C[Xb] # embed characters into vector space\n    x = emb.view((emb.shape[0], -1)) # flatten\n    for layer in layers:\n        x = layer(x)\n    # compute loss\n    loss = F.cross_entropy(x, Yb)\n\n    # backward pass\n    for layer in layers:\n        layer.out.retain_grad()\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 10000 else 0.01\n    for p in parameters:\n        p.data -= lr * p.grad\n    \n    # track stats\n    if i % 10000 == 0:\n        print(f\"step {i} loss {loss.item()}\")\n\n    lossi.append(loss.item())\n\nstep 0 loss 3.2844254970550537\nstep 10000 loss 2.317671060562134\nstep 20000 loss 2.330378293991089\nstep 30000 loss 1.8735352754592896\nstep 40000 loss 2.1151928901672363\nstep 50000 loss 1.5009478330612183\nstep 60000 loss 1.5936698913574219\nstep 70000 loss 2.6373109817504883\nstep 80000 loss 2.13984751701355\nstep 90000 loss 2.172301769256592\nstep 100000 loss 2.2835309505462646\nstep 110000 loss 2.4028546810150146\nstep 120000 loss 2.017624855041504\nstep 130000 loss 1.9769095182418823\nstep 140000 loss 2.0796420574188232\nstep 150000 loss 1.9310541152954102\nstep 160000 loss 2.306513547897339\nstep 170000 loss 1.9171533584594727\nstep 180000 loss 1.7749229669570923\nstep 190000 loss 1.8716074228286743"
  },
  {
    "objectID": "wavenet.html#fixing-the-learning-rate-plot",
    "href": "wavenet.html#fixing-the-learning-rate-plot",
    "title": "Wavenet",
    "section": "Fixing the Learning Rate Plot",
    "text": "Fixing the Learning Rate Plot\n\nplt.plot(torch.tensor(lossi).view(-1, 1000).mean(1, keepdim= True).data);"
  },
  {
    "objectID": "wavenet.html#classes-definitions",
    "href": "wavenet.html#classes-definitions",
    "title": "Wavenet",
    "section": "Classes Definitions",
    "text": "Classes Definitions"
  },
  {
    "objectID": "wavenet.html#torch.nn.embedding.",
    "href": "wavenet.html#torch.nn.embedding.",
    "title": "Wavenet",
    "section": "torch.nn.Embedding.",
    "text": "torch.nn.Embedding.\n\n1. The Concept of Embeddings:\nEmbeddings are a powerful tool in the world of deep learning, especially when dealing with categorical data, like words in a language. Instead of representing words or other categorical variables as discrete values (like integers), embeddings represent them as continuous vectors. These vectors capture more information and relationships between different words or categories.\n\n\n2. torch.nn.Embedding:\ntorch.nn.Embedding is PyTorch’s module to create an embedding layer. Essentially, it’s a lookup table that maps from integer indices (representing specific words or categories) to dense vectors (their embeddings).\n\n\n3. Parameters:\n\nnum_embeddings: Total number of distinct categories/words.\nembedding_dim: The size of each embedding vector, i.e., the number of units each embedding should have.\n\n\n\n4. Why Use Embeddings?:\n\nDimensionality Reduction: One-hot encoded vectors can be massive (imagine a vector of length 50,000 for a moderate-sized vocabulary, with all zeros except for a single one). Embeddings condense this information into a much smaller dimension, like 300 for word embeddings.\nCapture Relationships: Embeddings are learned from data. This means that words or categories that have similar meanings or behaviors can have embeddings that are close to each other in the vector space.\nFlexibility: Embeddings can be fine-tuned during training. This means that as a model learns a task, it can also adjust the embeddings to capture any task-specific insights.\n\n\n\n5. Usage:\nAn embedding layer is typically initialized with random weights and will learn an embedding for all the words in the training dataset. It is a flexible layer that can be used in a variety of ways, such as:\n\nPre-trained Embeddings: Sometimes, embeddings are pre-trained on a larger dataset and then fine-tuned on a specific task. Word2Vec, GloVe, and FastText are popular pre-trained word embeddings.\nTask-specific Embeddings: For some tasks, it might be beneficial to let the embedding layer learn embeddings from scratch, tailored to the specific task.\n\n\n\n6. Under the Hood:\nAt its core, an embedding layer is a weight matrix. The rows of this matrix correspond to each category’s unique ID (like a word’s ID), and the columns correspond to the embedding dimensions. When you “pass” an integer to this layer, it returns the corresponding row of the weight matrix. This operation is essentially a lookup, making it efficient.\n\n\nConclusion:\ntorch.nn.Embedding provides an efficient and straightforward way to handle categorical data in neural networks. By converting discrete categorical values into continuous vectors, embeddings enable models to capture intricate relationships in the data and improve performance on a variety of tasks."
  },
  {
    "objectID": "wavenet.html#torch.nn.flatten.",
    "href": "wavenet.html#torch.nn.flatten.",
    "title": "Wavenet",
    "section": "torch.nn.Flatten.",
    "text": "torch.nn.Flatten.\n\n1. The Basic Idea:\nWhen working with neural networks, especially convolutional neural networks (CNNs), we often deal with multi-dimensional data (like images). After passing this data through several convolutional and pooling layers, we often want to use the resulting multi-dimensional feature maps in fully connected layers (dense layers). However, fully connected layers expect a 1D input. Here’s where torch.nn.Flatten comes in: it’s used to transform multi-dimensional data into a one-dimensional format.\n\n\n2. torch.nn.Flatten:\ntorch.nn.Flatten is a layer provided by PyTorch that reshapes its input into a one-dimensional tensor. It’s effectively a ‘flattening’ operation.\n\n\n3. Parameters:\n\nstart_dim: Dimension to start the flattening. Typically, for a batch of images, the data shape might be [batch_size, channels, height, width]. If we want to flatten the channel, height, and width dimensions, we’d start the flattening from dimension 1 (0-based indexing for dimensions). By default, start_dim is 1.\nend_dim: Dimension to end the flattening. By default, it’s -1, meaning it will flatten all dimensions from start_dim to the last dimension.\n\n\n\n4. Why Use Flatten?:\n\nTransitioning in Architectures: It’s common in CNNs to have convolutional layers followed by dense layers. The flatten layer acts as a bridge between these two, reshaping the output of the convolutional layers to a format that dense layers can work with.\nSimplicity: Instead of manually reshaping tensors using .view() or .reshape(), torch.nn.Flatten provides a more readable and explicit way to flatten data within a model architecture.\n\n\n\n5. Usage:\nImagine you have a batch of images with the shape [batch_size, channels, height, width]. After passing them through convolutional layers, you might get a shape like [batch_size, 64, 7, 7]. Before sending this to a fully connected layer, you’d use the flatten layer:\nflat_layer = torch.nn.Flatten()\nflattened_data = flat_layer(conv_output)\nNow, flattened_data will have a shape [batch_size, 64*7*7], ready to be passed to a fully connected layer.\n\n\n6. In Context:\nIf you’re familiar with other deep learning frameworks, you might recognize this as similar to TensorFlow’s tf.keras.layers.Flatten or Keras’s Flatten layer. It’s a staple in the toolkit of designing deep learning architectures.\n\n\nConclusion:\ntorch.nn.Flatten is a utility layer in PyTorch that streamlines the process of converting multi-dimensional tensors into a one-dimensional format, easing the transition from convolutional layers to fully connected layers in neural network architectures. It’s a straightforward yet crucial component for many deep learning models, particularly CNNs."
  },
  {
    "objectID": "wavenet.html#torch.nn.sequential.",
    "href": "wavenet.html#torch.nn.sequential.",
    "title": "Wavenet",
    "section": "torch.nn.Sequential.",
    "text": "torch.nn.Sequential.\n\n1. The Basic Idea:\nWhen building neural networks, we often create architectures that involve a series of layers or operations that process data in a specific order. torch.nn.Sequential is a container provided by PyTorch that allows us to encapsulate a sequence of modules or operations into a single module, streamlining both the definition and execution of such sequences.\n\n\n2. torch.nn.Sequential:\nAt its core, torch.nn.Sequential is essentially an ordered container of modules. Data passed to a Sequential module will traverse through each contained module in the order they were added, with the output of one module becoming the input to the next.\n\n\n3. Advantages:\n\nReadability: Architectures, especially simpler ones, become more readable and compact. Instead of defining and calling layers separately, you can consolidate them into a single Sequential block.\nModularity: It allows for easy reuse of certain sequences of operations across different architectures. If a specific sequence of layers gets used frequently, encapsulating it within a Sequential block makes it easier to plug into various models.\n\n\n\n4. Usage:\nSuppose you’re designing a simple feedforward neural network with two hidden layers and ReLU activations:\nWithout Sequential:\nself.fc1 = torch.nn.Linear(input_size, hidden_size)\nself.relu1 = torch.nn.ReLU()\nself.fc2 = torch.nn.Linear(hidden_size, hidden_size)\nself.relu2 = torch.nn.ReLU()\nself.fc3 = torch.nn.Linear(hidden_size, output_size)\nWith Sequential:\nself.layers = torch.nn.Sequential(\n    torch.nn.Linear(input_size, hidden_size),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_size, hidden_size),\n    torch.nn.ReLU(),\n    torch.nn.Linear(hidden_size, output_size)\n)\nThe latter is clearly more concise and readable.\n\n\n5. Points to Remember:\n\nWhile Sequential is convenient, it’s most suited for networks where the data flow is linear. For architectures with branches (like skip connections in ResNets) or multiple inputs/outputs, manual layer definition might be more appropriate.\nModules in Sequential are executed in the order they’re added, making the order crucial. Always ensure that layers are added in the intended sequence.\n\n\n\n6. In Context:\nIf you’re familiar with other deep learning frameworks, the concept might remind you of Keras’s Sequential model. The idea of simplifying linear stacks of layers is a common one across various deep learning libraries, given its convenience.\n\n\nConclusion:\ntorch.nn.Sequential is a convenient tool in the PyTorch library that helps in compactly defining and organizing linear sequences of operations in neural network architectures. While incredibly useful for straightforward, linear data flows, it’s essential to remember its limitations when dealing with more complex architectures.\n\n# Layers made in part 3\nclass Linear:\n    def __init__(self, fan_in, fan_out, bias = True):\n        self.weight = torch.randn((fan_in, fan_out))\n        self.weight /= fan_in ** 0.5\n        self.bias = torch.zeros((fan_out)) if bias else None\n\n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n\n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n        # buffers (trained while running `momentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        if self.training:\n            # batch mean\n            xmean = x.mean(0, keepdim= True)\n            # batch variance\n            xvar = x.var(0, keepdim= True)\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n\n        # update the buffers in training\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        \n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n    \n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n\n    def parameters(self):\n        return []\n    \n# ---------------- new ----------------\nclass Embedding:\n    def __init__(self, num_embeddings, embedding_dim):\n        self.weight = torch.randn((num_embeddings, embedding_dim))\n    \n    def __call__(self, x):\n        self.out = self.weight[x]\n        return self.out\n    \n    def parameters(self):\n        return [self.weight]\n    \nclass Flatten:\n    def __call__(self, x):\n        self.out = x.view((x.shape[0], -1))\n        return self.out\n    \n    def parameters(self):\n        return []\n\nclass Sequential:\n    def __init__(self, layers):\n        self.layers = layers\n    \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n\n        self.out = x\n        return self.out\n\n    def parameters(self):\n        return [p for l in self.layers for p in l.parameters()]"
  },
  {
    "objectID": "wavenet.html#initialize-the-model",
    "href": "wavenet.html#initialize-the-model",
    "title": "Wavenet",
    "section": "Initialize the model",
    "text": "Initialize the model\n\nn_embd = 10\nn_hidden = 200\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    Flatten(),\n\n    Linear(n_embd * block_size, n_hidden, bias= False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n\n    Linear(n_hidden, vocab_size)\n])\n\nwith torch.no_grad():\n    layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(f\"num parameters: {sum(p.numel() for p in parameters)}\")\nfor p in parameters:\n    p.requires_grad_()\n\nnum parameters: 12097"
  },
  {
    "objectID": "wavenet.html#training-the-model",
    "href": "wavenet.html#training-the-model",
    "title": "Wavenet",
    "section": "Training the model",
    "text": "Training the model\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\nupdate_to_data_ratio = []\n\nfor i in range(max_steps):\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix]\n\n    # forward pass is now simpler\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb)\n\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 10000 else 0.01\n    for p in parameters:\n        p.data -= lr * p.grad\n    \n    # track stats\n    if i % 10000 == 0:\n        print(f\"step {i} loss {loss.item()}\")\n\n    lossi.append(loss.item())\n\nstep 0 loss 3.531754493713379"
  },
  {
    "objectID": "wavenet.html#model-evaluation",
    "href": "wavenet.html#model-evaluation",
    "title": "Wavenet",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\nfor layer in model.layers:\n    layer.training = False\n\n\n@torch.no_grad()\ndef split_loss(split):\n    x, y = {\n        \"train\": (Xtr, Ytr),\n        \"valid\": (Xdev, Ydev),\n        \"test\": (Xte, Yte)\n    }[split]\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    return loss.item()\n\nprint(\"train\", split_loss(\"train\"))\nprint(\"valid\", split_loss(\"valid\"))\n\ntrain 3.5045688152313232\nvalid 3.5048117637634277"
  },
  {
    "objectID": "wavenet.html#sample-from-the-model",
    "href": "wavenet.html#sample-from-the-model",
    "title": "Wavenet",
    "section": "Sample from the model",
    "text": "Sample from the model\n\n# sampling from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    out = []\n    context = [0] * block_size\n    while True:\n        # Forward pass\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim = 1)\n\n        ix = torch.multinomial(probs, num_samples = 1).item()\n\n        # Shift the Context Window\n        context = context[1:] + [ix]\n\n        if ix == 0:\n            break\n\n        out.append(ix)\n    \n    print(\"\".join(itos[i] for i in out))\n\nqwzqafikobfomyqgly\njqrfohseadzjqkfgqxaipbfdsgv\nwqf\noidazkdqzhiehjwnwfohppcgtyog\ncsugtawfuhifxaste\nj\nkqmgqxia\nhiahmjcplgpedqivr\nt\nnnoxoxuuxccvktuku\niatuuxghlharqrfzxabcr\nqlocpyradlmtkstjutwjunolzoflgiizsxtnlexesdcbk\nilgulzmehtfglvbafwqxuxxuycvtknohajlsgevrrbbqr\nqjjflupnv\nj\ntiesaedmgwijkcmjcftflpebyfnrqeqix\ngtibmpgexvpynncobkjpnbotjez\nmeqfiuhkejfcjvsigosxgzfhbbkqximglxzmlhvcw\nqidzkdebwwbncdrbwgtatqntzrfshjeqsydqaeohghojkqnkpbldigvxzahljktlupscrthmazgmegwxzsidqjwkn\nbteruejqewqhgiljpdanqpnkogvluvpyofsqitcjcfmtcrdlpxlcfdnrnpj"
  },
  {
    "objectID": "wavenet.html#changing-dataset-blocksize",
    "href": "wavenet.html#changing-dataset-blocksize",
    "title": "Wavenet",
    "section": "Changing Dataset blocksize",
    "text": "Changing Dataset blocksize\n\nblock_size = 8\ndef build_dataset(words):  \n\n  X, Y = [], []\n  \n  for w in words:\n\n    context = [0] * block_size\n    for ch in w + '.':\n\n      ix = stoi[ch]\n\n      X.append(context)\n      Y.append(ix)\n\n      context = context[1:] + [ix]\n\n  X = torch.tensor(X)\n  Y = torch.tensor(Y)\n  print(X.shape, Y.shape)\n  return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr, Ytr = build_dataset(words[:n1])\nXdev, Ydev = build_dataset(words[n1:n2])\nXte, Yte = build_dataset(words[n2:])\n\ntorch.Size([182580, 8]) torch.Size([182580])\ntorch.Size([22767, 8]) torch.Size([22767])\ntorch.Size([22799, 8]) torch.Size([22799])\n\n\n\nfor x, y in zip(Xtr[:15], Ytr[:15]):\n  print(\"\".join(itos[ix.item()] for ix in x), \"----&gt;\", itos[y.item()])\n\n........ ----&gt; e\n.......e ----&gt; b\n......eb ----&gt; r\n.....ebr ----&gt; i\n....ebri ----&gt; m\n...ebrim ----&gt; a\n..ebrima ----&gt; .\n........ ----&gt; h\n.......h ----&gt; i\n......hi ----&gt; l\n.....hil ----&gt; t\n....hilt ----&gt; o\n...hilto ----&gt; n\n..hilton ----&gt; .\n........ ----&gt; j"
  },
  {
    "objectID": "wavenet.html#initializing-a-normal-network",
    "href": "wavenet.html#initializing-a-normal-network",
    "title": "Wavenet",
    "section": "Initializing a normal network",
    "text": "Initializing a normal network\n\nn_embd = 10\nn_hidden = 200\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    Flatten(),\n\n    Linear(n_embd * block_size, n_hidden, bias= False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n\n    Linear(n_hidden, vocab_size)\n])\n\nwith torch.no_grad():\n    layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(f\"num parameters: {sum(p.numel() for p in parameters)}\")\nfor p in parameters:\n    p.requires_grad_()\n\nnum parameters: 22097"
  },
  {
    "objectID": "wavenet.html#implementing-wavenet",
    "href": "wavenet.html#implementing-wavenet",
    "title": "Wavenet",
    "section": "Implementing WaveNet",
    "text": "Implementing WaveNet"
  },
  {
    "objectID": "wavenet.html#our-model-gets-improved-using-ideas-from-wavenet",
    "href": "wavenet.html#our-model-gets-improved-using-ideas-from-wavenet",
    "title": "Wavenet",
    "section": "Our model gets improved using ideas from Wavenet",
    "text": "Our model gets improved using ideas from Wavenet\nThe finished model is inspired by WaveNet, which is a deep learning architecture designed for generating raw audio waveforms.\nInnovation: 1. Hierarchical Fusion of Information: Instead of squashing all the character information into a single layer right at the beginning, the new model aims for a more hierarchical approach. This is akin to WaveNet’s methodology where information from previous contexts gets fused progressively as the network gets deeper. It’s a departure from the original network that was more linear in its approach. 2. FlattenConsecutive Layer: This new layer is essentially reshaping the data by grouping consecutive embeddings, which helps in retaining more granularity of information for longer sequences. 3. Increased Depth with Batch Normalization: The model has added depth, with multiple hidden layers interspersed with BatchNorm layers. Batch Normalization helps in stabilizing and accelerating the training of deeper networks.\nIntuition: 1. Preserving Contextual Information: By not immediately squashing all characters into a single layer, the network retains more of the raw, granular information from the input. This is crucial when predicting the next character based on a sequence of prior characters. The more original context the model has, the better its predictive capability. 2. Progressive Fusion of Information: Just as our human cognition processes information hierarchically (from letters to words to sentences to paragraphs), the model is designed to gradually combine information. It first understands pairs of characters, then bigger chunks, and so on. This allows the model to capture both short-term and long-term dependencies in the data. 3. Stability with Batch Normalization: Deep networks can suffer from internal covariate shift where the distribution of layer inputs changes during training. Batch normalization standardizes the inputs of a layer, making training more stable and faster. 4. Embedding Layer: It’s a look-up table that maps from integer indices (representing specific words or characters) to dense vectors (their embeddings). These vectors are trainable and can capture the semantic relationship between words or characters. By using embeddings, the model can capture richer representations of the input data.\nIn summary, the hierarchical approach is inspired by WaveNet’s methodology of processing audio signals, where the prediction for the next audio sample depends on a gradually fused context of previous samples. By applying a similar approach to character prediction, the model aims to capture richer contextual information, leading to better predictions."
  },
  {
    "objectID": "wavenet.html#wavenet-implementation-and-tensor-management",
    "href": "wavenet.html#wavenet-implementation-and-tensor-management",
    "title": "Wavenet",
    "section": "WaveNet Implementation and Tensor Management",
    "text": "WaveNet Implementation and Tensor Management\n\nForward Pass Visualization\nThe lecturer is working on a neural network implementation of WaveNet. To ensure understanding and correct functioning, they visualize the forward pass by observing tensor shapes at each stage. This helps in understanding data transformations as it progresses through the network.\n\n\nInput Batch and Shape\nA batch of 4 random examples is created for debugging. The shape of the batch (referred to as ( xB )) is ($ 4 $) due to having 4 examples and a block size of 8.\n\n\nEmbedding Layer\nThe first layer is the embedding layer. When the integer tensor ( xB ) is passed through this layer, the output shape becomes ( $4 $). Here, each character has a 10-dimensional vector representation. The embedding layer takes the integers and converts them into these 10-dimensional vectors.\n\n\nFlattening and Concatenation\nThe flattened layer views the ( $4 $) tensor as a ( $4 $) tensor. The effect is that the 10-dimensional embeddings for the 8 characters are lined up in a row, appearing as if they’ve been concatenated.\n\n\nLinear Layer and Matrix Multiplication\nThe linear layer is responsible for transforming the shape from ( $4 \\(\\) to \\(\\) 4 $ ). This is achieved through matrix multiplication. The lecturer emphasizes that in PyTorch, the matrix multiplication operator is versatile and can handle higher-dimensional tensors, treating earlier dimensions as batch dimensions.\n\n\nRestructuring Input\nA key insight is that instead of flattening the entire input, we can group and process parts of it. For instance, the lecturer suggests grouping every two consecutive elements for processing in parallel. This results in a tensor shape of ($ 4 $).\n\n\nFlattening Consecutively\nTo achieve the desired restructuring, the lecturer introduces a new method called “Flatten Consecutive”. This method differs from the regular flattening by allowing for flattening only a specified number of consecutive elements, leading to multi-dimensional outputs rather than fully flattened ones.\n\n\nModel Layers and Parameter Count\nThe lecturer moves on to demonstrate how the neural network layers are organized. They ensure that the number of parameters remains consistent as the model architecture evolves, emphasizing the importance of maintaining model capacity.\n\n\nWaveNet’s Performance\nAfter restructuring the neural network, the lecturer observes that the validation loss remains nearly identical to the original, simpler model. This suggests that, at least in this instance, the added complexity doesn’t yield performance benefits.\n\n\nPotential Issues with BatchNorm1D\nThe lecturer points out that while the model runs, there might still be issues, specifically with the BatchNorm1D layer. A thorough review of this layer is necessary to ensure it’s functioning correctly.\n\n\nShape Exploration\n\n# look at batch of 5 examples (it's 4 in the original video but I changed it to 5 to prevent confusion)\nix = torch.randint(0, Xtr.shape[0], (5,))\nXb, Yb = Xtr[ix], Ytr[ix]\n\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([5, 8])\n\n\ntensor([[ 0, 19,  1, 14, 20,  9, 14, 15],\n        [ 0,  0,  0,  0, 26,  1, 13,  9],\n        [ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0, 16,  5],\n        [ 0,  0,  0,  0,  0, 14,  1,  8]])"
  },
  {
    "objectID": "wavenet.html#update-embedding-layer",
    "href": "wavenet.html#update-embedding-layer",
    "title": "Wavenet",
    "section": "Update Embedding Layer",
    "text": "Update Embedding Layer\n\n1. Current Model State:\nThe current model has training and validation losses that are close to each other. This suggests that the model isn’t overfitting. In such cases, a common approach to improve performance is to expand the model: increase its capacity by adding more neurons or layers.\n\n\n2. Problem with Current Architecture:\nRight now, the model takes in a sequence of characters, processes them through a single layer, and predicts the next character. This is somewhat akin to trying to understand a sentence by reading all its words at once. While you can add more layers, you’re still compressing all the information at the very beginning, which might be suboptimal.\n\n\n3. Inspiration from WaveNet:\nWaveNet offers a different approach. Instead of compressing all characters at once, it processes the input in a hierarchical manner. Imagine trying to understand a sentence not word by word, but by understanding two words at a time, then four words, then eight, and so on. This allows the model to capture relationships and patterns at different scales.\n\n\n4. Progressive Fusion:\nThe key idea is to combine (or “fuse”) input data progressively. Start by combining pairs of characters (bigrams). Then, combine pairs of bigrams to form four-character chunks, and so on. This slow fusion ensures that the model has a more refined understanding of the input data at various levels of granularity.\n\n\n5. Dilated Causal Convolutions:\nWhile it sounds complex, the core idea is about efficiency and preserving information. In standard convolutions, each layer can only see a limited portion of the input. By using dilated convolutions, each layer can see a wider range of input, allowing the model to capture longer-term dependencies without needing extremely deep architectures. The “causal” part ensures that the prediction at any time step is only based on past and current data, not future data.\n\n\nConclusion:\nIn essence, the lecturer is suggesting moving from a simplistic model that quickly compresses input information to a more sophisticated architecture that understands the input in a layered and hierarchical manner. This approach, inspired by WaveNet, allows the model to capture patterns and relationships at different scales, potentially leading to better performance. The implementation details, like dilated causal convolutions, are there to ensure efficiency and respect the temporal nature of the data.\n\nfor layer in model.layers:\n    print(f\"{layer.__class__.__name__} has output size of: {layer.out.shape}\")\n\nEmbedding has output size of: torch.Size([5, 8, 10])\nFlatten has output size of: torch.Size([5, 80])\nLinear has output size of: torch.Size([5, 200])\nBatchNorm1d has output size of: torch.Size([5, 200])\nTanh has output size of: torch.Size([5, 200])\nLinear has output size of: torch.Size([5, 27])\n\n\nWe don’t want to process the 8 characters at the same time\n1 2 3 4 5 6 7 8\nBut we want to process them in 4 groups of 2 characters in parallel\n(1 2) (3 4) (5 6) (7 8)\nSo instead of multiplying (5, 80) @ (80, 200) = (5, 200) we want to multiply (5, 4, 20) @ (20, 200) = (5, 4, 200)\n\n# output of layer 0\ne = torch.randn(5, 8, 10)\n# contacenate even and odd (on character dimension) elements of the last dimension\nexplicit = torch.cat([e[:, ::2, :], e[:, 1::2, :]], dim = 2)\n# you can do the same using view\nimplicit = e.view(5, 4, 20)\n\n(implicit == explicit).all()\n\ntensor(True)\n\n\n\n\nFlattenConsectutive Class\n## torch.squeeze\nThe torch.squeeze function removes dimensions of size 1 from a tensor. It’s particularly useful when certain operations introduce unwanted singleton dimensions, and you want to revert back to a more compact shape.\nFunction signature:\ntorch.squeeze(input, dim=None, *, out=None)\n\ninput (Tensor): The input tensor.\ndim (int, optional): Specifies which dimension to squeeze. If not specified, all dimensions of size 1 will be squeezed.\nout (Tensor, optional): The output tensor.\n\n\n\nExamples:\n\nSqueezing all dimensions of size 1:\n\nimport torch\n\n# A tensor with shape [1, 3, 1, 2]\nx = torch.tensor([[[[1, 2]], [[3, 4]], [[5, 6]]]])\nprint(x.shape)  # torch.Size([1, 3, 1, 2])\n\ny = torch.squeeze(x)\nprint(y.shape)  # torch.Size([3, 2])\nHere, torch.squeeze removed the first and third dimensions, both of size 1.\n\nSqueezing a specific dimension:\n\nIf you only want to squeeze a specific dimension, you can specify it using the dim argument.\nz = torch.squeeze(x, dim=0)\nprint(z.shape)  # torch.Size([3, 1, 2])\nIn this case, only the first dimension of size 1 was squeezed.\n\nA tensor with no dimensions of size 1:\n\na = torch.tensor([[1, 2], [3, 4]])\nprint(a.shape)  # torch.Size([2, 2])\n\nb = torch.squeeze(a)\nprint(b.shape)  # torch.Size([2, 2])\nAs there were no dimensions of size 1, torch.squeeze had no effect on the tensor’s shape.\n\n\nNote:\nBe cautious when using torch.squeeze without specifying a dimension. In some cases, especially when your tensor might sometimes have singleton dimensions due to variable data sizes (e.g., batch size of 1 in deep learning models), unintended squeezing might lead to shape mismatches or other errors in subsequent operations.\n\n# Reimplement Flatten\nclass FlattenConsecutive:\n    def __init__(self, n):\n        # n is the number of consecutive elements we want (2 in our example)\n        self.n = n\n    \n    def __call__(self, x):\n        # in our example: B = 5, T = 8, C = 10\n        B, T, C = x.shape\n        # we want to convert X to (5, 4, 20)\n        x = x.view(B, T // self.n, C * self.n)\n\n        if x.shape[1] == 1:\n            x = x.squeeze(1)\n        \n        self.out = x\n        return self.out\n\n    def parameters(self):\n        return []\n\n\n\nPrevious behavior using FlattenConsecutive\n\nn_embd = 10\nn_hidden = 200\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    # calling FlattenConsecutive(block_size) will return in the same previous behavior\n    FlattenConsecutive(block_size),\n\n    Linear(n_embd * block_size, n_hidden, bias= False),\n    BatchNorm1d(n_hidden),\n    Tanh(),\n\n    Linear(n_hidden, vocab_size)\n])\n\nwith torch.no_grad():\n    layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(f\"num parameters: {sum(p.numel() for p in parameters)}\")\nfor p in parameters:\n    p.requires_grad_()\n\nnum parameters: 22097\n\n\n\nix = torch.randint(0, Xtr.shape[0], (5,))\nXb, Yb = Xtr[ix], Ytr[ix]\n\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([5, 8])\n\n\ntensor([[ 0,  0,  3,  1, 18, 18,  9,  3],\n        [ 0,  0,  0, 19, 20,  1, 18, 18],\n        [ 0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0, 11,  1, 12,  5, 20],\n        [ 0,  0,  0,  4,  1, 25, 12,  5]])\n\n\n\nfor layer in model.layers:\n    print(f\"{layer.__class__.__name__} has output size of: {(layer.out.shape)}\")\n\nEmbedding has output size of: torch.Size([5, 8, 10])\nFlattenConsecutive has output size of: torch.Size([5, 80])\nLinear has output size of: torch.Size([5, 200])\nBatchNorm1d has output size of: torch.Size([5, 200])\nTanh has output size of: torch.Size([5, 200])\nLinear has output size of: torch.Size([5, 27])\n\n\n\n\nProcessing Hierarchically: FlattenConsecutive(2)\n\nn_embd = 10\n# changing the number of hidden units to 68 keeps the same number of parameters as the previous model (22k)\nn_hidden = 68\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias= False), BatchNorm1d(n_hidden),Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias= False), BatchNorm1d(n_hidden),Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias= False), BatchNorm1d(n_hidden),Tanh(),\n    Linear(n_hidden, vocab_size),\n])\n\nwith torch.no_grad():\n    layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(f\"num parameters: {sum(p.numel() for p in parameters)}\")\nfor p in parameters:\n    p.requires_grad_()\n\nnum parameters: 22397\n\n\n\nix = torch.randint(0, Xtr.shape[0], (5,))\nXb, Yb = Xtr[ix], Ytr[ix]\n\nlogits = model(Xb)\nprint(Xb.shape)\nXb\n\ntorch.Size([5, 8])\n\n\ntensor([[ 0,  0,  0,  0,  0,  0, 13,  5],\n        [ 0,  0,  0,  0,  0, 14,  9, 19],\n        [ 0,  0,  1,  9, 25,  1, 14, 14],\n        [ 0,  0,  0,  0, 25, 15, 21, 19],\n        [ 0,  0,  0,  0,  0,  0,  0,  4]])\n\n\n\nfor layer in model.layers:\n    print(f\"{layer.__class__.__name__} has output size of: {layer.out.shape}\")\n\nEmbedding has output size of: torch.Size([5, 8, 10])\nFlattenConsecutive has output size of: torch.Size([5, 4, 20])\nLinear has output size of: torch.Size([5, 4, 68])\nBatchNorm1d has output size of: torch.Size([5, 4, 68])\nTanh has output size of: torch.Size([5, 4, 68])\nFlattenConsecutive has output size of: torch.Size([5, 2, 136])\nLinear has output size of: torch.Size([5, 2, 68])\nBatchNorm1d has output size of: torch.Size([5, 2, 68])\nTanh has output size of: torch.Size([5, 2, 68])\nFlattenConsecutive has output size of: torch.Size([5, 136])\nLinear has output size of: torch.Size([5, 68])\nBatchNorm1d has output size of: torch.Size([5, 68])\nTanh has output size of: torch.Size([5, 68])\nLinear has output size of: torch.Size([5, 27])\n\n\nhowever, this network gives the same loss = 2.0\n\n\nFixing BatchNorm Bug\nWe implemented batchnorm for X 2D only. We calculated mean and variance for the first dimension only. We don’t want to average over the batch dimension only, but also over the 2nd dimension (the 4 groups of 2 characters)\n\nIssue: The current BatchNorm1D implementation assumes a two-dimensional input, but the actual input is three-dimensional. This discrepancy leads to improper calculations.\nCurrent Behavior: The BatchNorm receives an input with dimensions 32x4x68. Although this shape allows the code to run without errors due to broadcasting, it doesn’t work as intended.\nDesired Behavior: The BatchNorm should be modified to consider both the zeroth and first dimensions as batch dimensions. Instead of averaging over 32 numbers, the average should be over (\\(32 \\times 4\\)) numbers for each of the 68 channels.\nSolution: The lecturer suggests using the torch.mean function, which can reduce over multiple dimensions at the same time. By passing in a tuple (0,1) as dimensions, the mean is calculated over both the zeroth and first dimensions, leading to a 1x1x68 shape."
  },
  {
    "objectID": "wavenet.html#deviation-from-pytorch-api",
    "href": "wavenet.html#deviation-from-pytorch-api",
    "title": "Wavenet",
    "section": "Deviation from PyTorch API",
    "text": "Deviation from PyTorch API\nThere’s a highlighted difference between the lecturer’s implementation and PyTorch’s BatchNorm1D:\n\nPyTorch’s BatchNorm1D: Assumes that when input is three-dimensional, it should be in the form of nxCxL (with C being the number of features or channels).\nLecturer’s Implementation: Assumes the input to be in the form of nxLxC."
  },
  {
    "objectID": "wavenet.html#development-process-of-building-deep-neural-nets",
    "href": "wavenet.html#development-process-of-building-deep-neural-nets",
    "title": "Wavenet",
    "section": "Development Process of Building Deep Neural Nets",
    "text": "Development Process of Building Deep Neural Nets\n\nReference to Documentation: It’s essential to frequently refer to the documentation to understand the various layers, their expected input shapes, and functionalities. However, the lecturer notes that PyTorch documentation can sometimes be misleading or incomplete.\nShape Management: A significant amount of time is spent ensuring tensor shapes are compatible. This involves reshaping tensors, understanding expected input and output shapes, and sometimes prototyping to ensure shapes align.\nPrototyping: The lecturer emphasizes the utility of Jupyter notebooks for prototyping. Once satisfied with the prototype, the code is transferred to a more permanent codebase.\nUse of Convolutions: Convolutions are introduced as a means for efficiency. Instead of processing inputs individually, convolutions allow the model to process multiple inputs simultaneously by sliding filters over the input sequence. This concept connects with future topics, like Convolutional Neural Networks (CNNs).\n\n\ne = torch.rand(32, 4, 68)\nemean = e.mean(dim = (0,1), keepdim = True) # (1, 1, 68)\nevar = e.var((0,1), keepdim = True) # (1, 1, 68)\nehat = (e - emean) / torch.sqrt(evar + 1e-5)\n\nprint(ehat.shape)\nprint(f\"shape of running mean is {model.layers[3].running_mean.shape}\")\n\ntorch.Size([32, 4, 68])\nshape of running mean is torch.Size([1, 4, 68])\n\n\n\nclass BatchNorm1d:\n    def __init__(self, dim, eps=1e-5, momentum = 0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n        # buffers (trained while running `momentum update`)\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n\n    def __call__(self, x):\n        if self.training:\n            # determine the dimension to reduce over\n            if x.ndim == 2:\n                dim = 0\n            elif x.ndim == 3:\n                dim = (0,1)\n            \n            xmean = x.mean(dim, keepdim= True)\n            # batch variance\n            xvar = x.var(dim, keepdim= True)\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        \n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n        self.out = self.gamma * xhat + self.beta\n\n        # update the buffers in training\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        \n        return self.out\n    \n    def parameters(self):\n        return [self.gamma, self.beta]\n\n\nn_embd = 10\n# changing the number of hidden units to 68 keeps the same number of parameters as the previous model (22k)\nn_hidden = 68\n\nmodel = Sequential([\n    Embedding(vocab_size, n_embd),\n    FlattenConsecutive(2), Linear(n_embd * 2, n_hidden, bias= False), BatchNorm1d(n_hidden),Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias= False), BatchNorm1d(n_hidden),Tanh(),\n    FlattenConsecutive(2), Linear(n_hidden * 2, n_hidden, bias= False), BatchNorm1d(n_hidden),Tanh(),\n    Linear(n_hidden, vocab_size),\n])\n\nwith torch.no_grad():\n    layers[-1].weight *= 0.1\n\nparameters = model.parameters()\nprint(f\"num parameters: {sum(p.numel() for p in parameters)}\")\nfor p in parameters:\n    p.requires_grad_()\n\nnum parameters: 22397"
  },
  {
    "objectID": "wavenet.html#improving-wavenets-performance",
    "href": "wavenet.html#improving-wavenets-performance",
    "title": "Wavenet",
    "section": "Improving WaveNet’s Performance",
    "text": "Improving WaveNet’s Performance\n\nCurrent Performance: The model’s performance has improved from a loss of 2.1 to 1.993.\nChallenges: The lecturer points out that the current approach lacks an experimental harness, meaning they’re mostly making educated guesses without a systematic way to evaluate changes.\nPotential Improvements: Suggestions include re-allocating channels, tweaking the number of dimensions for embeddings, or even reverting to a simpler network structure. The WaveNet paper itself might also have additional strategies or layers worth implementing."
  },
  {
    "objectID": "mlp.html",
    "href": "mlp.html",
    "title": "Multilayer Perceptron",
    "section": "",
    "text": "import string\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nwith open('./names.txt') as f:\n    names = f.read().splitlines()\nTotal number of names\nlen(names)\n\n32033\nProblem with bigrams is that we were only looking the previous character to predict the next character, the more characters we look back, the bigger the matrix gets, with 2 characters then 27**2 sized matrix.\nInstead we use this:"
  },
  {
    "objectID": "mlp.html#a-neural-probabilistic-language-model-paper",
    "href": "mlp.html#a-neural-probabilistic-language-model-paper",
    "title": "Multilayer Perceptron",
    "section": "A Neural Probabilistic Language Model: paper",
    "text": "A Neural Probabilistic Language Model: paper\nIn the paper, they’ve 17000 words in the vocabulary and for each word they assign a feature vector of dimension (30,). So all the 17000 words are embedded into a 30-dimensional space.\nInitially the feature vectors are random and on training the feature vectors are updated…words with similar meanings are closer to each other in the vector space and converse is true.\nModelling approach: maximize log likelihood.\n\nModel Generalization: example (from the paper):\nIn the training set we might have a sentence like “the cat is walking in the bedroom”, “the dog is running in a room”.\nDue to the feature vectors, (a,the), (cat,dog), (room,bedroom), (is,was), (running,walking) might be closer together in the embedding space.\nThis allows the model to predict stuff like “the dog is walking in a ________” -&gt; bedroom, “a cat is running the the _______” -&gt; room\n\n\nDiagram\ninput: 3 previous words, output: 4th word. For each word there’s a one-hot index vector, and then there’s the embedding vector 17000X30. So a dot product will result in the feature vector of that word.\n# with 10 words vocab and 3-dimensional embedding feature vector\nx = torch.tensor([0,0,1,0,0,0,0,0,0,0]) # word index vector\ny = torch.randn(10,3) # embedding vector\ny[2,:], x.float() @ y\n\n&gt;&gt;&gt; (tensor([1.2606, 0.1792, 0.1153]), tensor([1.2606, 0.1792, 0.1153]))\nso for the 3 words, 90 neurons: 30 from each word\nhidden layer: size is the hyperparameter, fully connected to the 90 input neurons with tanh activation.\noutput layer: (17000,) layer with softmax which will give probability distribution for the next word in sequence. with argmax(output), we can get the index of the next word.\n\n\n\nmodel"
  },
  {
    "objectID": "bigram.html",
    "href": "bigram.html",
    "title": "Bigram Language Model Character Level",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nReading the names from the names.txt which contains the names in lowercase, separated by new line. Dataset is downloaded from https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\nwords = open(\"names.txt\", \"r\").read().splitlines()\n\n# Exploring\nprint(f\"first 10 words{words[:10]}\")\nprint(f\"length of words: {len(words)}\")\nprint(f\"min word length {min(len(w) for (w) in words)} and max word length {max(len(w) for (w) in words)}\")\n\nfirst 10 words['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\nlength of words: 32033\nmin word length 2 and max word length 15"
  },
  {
    "objectID": "bigram.html#bigram-dictionary",
    "href": "bigram.html#bigram-dictionary",
    "title": "Bigram Language Model Character Level",
    "section": "Bigram Dictionary",
    "text": "Bigram Dictionary\n\n# getting the Bigrams\nb = {}\nfor w in words:\n    # add start and end tokens\n    chs = [\"&lt;S&gt;\"] + list(w) + [\"&lt;E&gt;\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        bigram = (ch1, ch2)\n        # print(bigram)\n\n        # use get instead of b[bigram] to avoid KeyError and set default to 0\n        b[bigram] = b.get(bigram, 0) + 1\n\nWe initially used &lt;S&gt; and &lt;E&gt; as the start and end token respectively. But this approach is not useful enough because we can’t have a character that starts before &lt;S&gt; so in the N lookup table there will be a complete row which have 0’s in it.\nHowever instead of using &lt;S&gt; and &lt;E&gt;, we later substitue it for a single . character which indicates both the start and the end and make slight improvement in our code.\n\n# print the most common bigrams in the data (sort by value)\nsorted(b.items(), key = lambda kv: kv[1], reverse = True)[:10]\n\n[(('n', '&lt;E&gt;'), 6763),\n (('a', '&lt;E&gt;'), 6640),\n (('a', 'n'), 5438),\n (('&lt;S&gt;', 'a'), 4410),\n (('e', '&lt;E&gt;'), 3983),\n (('a', 'r'), 3264),\n (('e', 'l'), 3248),\n (('r', 'i'), 3033),\n (('n', 'a'), 2977),\n (('&lt;S&gt;', 'k'), 2963)]"
  },
  {
    "objectID": "bigram.html#bigram-2d-tensor",
    "href": "bigram.html#bigram-2d-tensor",
    "title": "Bigram Language Model Character Level",
    "section": "Bigram 2D Tensor",
    "text": "Bigram 2D Tensor\nWe are declaring a tensor which we will use to store the counts of our bigrams\n\nN = torch.zeros(28, 28, dtype = torch.int32)\n\nNow we can’t pass string data to our model we need to convert the characters to the number. Why characters? because it is a character level language model Bigram which means we will feed one character to our model and it will try to predict the next character in a sequence. We can also pass multiple character to our model but this is not the architecture of Bigram.\n\n# make a list of characters (a -&gt; z)\nchars = sorted(list(set(\"\".join(words))))\n\n# make a dictionary of character to index\nstoi = {ch: i for (i, ch) in enumerate(chars)}  # alphabet as key, integer as value\nstoi[\"&lt;S&gt;\"] = len(chars)\nstoi[\"&lt;E&gt;\"] = len(chars) + 1\n\n# make a dictionary of index to character\nitos = {i: ch for (ch, i) in stoi.items()} # integer as key, alphabet as value\n\nSample Bigram Language Model – Now basically A bigram language model is a type of language model that predicts the probability of a word in a sequence based on the previous word. Same is true for the character.\nIn the word case our vocabulary can be 17000 words or 100000 words based on the size of the problem, which in this case each word is assigned a index to be feed in to the model. But in the character level language model our vocabulary size is total number of character used in our whole dataset which are in this case 26 characters and we append . at the start and end of each name so total of we have 27 characters in our problem so our vocabulary size is 27 characters.\n\n# getting the Bigrams\nfor w in words:\n    # add start and end tokens\n    chs = [\"&lt;S&gt;\"] + list(w) + [\"&lt;E&gt;\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        N[ix1, ix2] += 1\n\nPlotting Counts\n\n# visualize this matrix\nplt.figure(figsize = (16, 16))\nplt.imshow(N, cmap = \"Blues\")\nfor i in range(28):\n    for j in range(28):\n        # character strings\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha = \"center\", va = \"bottom\", color = \"black\")\n        # bigram counts (.item() converts tensor to int)\n        plt.text(j, i, N[i, j].item(), ha = \"center\", va = \"top\", color = \"black\")\nplt.show()\n\n\n\n\n\nFixing zero row and column problem\nNotice we have empty row and column for &lt;E&gt; and &lt;S&gt; tokens respectively\nWe will solve that by replacing both of them with . token as follows:\n# make a list of characters (a -&gt; z)\nchars = sorted(list(set(\"\".join(words))))\n# make a dictionary of character to index\n\nstoi = {ch: i for (i, ch) in enumerate(chars)}\n# remove theses tokens from the dictionary\n\nstoi[\"&lt;S&gt;\"] = len(chars)\nstoi[\"&lt;E&gt;\"] = len(chars) + 1\n\n# and add this token\nstoi[\".\"] = len(chars)\n# make a dictionary of index to character\nitos = {i: ch for (ch, i) in stoi.items()}\nThis time we are taking 27 characters because we are using . character at the start and at the end of each name\n\nN = torch.zeros(27, 27, dtype = torch.int32)\n\n\n# make a list of characters (a -&gt; z)\nchars = sorted(list(set(\"\".join(words))))\n\n# make a dictionary of character to index\nstoi = {ch: i + 1 for (i, ch) in enumerate(chars)}  # alphabet as key, integer as value\n#  set start and end tokens to 0\nstoi[\".\"] = 0\n\n# make a dictionary of index to character\nitos = {i: ch for (ch, i) in stoi.items()} # integer as key, alphabet as value\n\nIn the below code we are counting out of total 27*27 pairs of bigrams, how many times each bigram appeared in our names dataset. Also note than we append . character at the start and at the end of each name.\n\n# getting the Bigrams\nfor w in words:\n    # add start and end tokens\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        N[ix1, ix2] += 1\n\nPlotting the Counts\n\n# visualize this matrix\nplt.figure(figsize = (16, 16))\nplt.imshow(N, cmap = \"Blues\")\nfor i in range(27):\n    for j in range(27):\n        # character strings\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha = \"center\", va = \"bottom\", color = \"black\")\n        # bigram counts (.item() converts tensor to int)\n        plt.text(j, i, N[i, j].item(), ha = \"center\", va = \"top\", color = \"black\")\nplt.show()\n\n\n\n\nNotice: - .. is zero, since we don’t have empty words - the first row is start words - the first column is end words"
  },
  {
    "objectID": "bigram.html#sampling-from-the-model",
    "href": "bigram.html#sampling-from-the-model",
    "title": "Bigram Language Model Character Level",
    "section": "Sampling from the model",
    "text": "Sampling from the model\n\n# probability of the first character\np = N[0].float() / N[0].sum()\np\n\ntensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])\n\n\nProbability that any of the 27 characters can be the first character is 1\n\np.sum()\n\ntensor(1.)\n\n\n\n# probability \np = N[0].float() / N[0].sum()\n\n# generator is a seed for reproducibility\ng = torch.Generator().manual_seed(2147483647)\n\n# sample from the p distribution using the generator (I got different results from Andrej's idk why)\nix = torch.multinomial(p, num_samples = 1 , replacement = True, generator = g).item()\n# convert index to character\nitos[ix]\n\n'j'\n\n\nUsing the p as probability distribution we will use it with torch.multinomial to draw samples from p based on the probability\ntorch.multinomial Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor p.\nSo for each row of the character we draw using torch.multinomial we then calculate probability distribution for that row in the loop which undermine our performance\ntorch.rand Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0,1)\n\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        p = N[ix].float() / N[ix].sum()\n        ix = torch.multinomial(p, num_samples = 1 , replacement = True, generator = g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n\n    print(\"\".join(out))\n\njunide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias.\n\n\n\nVectorization of Probabilities\nInstead of calculating the probability distribution p everytime\n\nBroadcasting Example\nTwo tensors are “broadcastable” if the following rules hold:\n\nEach tensor has at least one dimension.\nWhen iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\n\nSame shapes are always broadcastable (i.e. the above rules always hold)x=torch.empty(5,7,3) y=torch.empty(5,7,3)\n\nx=torch.empty(5,7,3)\ny=torch.empty(5,7,3)\n\n\nx and y are not broadcastable, because x does not have at least 1 dimension\n\nx=torch.empty((0,))\ny=torch.empty(2,2)\n\n\nCan line up trailing dimensions - x and y are broadcastable. - 1st trailing dimension: both have size 1 - 2nd trailing dimension: y has size 1 - 3rd trailing dimension: x size == y size - 4th trailing dimension: y dimension doesn’t exist\n\nx=torch.empty(5,3,4,1)\ny=torch.empty(  3,1,1)\n\n\nx and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n\nx=torch.empty(5,2,4,1)\ny=torch.empty(  3,1,1)\n\n\n\nVisual Example\n\nx = torch.tensor([[1,2,3],[4,5,6], [7,8,9]])\n# visualize this matrix\nplt.figure(figsize = (3, 3))\nplt.imshow(x, cmap = \"Blues\")\nfor i in range(3):\n    for j in range(3):\n        # character strings\n        chstr = x[i, j].item()\n        plt.text(j, i, chstr, ha = \"center\", va = \"bottom\", color = \"black\")\n        plt.xticks([])\n        plt.yticks([])\n\n\n\n\n\n# visualize this matrix\nx = torch.tensor([[1,2,3],[4,5,6], [7,8,9]])\nxs = [\n    torch.cat([(x / x.sum(dim = 0, keepdim = True)), (x / x.sum(dim = 0, keepdim = True)).sum(dim = 1, keepdim = True)], dim = 1), # normalize by column =&gt; incorrect [Wrong]\n    torch.cat([(x / x.sum(dim = 0, keepdim = False)), (x / x.sum(dim = 0, keepdim = False)).sum(dim = 1, keepdim = True)], dim = 1), # normalize by column =&gt; incorrect [Wrong]\n    torch.cat([(x / x.sum(dim = 1, keepdim = True)), (x / x.sum(dim = 1, keepdim = True)).sum(dim = 1, keepdim = True)], dim = 1), # normalize by row =&gt; correct (sum of each row == 1)[Right]\n    torch.cat([(x / x.sum(dim = 1, keepdim = False)), (x / x.sum(dim = 1, keepdim = False)).sum(dim = 1, keepdim = True)], dim = 1) # normalize by column  =&gt; incorrect [Wrong]\n    ]\n\n# visualize this matrices\nfig, axes = plt.subplots(2, 2, figsize = (8, 8))\nfor n in range(4):\n    axes[n // 2, n % 2].imshow(xs[n], cmap = \"Blues\")\n    for i in range(3):\n        for j in range(4):\n            chstr = xs[n][i, j].item()\n            if j == 3:\n                axes[n // 2, n % 2].text(j, i - 0.25, f\"row sum\\n{round(chstr, 3)}\", ha = \"center\", va = \"top\", color = \"white\")\n            else:\n                axes[n // 2, n % 2].text(j, i, round(chstr, 3), ha = \"center\", va = \"bottom\", color = \"black\")\n            if n // 2 == 0 or not(n % 2 == 0):\n                axes[n // 2, n % 2].set_title(f\"dim = {n // 2}, keepdim = {not bool(n % 2)} (incorrect)\")\n            else:\n                axes[n // 2, n % 2].set_title(f\"dim = {n // 2}, keepdim = {not bool(n % 2)} (correct)\")\n\n            # remove ticks\n            axes[n // 2, n % 2].set_xticks([])\n            axes[n // 2, n % 2].set_yticks([])\n\n\n\n\n\n\nVectorization\nWrong boradcasting Example:\nP = N.float() / N.sum(dim = 1, keepdim = False)\n# Broadcasting:\n# N.sum(dim = 1) =&gt; (27) =&gt; (1, 27) it became a row vector [Wrong]\n# N.float() =&gt; (27, 27)\n# P =&gt; (27, 27)\n# Wrong [Wrong] sum of each row != 1\n\nP = N.float() / N.sum(dim = 1, keepdim = True)\n# Broadcasting:\n# N.sum(dim = 1, keepdim = True) =&gt; (27, 1)\n# N.float() =&gt; (27, 27)\n# P =&gt; (27, 27)\n# Correct [Right] sum of each row == 1\n\n\n# visualize this probability matrix, it matches the bigram matrix\nplt.figure(figsize = (16, 16))\nplt.imshow(N, cmap = \"Blues\")\nfor i in range(27):\n    for j in range(27):\n        # character strings\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha = \"center\", va = \"bottom\", color = \"black\")\n        # bigram counts (.item() converts tensor to int)\n        plt.text(j, i, round(P[i, j].item(),3), ha = \"center\", va = \"top\", color = \"black\")\nplt.show()\n\n\n\n\n\n# sample from P\nnames = []\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        p = P[ix]\n        ix = torch.multinomial(p, num_samples = 1 , replacement = True, generator = g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n\n    names.append(\"\".join(out))\nprint(names)\n\n['junide.', 'janasah.', 'p.', 'cony.', 'a.', 'nn.', 'kohin.', 'tolian.', 'juee.', 'ksahnaauranilevias.']"
  },
  {
    "objectID": "bigram.html#training-loss-negative-log-likelihood",
    "href": "bigram.html#training-loss-negative-log-likelihood",
    "title": "Bigram Language Model Character Level",
    "section": "Training Loss (Negative Log Likelihood)",
    "text": "Training Loss (Negative Log Likelihood)\n\n# getting the Bigrams\nfor w in words[:3]:\n    # add start and end tokens\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        # probability of each bigram\n        # should be 1/27 = 0.037 for a uniform distribution i.e., random guessing\n        prob = P[ix1, ix2]\n        print(f\"{ch1}{ch2} -&gt; {prob:.4f}\")\n\n.e -&gt; 0.0478\nem -&gt; 0.0377\nmm -&gt; 0.0253\nma -&gt; 0.3899\na. -&gt; 0.1960\n.o -&gt; 0.0123\nol -&gt; 0.0780\nli -&gt; 0.1777\niv -&gt; 0.0152\nvi -&gt; 0.3541\nia -&gt; 0.1381\na. -&gt; 0.1960\n.a -&gt; 0.1377\nav -&gt; 0.0246\nva -&gt; 0.2495\na. -&gt; 0.1960\n\n\nWe need a single number to combine all of these probabilities and measure the quality of the model Maximum Likelihood Estimation\nSo, the product of all probabilities == the sum of log of all probabilities should be as high as possible\nNow when calculating the log likelihood if any of our probability is 0 our loss will be very much high. So what we will do we will add a very small amount to our N model so that each biagram will atleast be contributing a little to the output\n\n# for smoothing (to prevent zero probabilities = log(0) = -inf)\nP = (N+1).float() / N.sum(dim = 1, keepdim = True)\n\nIn language models, the negative log-likelihood (NLL) is commonly used as a loss function during training. The goal of a language model is to predict the probability distribution of the next word in a sequence given the context of preceding words.\nThe NLL measures the difference between the predicted probability distribution and the actual distribution of the next word. Minimizing the NLL during training encourages the model to assign higher probabilities to the correct words. Mathematically, maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood.\n\ndef nll_loss(input_list, verbose = False):\n    log_likelihood = 0.0\n    n = 0\n    for w in input_list:\n        # add start and end tokens\n        chs = [\".\"] + list(w) + [\".\"]\n        for ch1, ch2 in zip(chs, chs[1:]):\n            ix1 = stoi[ch1]\n            ix2 = stoi[ch2]\n            # probability of each bigram\n            # should be 1/27 = 0.037 for a uniform distribution i.e., random guessing\n            prob = P[ix1, ix2]\n            logprob = torch.log(prob)\n            log_likelihood += logprob\n            n += 1\n            # higher the log probability (closer to 0) is better\n            if verbose:\n                print(f\"{ch1}{ch2} -&gt; {prob:.4f} {logprob:.4f}\")\n\n    # higher the log likelihood (closer to 0) is better\n    print(f\"log Likelihood: {log_likelihood}\")\n\n    # but in loss function lower is better, so we negate it\n    nll = -log_likelihood\n    print(f\"Negative log likelihood: {nll}\")\n\n    # normalize it\n    print(f\"Normalized Negative log Likelihood: {(nll / n)}\") # we need to minimize this\n\nnll_loss(words[:5], verbose = True)\n\n.e -&gt; 0.0478 -3.0402\nem -&gt; 0.0377 -3.2780\nmm -&gt; 0.0254 -3.6713\nma -&gt; 0.3901 -0.9414\na. -&gt; 0.1960 -1.6297\n.o -&gt; 0.0123 -4.3956\nol -&gt; 0.0781 -2.5492\nli -&gt; 0.1777 -1.7274\niv -&gt; 0.0153 -4.1830\nvi -&gt; 0.3545 -1.0372\nia -&gt; 0.1382 -1.9792\na. -&gt; 0.1960 -1.6297\n.a -&gt; 0.1377 -1.9827\nav -&gt; 0.0246 -3.7033\nva -&gt; 0.2499 -1.3867\na. -&gt; 0.1960 -1.6297\n.i -&gt; 0.0185 -3.9910\nis -&gt; 0.0744 -2.5983\nsa -&gt; 0.1483 -1.9086\nab -&gt; 0.0160 -4.1355\nbe -&gt; 0.2480 -1.3943\nel -&gt; 0.1591 -1.8383\nll -&gt; 0.0964 -2.3389\nla -&gt; 0.1880 -1.6714\na. -&gt; 0.1960 -1.6297\n.s -&gt; 0.0642 -2.7460\nso -&gt; 0.0656 -2.7237\nop -&gt; 0.0121 -4.4146\nph -&gt; 0.1998 -1.6104\nhi -&gt; 0.0959 -2.3450\nia -&gt; 0.1382 -1.9792\na. -&gt; 0.1960 -1.6297\nlog Likelihood: -77.71862030029297\nNegative log likelihood: 77.71862030029297\nNormalized Negative log Likelihood: 2.4287068843841553\n\n\n\n# check the loss of the sample names\nnll_loss(names)\n\nlog Likelihood: -256.8978576660156\nNegative log likelihood: 256.8978576660156\nNormalized Negative log Likelihood: 3.471592664718628\n\n\nfor first training example: .emma.: the nll is 2.512\nfor the first 5 training exampless: the average nll is 2.429"
  },
  {
    "objectID": "bigram.html#training-set-of-bigrams",
    "href": "bigram.html#training-set-of-bigrams",
    "title": "Bigram Language Model Character Level",
    "section": "Training set of bigrams",
    "text": "Training set of bigrams\nIn the below code ys is basically the shifted version of the xs. Which means training set will contains the previous character and the next character in the name\n\nxs , ys = [], []\n\nfor w in words:\n    # add start and end tokens\n    chs = [\".\"] + list(w) + [\".\"]\n    for ch1, ch2 in zip(chs, chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        xs.append(ix1)\n        ys.append(ix2)\n\n# convert to tensors\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n\n# first word: 5 separate examples (.emma =&gt; emma.)\nprint(xs[:5])\nprint(ys[:5])\n\ntensor([ 0,  5, 13, 13,  1])\ntensor([ 5, 13, 13,  1,  0])"
  },
  {
    "objectID": "bigram.html#one-hot-encoding",
    "href": "bigram.html#one-hot-encoding",
    "title": "Bigram Language Model Character Level",
    "section": "One-hot encoding",
    "text": "One-hot encoding\nxenc = F.one_hot(xs, num_classes=27).float()\n\nF.one_hot(xs, num_classes=27) converts the input sequence xs into a one-hot encoded representation. Each element in xs is replaced by a one-hot vector with a length of 27 (assuming 27 classes or tokens).\n.float() converts the one-hot encoded tensor to floating-point format, which is often required for further operations.\n\n\n# you have to cast to float for one_hot (doesn't accept dtype parameter)\nxenc = F.one_hot(xs, num_classes = 27).float()\nplt.imshow(xenc[:5], cmap = \"Blues\")\n\n&lt;matplotlib.image.AxesImage at 0x7f724696a2d0&gt;"
  },
  {
    "objectID": "bigram.html#define-the-network",
    "href": "bigram.html#define-the-network",
    "title": "Bigram Language Model Character Level",
    "section": "Define the network",
    "text": "Define the network\n\nOne Ouptut Neuron Single outputs for each example\nxenc @ W - @ is the matrix multiplication operator. It calculates the dot product of the one-hot encoded input xenc and the weight matrix W. - xenc @ W represents the predicted log-counts for each class.\n\nW = torch.randn((27,1))\n# apply matrix multiplication (dot product): (5, 27) @ (27, 1) = (5, 1)\nxenc[:5] @ W\n\ntensor([[-0.3263],\n        [-0.3286],\n        [-0.7713],\n        [-0.7713],\n        [-1.6685]])\n\n\n\n\nOutput Neurons\n\nW = torch.randn((27,27), requires_grad = True)\n# apply matrix multiplication (dot product): (5, 27) @ (27, 27) = (5, 27)\nxenc[:5] @ W\nplt.imshow(xenc[:5] @ W.detach().numpy(), cmap = \"Blues\")\n\n&lt;matplotlib.image.AxesImage at 0x7f724d981310&gt;"
  },
  {
    "objectID": "bigram.html#gradient-descent",
    "href": "bigram.html#gradient-descent",
    "title": "Bigram Language Model Character Level",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nW = torch.randn((27,27), requires_grad = True)\n\ncounts = logits.exp()  # counts, equivalent to N - logits.exp() exponentiates the predicted log-counts, converting them into counts. This step is common in models where the output is interpreted as log-probabilities.\nprobs = counts / counts.sum(1, keepdims=True) - counts.sum(1, keepdims=True) computes the sum of counts along the second dimension, ensuring that the result has the same shape as counts. - probs is the probability distribution over the classes for the next character, obtained by normalizing the counts.\nloss = -probs[torch.arange(5), ys[:5]].log().mean() - The above equation computes the negative log-likelihood loss for the first five names in the dataset. It selects the log-probabilities corresponding to the true labels ys and computes their negative mean.\n\n# forward pass\n\n# log-counts\nlogits = xenc @ W \n# exp them =&gt; counts\ncounts = logits.exp()\n# convert them into probabilities\nprobs = counts / counts.sum(dim = 1, keepdim = True)\n\n# # the previous 3 lines can be replaced by:\n# probs = F.softmax(logits, dim = 1)\n\nloss = - probs[torch.arange(5), ys[:5]].log().mean()\n\n\nloss.item()\n\n3.5917840003967285\n\n\n\nW.grad = None initializes the gradient of the weight matrix to zero before computing the backward pass.\nloss.backward() computes the gradients of the loss with respect to the parameters using backpropagation.\n\nW -= 0.1 * W.grad\n\nThis performs a gradient descent update. It subtracts a multiple of the gradient from the current weight values to update them.\nThe learning rate is represented by the value -0.1. The negative sign indicates that it’s a gradient descent step.\n\n\n# backward pass\n# set gradients to zero\nW.grad = None\nloss.backward()\n\n# update weights\nwith torch.no_grad():\n    W -= 0.1 * W.grad\n\n\nprint(f\"{xs[:5]=}\")\nprint(f\"{ys[:5]=}\")\n# The effect of all gradients are positive (increasing the loss) except for the correct one is negative (decreasing the loss)\nplt.imshow(W.grad.detach().numpy()[xs[:5]], cmap = \"Blues\")\nplt.xticks(range(27), itos, rotation = 90);\n\nxs[:5]=tensor([ 0,  5, 13, 13,  1])\nys[:5]=tensor([ 5, 13, 13,  1,  0])"
  },
  {
    "objectID": "bigram.html#combine-the-forward-and-backward-pass-into-a-single-function",
    "href": "bigram.html#combine-the-forward-and-backward-pass-into-a-single-function",
    "title": "Bigram Language Model Character Level",
    "section": "Combine the forward and backward pass into a single function",
    "text": "Combine the forward and backward pass into a single function\n\nW = torch.randn((27,27), requires_grad = True)\n\n\nfor k in range(100):\n    # forward pass\n    xenc = F.one_hot(xs, num_classes = 27).float()\n    logits = xenc @ W\n    counts = torch.exp(logits)\n    probs = counts / counts.sum(dim = 1, keepdim = True)\n    \n    # loss\n    loss = - probs[torch.arange(len(xs)), ys].log().mean()\n    # add regularization\n    loss += 0.1 * W.pow(2).mean()\n\n    if k % 10 == 0:\n        print(f\"{k}: {loss.item():.4f}\")\n\n    # backward pass\n    W.grad = None\n    loss.backward()\n\n    # update weights\n    with torch.no_grad():\n        W -= 50 * W.grad\n\n0: 3.8628\n10: 2.7460\n20: 2.6501\n30: 2.6189\n40: 2.6046\n50: 2.5972\n60: 2.5930\n70: 2.5906\n80: 2.5891\n90: 2.5882\n\n\nWe are expecting loss similar to the training example (about 2.5)"
  },
  {
    "objectID": "bigram.html#sampling-from-the-model-1",
    "href": "bigram.html#sampling-from-the-model-1",
    "title": "Bigram Language Model Character Level",
    "section": "Sampling from the model",
    "text": "Sampling from the model\nNow to get sample from the model we start from the 0 as index which is . character and pass it to our model and use torch.multinomial to draw sample from the distribution calculated by the last layer of our nerual network model\n\n# sample from P\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        # previosly we used P[ix]\n        p = P[ix]\n\n        # now we use the softmax of the logits\n        # xenc = F.one_hot(torch.tensor([ix]), num_classes = 27).float()\n        # logits = xenc @ W\n        # counts = torch.exp(logits)\n        # p = counts / counts.sum(dim = 1, keepdim = True)\n\n        ix = torch.multinomial(p, num_samples = 1 , replacement = True, generator = g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n\n    print(\"\".join(out))\n\njunide.\njanasah.\np.\ncony.\na.\nnn.\nkohin.\ntolian.\njuee.\nksahnaauranilevias.\n\n\n\n# sample from MLP\ng = torch.Generator().manual_seed(2147483647)\n\nfor i in range(10):\n    out = []\n    ix = 0\n    while True:\n        # previosly we used P[ix]\n        # p = P[ix]\n\n        # now we use the softmax of the logits\n        xenc = F.one_hot(torch.tensor([ix]), num_classes = 27).float()\n        logits = xenc @ W\n        counts = torch.exp(logits)\n        p = counts / counts.sum(dim = 1, keepdim = True)\n\n        ix = torch.multinomial(p, num_samples = 1 , replacement = True, generator = g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n\n    print(\"\".join(out))\n\njunide.\njanaqah.\np.\ncfay.\na.\nnn.\nkohin.\ntolian.\njgee.\nksahnaauyanilevias.\n\n\nThe results are the same, since we’re using the same model with the same loss\nW is the log counts (estimated before from the bigram model)"
  },
  {
    "objectID": "bn.html",
    "href": "bn.html",
    "title": "Batch Normalization",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport math"
  },
  {
    "objectID": "bn.html#explaining-the-problem",
    "href": "bn.html#explaining-the-problem",
    "title": "Batch Normalization",
    "section": "3.1 - Explaining The Problem",
    "text": "3.1 - Explaining The Problem\n\nfig, axes = plt.subplots(1, 3, figsize = (15, 4))\n\n\naxes[0].hist(hpreact.flatten().data, bins = 50);\naxes[0].set_title(\"h pre-activation\")\n\naxes[1].hist(h.flatten().data, bins = 50);\naxes[1].set_title(\"h\")\n\naxes[2].plot(torch.linspace(-5, 5, 100), torch.tanh(torch.linspace(-5, 5, 100)));\naxes[2].set_title(\"tanh\")\n\nText(0.5, 1.0, 'tanh')\n\n\n\n\n\ncomment: There’re many values = 1 or -1 in tanh because pre-activations with extreme values\n➡️ changing the input doesn’t change the outputvalue much\n➡️ the derivative of tanh is 1 - tanh^2\n➡️ the gradient is 0 for all values = 1 or -1\n\nplt.figure(figsize = (20, 10))\nplt.imshow(h.abs() &gt; 0.99, cmap = \"gray\", interpolation='nearest')\nplt.title(\"h &gt; 0.99: white is True, black is False\");\n\n\n\n\ncomment: if a column is completely white, it’s a dead neuron"
  },
  {
    "objectID": "bn.html#solution",
    "href": "bn.html#solution",
    "title": "Batch Normalization",
    "section": "3.2- Solution",
    "text": "3.2- Solution\nTo address the issue of dead neurons, various initialization techniques exist, such as He initialization, which helps mitigate the saturation problem during the initial phases of training. He initialization scales the weights based on the number of input units to the neuron.\n# He initialization for tanh activation\nW1 = torch.randn(input_size, hidden_size) * math.sqrt(2 / input_size)\nb1 = torch.zeros(hidden_size)\nApplying proper weight initialization techniques can significantly contribute to the alleviation of dead neurons and facilitate more effective training in neural networks.\nMultiplying weights with small number calculated using math.sqrt(2 / input_size) is more systematic approach and it’s the recommended way but for the moment we can multiply weights with small value to mitigate this issue\n\nn_embd = 10 \nn_hidden = 200\n\ng = torch.Generator().manual_seed(2147483647)\nC  = torch.randn((vocab_size, n_embd),generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden),generator=g) * 0.2\nb1 = torch.randn(n_hidden,generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size),generator=g) * 0.01 # Initialize to small values\nb2 = torch.randn(vocab_size,generator=g) * 0 # Remove bias\n\n\nparameters = [C, W1, b1, W2, b2]\n\nprint(sum(p.nelement() for p in parameters))\n\nfor p in parameters:\n  p.requires_grad = True\n\n11897\n\n\nNow using the above solution we are making sure that we are not wasting the first few cycles of our training to squash down our weights to the point where our neural network actually starts learning, instead we somehow get the idea what our initial loss will look like before training and initialize the weights of the neural network accordingly.\n\nix = torch.randint(0, len(Xtr), (batch_size,), generator=g)\n\n# batch \nXb, Yb = Xtr[ix], Ytr[ix]\n\n# forward pass\nemb = C[Xb] # embed characters into vector space\nembcat = emb.view((emb.shape[0], -1)) # flatten\nhpreact = embcat @ W1 + b1 # hidden layer pre-activation\nh = torch.tanh(hpreact) # hidden layer activation\nlogits = h @ W2 + b2 # output layer \nloss = F.cross_entropy(logits, Yb) # cross-entropy loss\n\nprint(f\"initial loss = {loss}\")\n\ninitial loss = 3.3134593963623047\n\n\n\nfig, axes = plt.subplots(1, 3, figsize = (15, 4))\n\n\naxes[0].hist(hpreact.flatten().data, bins = 50);\naxes[0].set_title(\"h pre-activation\")\n\naxes[1].hist(h.flatten().data, bins = 50);\naxes[1].set_title(\"h\")\n\naxes[2].plot(torch.linspace(-5, 5, 100), torch.tanh(torch.linspace(-5, 5, 100)));\naxes[2].set_title(\"tanh\")\n\nText(0.5, 1.0, 'tanh')\n\n\n\n\n\nAs we can see that most of our neurons in the neural network are active\n\nplt.figure(figsize = (20, 10))\nplt.imshow(h.abs() &gt; 0.99, cmap = \"gray\", interpolation='nearest')\nplt.title(\"h &gt; 0.99: white is True, black is False\");"
  },
  {
    "objectID": "bn.html#explaination",
    "href": "bn.html#explaination",
    "title": "Batch Normalization",
    "section": "5.1- Explaination",
    "text": "5.1- Explaination\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\n# BatchNorm parameters\nbngain = torch.ones((1, n_hidden))\nbnbias = torch.zeros((1, n_hidden))\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n12097\n\n\n\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n    \n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmeani = hpreact.mean(0, keepdim=True)\n    bnstdi = hpreact.std(0, keepdim=True)\n    # we want it to be gaussian only at initialization (not always), the Neural Network may need to change the it\n    # Scale and Shift: scale the normalized batch by a learnable parameter (gamma) and shift it by another learnable parameter (beta)\n    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    \n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    break\n\n      0/ 200000: 3.3239"
  },
  {
    "objectID": "bn.html#sampling",
    "href": "bn.html#sampling",
    "title": "Batch Normalization",
    "section": "5.2- Sampling",
    "text": "5.2- Sampling\nHow to forward a single example to the model and get a prediction, even the model uses the mean ans std of the batch to normalize the input\nWe need a step after training, and find the mean and std of the whole training set\n\n# Calibrate the batch norm statistics\nwith torch.no_grad():\n    emb = C[Xtr]\n    embcat = emb.view((emb.shape[0], -1))\n    hpreact = embcat @ W1 + b1\n\n    bnmean = hpreact.mean(dim=0, keepdim=True)\n    bnstd = hpreact.std(dim=0, keepdim=True)\n\n\n@torch.no_grad()\ndef split_loss(split):\n    x,y = {\n        \"train\": (Xtr, Ytr),\n        \"dev\": (Xdev, Ydev),\n        \"test\": (Xte, Yte),\n    }[split]\n\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view((emb.shape[0], -1)) # (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1 # (N, n_hidden)\n    \n    # batch norm in test mode\n    hpreact = (hpreact - bnmean) / (bnstd + 1e-6)\n    hpreact = hpreact * bngain + bnbias\n\n\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n\n    print(f\"{split} loss: {loss.item()}\")\n\nsplit_loss(\"train\")\nsplit_loss(\"dev\")\n\ntrain loss: 3.262329339981079\ndev loss: 3.2617669105529785\n\n\n\n5.2.1- It can be done during training (without additional step)\nNow with the above bngain and bnbias approach there is issue that we are about the draw the sample after training we had also to do extra step\n    # batch norm in test mode\n    hpreact = (hpreact - bnmean) / (bnstd + 1e-6)\n    hpreact = hpreact * bngain + bnbias\nTo solve this we could initialize the running mean and running standard deviation. And manually calculate the running mean and running standard deviation during training\nwith torch.no_grad():\n    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\nUsing the above approach we eventually have bnmean_running should equal to bnmean after training, same for bnstd_running and bnstd. And after the training we could use the bnmean_running and bnstd_running to draw sample\n    # batch norm in test mode\n    hpreact = (hpreact - bnmean_running) / (bnstd_running + 1e-6)\n    hpreact = hpreact * bngain + bnbias\nAnd another thing to notice with the running mean and running standard deviation approach is that now we don’t need to calculate the b1, because it wouldn’t have effect on bnmeani and bnstdi when calculating the hpreact in the training loop.\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\n# BatchNorm parameters\nbngain = torch.ones((1, n_hidden))\nbnbias = torch.zeros((1, n_hidden))\nbnmean_running = torch.zeros((1, n_hidden))\nbnstd_running = torch.ones((1, n_hidden))\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n    \n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmeani = hpreact.mean(0, keepdim=True)\n    bnstdi = hpreact.std(0, keepdim=True)\n    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n    with torch.no_grad():\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n    \n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n    \n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n    \n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    break\n\n      0/ 200000: 3.3239\n\n\n\n@torch.no_grad()\ndef split_loss(split):\n    x,y = {\n        \"train\": (Xtr, Ytr),\n        \"dev\": (Xdev, Ydev),\n        \"test\": (Xte, Yte),\n    }[split]\n\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view((emb.shape[0], -1)) # (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1 # (N, n_hidden)\n    \n    # batch norm in test mode\n    hpreact = (hpreact - bnmean_running) / (bnstd_running + 1e-6)\n    hpreact = hpreact * bngain + bnbias\n\n\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n\n    print(f\"{split} loss: {loss.item()}\")\n\nsplit_loss(\"train\")\nsplit_loss(\"dev\")\n\ntrain loss: 3.270005226135254\ndev loss: 3.269122362136841"
  },
  {
    "objectID": "bn.html#forward-pass-activations-stats",
    "href": "bn.html#forward-pass-activations-stats",
    "title": "Batch Normalization",
    "section": "7.1- Forward Pass Activations stats",
    "text": "7.1- Forward Pass Activations stats\n\nplt.figure(figsize=(20, 4))\nlegends = []\nfor i, layer in enumerate(layers[:-1]):\n    if isinstance(layer, Tanh):\n        t = layer.out\n        print(f\"layer {i} mean {t.mean().item():.2f} std {t.std().item():.2f} saturated {((t.abs() &gt; 0.97).float().mean().item()) * 100:.2f}%\")\n\n        hy, hx = torch.histogram(t, density=True)\n        plt.plot(hx[:-1].detach(), hy.detach())\n\n        legends.append(f\"layer {i} ({layer.__class__.__name__})\")\n\nplt.legend(legends)\nplt.title(\"activation distribution\")\nplt.show()\n\nlayer 2 mean -0.00 std 0.64 saturated 2.56%\nlayer 5 mean 0.00 std 0.66 saturated 2.00%\nlayer 8 mean 0.00 std 0.66 saturated 1.47%\nlayer 11 mean 0.00 std 0.66 saturated 1.22%\nlayer 14 mean -0.00 std 0.66 saturated 0.72%"
  },
  {
    "objectID": "bn.html#backward-pass-gradient-statistics",
    "href": "bn.html#backward-pass-gradient-statistics",
    "title": "Batch Normalization",
    "section": "7.2- Backward pass Gradient Statistics",
    "text": "7.2- Backward pass Gradient Statistics\n\nplt.figure(figsize=(20, 4))\nlegends = []\nfor i, layer in enumerate(layers[:-1]):\n    if isinstance(layer, Tanh):\n        t = layer.out.grad\n        print(f\"layer {i} mean {t.mean().item():.5f} std {t.std().item():.5f} saturated {((t.abs() &gt; 0.97).float().mean().item()) * 100:.8f}%\")\n\n        hy, hx = torch.histogram(t, density=True)\n        plt.plot(hx[:-1].detach(), hy.detach())\n\n        legends.append(f\"layer {i} ({layer.__class__.__name__})\")\n\nplt.legend(legends)\nplt.title(\"gradient distribution\")\nplt.show()\n\nlayer 2 mean -0.00000 std 0.00125 saturated 0.00000000%\nlayer 5 mean -0.00000 std 0.00110 saturated 0.00000000%\nlayer 8 mean -0.00000 std 0.00095 saturated 0.00000000%\nlayer 11 mean 0.00000 std 0.00094 saturated 0.00000000%\nlayer 14 mean -0.00000 std 0.00105 saturated 0.00000000%\n\n\n\n\n\ncomment: Gradients are not vanishing or exploding"
  },
  {
    "objectID": "bn.html#parameter-activation-and-gradient-statistics",
    "href": "bn.html#parameter-activation-and-gradient-statistics",
    "title": "Batch Normalization",
    "section": "7.3- Parameter Activation and Gradient Statistics",
    "text": "7.3- Parameter Activation and Gradient Statistics\n\nplt.figure(figsize=(20, 4))\nlegends = []\n\nfor i,p in enumerate(parameters):\n    t = p.grad\n    # neglect bias and batch norm parameters (weights only)\n    if p.ndim == 2:\n        print(f\"weight shape {tuple(p.shape)} | mean {t.mean().item():.5f} | std {t.std().item():.5f} | grad:data ratio {t.std() / p.std():.2e}\")\n        hy, hx = torch.histogram(t, density=True)\n        plt.plot(hx[:-1].detach(), hy.detach())\n        legends.append(f\"weight {i} ({p.shape})\")\n\nplt.legend(legends)\nplt.title(\"Weights Gradient distribution\")\n\nweight shape (27, 10) | mean 0.00000 | std 0.00737 | grad:data ratio 7.37e-03\nweight shape (30, 100) | mean 0.00031 | std 0.01392 | grad:data ratio 2.29e-01\nweight shape (100, 100) | mean 0.00003 | std 0.00601 | grad:data ratio 2.01e-01\nweight shape (100, 100) | mean -0.00002 | std 0.00522 | grad:data ratio 1.91e-01\nweight shape (100, 100) | mean -0.00005 | std 0.00506 | grad:data ratio 1.92e-01\nweight shape (100, 100) | mean -0.00011 | std 0.00522 | grad:data ratio 2.00e-01\nweight shape (100, 27) | mean 0.00002 | std 0.01235 | grad:data ratio 2.86e-01\n\n\nText(0.5, 1.0, 'Weights Gradient distribution')"
  },
  {
    "objectID": "bn.html#update-data-raito-over-time",
    "href": "bn.html#update-data-raito-over-time",
    "title": "Batch Normalization",
    "section": "7.4- update: data raito over time",
    "text": "7.4- update: data raito over time\n\nplt.figure(figsize=(20, 4))\nlegends = []\nfor i, p in enumerate(parameters):\n    t = p\n    # neglect bias and batch norm parameters (weights only)\n    if p.ndim == 2:\n        plt.plot([update_to_data_ratio[j][i] for j in range(len(update_to_data_ratio))])\n        legends.append(f\"weight {i}\")\n    \n# rough guide for what it should be: 1e-3\nplt.plot([0, len(update_to_data_ratio)], [-3, -3], color=\"black\", linestyle=\"--\")\nplt.legend(legends)\nplt.title(\"Update to Data ratio\")\n\nText(0.5, 1.0, 'Update to Data ratio')\n\n\n\n\n\nConclusion: Batchnorm makes the model more robust\n\nChanging the gain (when using batchnorm) doesn’t change too much except the Update to Data Ratio\nRemoving fan_it doesn’t change much too (when using batchnorm)"
  },
  {
    "objectID": "transformers.html",
    "href": "transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "This is the diagram of the Transformer network presented in the Attention is All You Need paper. We will go through all the different pieces of this network throughout this notebook."
  },
  {
    "objectID": "transformers.html#transformers-explained-step-by-step",
    "href": "transformers.html#transformers-explained-step-by-step",
    "title": "Transformers",
    "section": "Transformers Explained Step by Step",
    "text": "Transformers Explained Step by Step\n\nTokenization\nThe first step in processing text is to cut it into pieces called tokens. There are many variations of how to do it, and we won’t go into details, but BERT uses WordPiece tokenization. This means that tokens correspond roughly to words and punctuation, although a word can also be split into several tokens if it contains a common prefix or suffix. These are called sub-word tokens and usually contain ## characters. Words can even be spelled out if they have never been seen before. \n\n\n\n\n\nEmbedding\nThe second step is to associate each token with an embedding, which is nothing more than a vector of real numbers. Again, there are many ways to create embedding vectors. Fortunately, already trained embeddings are often provided by research groups, and we can just use an existing dictionary to convert the WordPiece tokens into embedding vectors. \n\n\n\n The embedding of tokens into vectors is an achievement in itself. The values inside an embedding carry information about the meaning of the token, but they are also arranged in such a way that one can perform mathematical operations on them, which correspond to semantic changes, like changing the gender of a noun, or the tense of a verb, or even the homeland of a city.\n\n\n\n\n\nContext\nHowever, embeddings are associated with tokens by a straight dictionary look-up, which means that the same token always gets the same embedding, regardless of its context. This is where the attention mechanism comes in, and specifically for BERT, the scaled dot-product self-attention. Attention transforms the default embeddings by analyzing the whole sequence of tokens, so that the values are more representative of the token they represent in the context of the sentence.\n\n\n\n\n\nSelf Attention Mechanism\nLet’s have a look at this process with the sequence of tokens walk, by, river, bank. Each token is initially replaced by its default embedding, which in this case is a vector with 768 components. \n\n\n\n\nLet’s color the embedding of the first token to follow what happens to it. We start by calculating the scalar product between pairs of embeddings. Here we have the first embedding with itself. When the two vectors are more correlated, or aligned, meaning that they are generally more similar, the scalar product is higher (darker in image), and we consider that they have a strong relationship. If they had less similar content, the scalar product would be lower (brighter in the image) and we would consider that they don’t relate to each other.\n\n\n\n\n\nWe go on and calculate the scalar product for every possible pair of embedding vectors in the input sequence. The values obtained are usually scaled down to avoid getting large values, which improves the numerical behavior. That’s done here by dividing by the square root of 768, which is the size of the vectors. \n\n\n\n\nThen comes the only non-linear operation in the attention mechanism: The scaled values are passed through a softmax activation function, by groups corresponding to each input token. So in this illustration, we apply the softmax column by column. What the softmax does is to exponentially amplify large values, while crushing low and negative values towards zero. It also does normalization, so that each column sums up to 1.\n\n\n\n\n\nFinally, we create a new embedding vector for each token by linear combination of the input embeddings, in proportions given by the softmax results. We can say that the new embedding vectors are contextualized, since they contain a fraction of every input embedding for this particular sequence of tokens. In particular, if a token has a strong relationship with another one, a large fraction of its new contextualized embedding will be made of the related embedding. If a token doesn’t relate much to any other, as measured by the scalar product between their input embeddings, its contextualized embedding will be nearly identical to the input embedding.\n\n\n\n\n\nFor instance, one can imagine that the vector space has a direction that corresponds to the idea of nature. The input embeddings of the tokens river and bank should both have large values in that direction, so that they are more similar and have a strong relationship. As a result, the new contextualized embeddings of the river and bank tokens would combine both input embeddings in roughly equal parts. On the other hand, the preposition by sounds quite neutral, so that its embedding should have a weak relationship with every other one and little modification of its embedding vector would occur. So there we have the mechanism that lets the scaled dot-product attention utilize context.\nTo recap: 1. First, it determines how much the input embedding vectors relate to each other using the scalar product. 2. The results are then scaled down, and the softmax activation function is applied, which normalizes these results in a non-linear way. 3. New contextualized embeddings are finally created for every token by linear combination of all the input embeddings, using the softmax proportions as coefficients \n\n\n\n\n\n\nKeys, Queries and Values\nHowever, that’s not the whole story. Most importantly, we don’t have to use the input embedding vectors as is. We can first project them using 3 linear projections to create the so-called Key, Query, and Value vectors. Typically, the projections are also mapping the input embeddings onto a space of lower dimension. In the case of BERT, the Key, Query, and Value vectors all have 64 components. \n\n\n\n\nEach projection can be thought of as focusing on different directions of the vector space, which would represent different semantic aspects. One can imagine that a Key is the projection of an embedding onto the direction of “prepositions”, and a Query is the projection of an embedding along the direction of “locations”. In this case, the Key of the token by should have a strong relationship with every other Query, since by should have strong components in the direction of “prepositions”, and every other token should have strong components in the direction of “locations”. The Values can come from yet another projection that is relevant, for example the direction of physical places. It’s these values that are combined to create the contextualized embeddings In practice, the meaning of each projection may not be so clear, and the model is free to learn whatever projections allow it to solve language tasks the most efficiently.\n\n\nMulti-head Attention\nIn addition, the same process can be repeated many times with different Key, Query, and Value projections, forming what is called a multi-head attention. Each head can focus on different projections of the input embeddings. For instance, one head could calculate the preposition/location relationships, while another head could calculate subject/verb relationships, simply by using different projections to create the Key, Query, and Value vectors. The outputs from each head are concatenated back in a large vector. BERT uses 12 such heads, which means that the final output contains one 768-component contextualized embedding vector per token, equally long with the input. \n\n\n\n\n\n\nPositional Encoding\nWe can also kickstart the process by adding the input embeddings to positional embeddings. Positional embeddings are vectors that contain information about a position in the sequence, rather than about the meaning of a token. This adds information about the sequence even before attention is applied, and it allows attention to calculate relationships knowing the relative order of the tokens.\n\n\n\n\n\nA detailed explanation of how it works can be found here, but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won’t really need to understand:\n\\[\\begin{equation}\n  p_{i,j} = \\left\\{\n  \\begin{array}{@{}ll@{}}\n    \\sin \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=even \\\\\n    \\cos \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=odd \\\\\n  \\end{array}\\right.\n\\end{equation}\n\\]\n\n\nBERT\nFinally, thanks to the non-linearity introduced by the softmax function, we can achieve even more complex transformations of the embeddings by applying attention again and again, with a couple of helpful steps between each application. A complete model like BERT uses 12 layers of attention, each with its own set of projections So when you search for suggestions for a “walk by the river bank”, the computer doesn’t only get a chance to recognize the keyword “river”, but even the numerical values given to “bank” indicate that you’re interested in enjoying the waterside, and not in need of the nearest cash machine."
  },
  {
    "objectID": "transformers.html#importing-libraries",
    "href": "transformers.html#importing-libraries",
    "title": "Transformers",
    "section": "Importing Libraries",
    "text": "Importing Libraries"
  },
  {
    "objectID": "transformers.html#multi-head-attention-1",
    "href": "transformers.html#multi-head-attention-1",
    "title": "Transformers",
    "section": "Multi Head Attention",
    "text": "Multi Head Attention\nAttention is a mechanism that allows neural networks to assign a different amount of weight or attention to each element in a sequence. For text sequences, the elements are token embeddings, where each token is mapped to a vector of some fixed dimension. For example, in BERT each token is represented as a 768-dimensional vector. The “self” part of self-attention refers to the fact that these weights are computed for all hidden states in the same set—for example, all the hidden states of the encoder. By contrast, the attention mechanism associated with recurrent models involves computing the relevance of each encoder hidden state to the decoder hidden state at a given decoding timestep.\nThe main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a weighted average of each embedding. Another way to formulate this is to say that given a sequence of token embeddings \\(x_{1}, x_{2}, ..., x_{n}\\), self-attention produces a sequence of new embeddings \\(x^{'}_{1}, x^{'}_{2}, ..., x^{'}_{n}\\) where each \\(x^{'}_{i}\\) is a linear combination of all the \\(x_{j}\\):\n\\[x^{'}_{i} = \\sum^{n}_{j=1} w_{ji} . x_{j}\\]\n\n\nMultiHeadAttention\n\n MultiHeadAttention (embed_size, heads)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "transformers.html#encoder-layer",
    "href": "transformers.html#encoder-layer",
    "title": "Transformers",
    "section": "Encoder Layer",
    "text": "Encoder Layer\n\n\n\n\n\nWe will be referring to the encoder layer. The encoder layer/block consists of: 1. Multi-Head Attention 2. Add & Norm 3. Feed Forward 4. Add & Norm again.\n\nnn.LayerNorm()\nThe forward_expansion is a parameter in the “Attention is All You Need” paper which simply adds nodes to the Linear Layer. Since it’s used in two different layers in the end it doesn’t affect the shape of the output (same as input) it just add some extra computation. Its default value is 4.\n\n\n\nTransformerLayer\n\n TransformerLayer (embed_size, heads, dropout, forward_expansion=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "transformers.html#encoder",
    "href": "transformers.html#encoder",
    "title": "Transformers",
    "section": "Encoder",
    "text": "Encoder\n\n\n\n\n\nWe will be referring to the transformer block. The transformer block consists of: 1. Embedding 2. Positional Encoding 3. Transformer Block\n\n\nEncoder\n\n Encoder (src_vocab_size, embed_size, num_layers, heads, device,\n          forward_expansion, dropout, max_length)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "uml_diagrams.html",
    "href": "uml_diagrams.html",
    "title": "UML Diagrams",
    "section": "",
    "text": "import base64\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\n\ndef mm(graph):\n    graphbytes = graph.encode(\"utf8\")\n    base64_bytes = base64.b64encode(graphbytes)\n    base64_string = base64_bytes.decode(\"ascii\")\n    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))"
  },
  {
    "objectID": "uml_diagrams.html#sequence-diagram",
    "href": "uml_diagrams.html#sequence-diagram",
    "title": "UML Diagrams",
    "section": "Sequence Diagram",
    "text": "Sequence Diagram\nThe sequence diagram that follows shows how the developer, IDE, model, and training process interact with one another. The process begins with the developer writing code in the IDE, the IDE sending the model the current code snippet, and the model using the machine learning model it has been trained to produce recommendations. After that, the developer can engage with the recommendations and offer input that helps the model get better over time by periodically retraining it. As the developer integrates the recommendations into their code, the process proceeds.\n\nmm(\"\"\"\nsequenceDiagram\n    participant Developer\n    participant IDE\n    participant Model\n    participant Dataset\n    participant MLTraining\n\n    Developer-&gt;&gt;IDE: Starts coding in IDE\n    IDE-&gt;&gt;Model: Developer's current code snippet\n    Model-&gt;&gt;MLTraining: Retrieve context from model\n    MLTraining--&gt;&gt;Model: Trains machine learning model\n    Model--&gt;&gt;IDE: Trained model\n\n    alt Code Completion Activated\n        Developer-&gt;&gt;IDE: Continues typing\n        IDE-&gt;&gt;Model: Current code snippet\n        Model-&gt;&gt;Model: Generate code suggestions\n        Model--&gt;&gt;IDE: List of suggestions\n    else User Interaction\n        Developer-&gt;&gt;IDE: Reviews and selects suggestions\n        IDE-&gt;&gt;Model: User feedback\n        Model--&gt;&gt;MLTraining: Update model with feedback\n        MLTraining--&gt;&gt;Model: Retrain model\n    end\n\n    Developer--&gt;&gt;IDE: Continues coding with suggestions\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#class-diagram",
    "href": "uml_diagrams.html#class-diagram",
    "title": "UML Diagrams",
    "section": "Class Diagram",
    "text": "Class Diagram\nThe primary components and their interactions in an intelligent code completion system are depicted in this class diagram, which also emphasizes the information flow between the training component, the machine learning model, the integrated development environment, and the developer. However they might now reflect same in the code as we might change them according to the need.\n\nmm(\"\"\"\nclassDiagram\n  class Developer {\n    +writeCode()\n  }\n\n  class IDE {\n    +sendCodeSnippet()\n  }\n\n  class Model {\n    +generateCodeSuggestions()\n    +receiveUserFeedback()\n  }\n\n  class MLTraining {\n    +trainModel()\n    +updateModel()\n  }\n\n  Developer --&gt; IDE: Uses\n  IDE --&gt; Model: Sends code snippet\n  Model --&gt; MLTraining: Trains on dataset\n  Model --&gt; IDE: Sends suggestions\n  Model --&gt; MLTraining: Receives user feedback\n  MLTraining --&gt; Model: Updates model\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#state-machine-diagram",
    "href": "uml_diagrams.html#state-machine-diagram",
    "title": "UML Diagrams",
    "section": "State Machine Diagram",
    "text": "State Machine Diagram\nThe system is shown in the state machine diagram as “Coding” upon startup. It changes to the “Code Suggestions” state when code completion is enabled. In the event that the user interacts with the suggestions, the system can either move to the “User Interaction” stage or revert to the “Coding” state. The “User Interaction” state permits the developer to type further or approve/disapprove recommendations.\n\nmm(\"\"\"\nstateDiagram\n  state \"Coding\" as Coding\n  state \"Code Suggestions\" as Suggestions\n  state \"User Interaction\" as Interaction\n\n  [*] --&gt; Coding\n\n  Coding --&gt; Suggestions: Code Completion Activated\n  Suggestions --&gt; Coding: Suggestions Rejected\n  Suggestions --&gt; Interaction: User Interaction\n\n  Interaction --&gt; Suggestions: Continue Typing\n  Interaction --&gt; Coding: Suggestions Accepted\n\"\"\")"
  },
  {
    "objectID": "uml_diagrams.html#user-journey-diagram",
    "href": "uml_diagrams.html#user-journey-diagram",
    "title": "UML Diagrams",
    "section": "User Journey Diagram",
    "text": "User Journey Diagram\nThe steps a developer takes to use the intelligent code completion system and get started with coding in the IDE are shown in this user journey diagram. Code completion, suggestion creation, user interaction, and the loop of continual improvement via user feedback and model retraining are all included.\n\nmm(\"\"\"\njourney\n  title Developer's Journey with Intelligent Code Completion\n\n  section Getting Started\n    Developer --&gt; IDE: Starts coding in IDE\n\n  section Code Completion Activated\n    IDE --&gt; Model: Developer's current code snippet\n    Model --&gt; MLTraining: Retrieve context from 70GB C code dataset\n    MLTraining --&gt;&gt; Model: Train machine learning model\n    Model --&gt;&gt; IDE: Trained model\n    Developer --&gt; IDE: Continues typing\n\n  section Suggestions\n    IDE --&gt; Model: Current code snippet\n    Model --&gt; Model: Generate code suggestions\n    Model --&gt;&gt; IDE: List of suggestions\n    Developer --&gt; IDE: Reviews and selects suggestions\n\n  section User Interaction\n    IDE --&gt; Model: User feedback\n    Model --&gt;&gt; MLTraining: Update model with feedback\n    MLTraining --&gt;&gt; Model: Retrain model\n    Model --&gt;&gt; IDE: Updated model\n    Developer --&gt; IDE: Continues coding with suggestions\n\n  section Conclusion\n    Developer --&gt; IDE: Finishes coding\n\n\"\"\")"
  }
]